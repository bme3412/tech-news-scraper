
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>News Article Clusters</title>
            
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.6;
                color: #333;
                max-width: 1200px;
                margin: 0 auto;
                padding: 20px;
                background-color: #f7f7f7;
            }
            .container {
                background-color: white;
                padding: 30px;
                border-radius: 8px;
                box-shadow: 0 0 10px rgba(0,0,0,0.1);
            }
            header {
                text-align: center;
                margin-bottom: 30px;
                padding-bottom: 20px;
                border-bottom: 1px solid #eaeaea;
            }
            h1 {
                font-size: 2.2em;
                color: #2c3e50;
                margin-bottom: 10px;
            }
            h2 {
                font-size: 1.8em;
                color: #3498db;
                margin-top: 40px;
                margin-bottom: 20px;
                padding-bottom: 10px;
                border-bottom: 1px solid #eaeaea;
            }
            h3 {
                font-size: 1.4em;
                color: #2c3e50;
                margin-top: 25px;
                margin-bottom: 15px;
            }
            h4 {
                font-size: 1.1em;
                color: #555;
                margin-top: 20px;
                margin-bottom: 10px;
                font-weight: bold;
            }
            .article-card {
                background-color: white;
                border: 1px solid #ddd;
                border-left: 5px solid #3498db;
                border-radius: 4px;
                padding: 15px;
                margin-bottom: 20px;
                box-shadow: 0 1px 3px rgba(0,0,0,0.1);
            }
            .article-title {
                font-weight: bold;
                font-size: 1.2em;
                margin-bottom: 8px;
                cursor: pointer;
                color: #3498db;
                display: flex;
                align-items: center;
            }
            .article-title:hover {
                text-decoration: underline;
            }
            .article-title:after {
                content: "▼";
                font-size: 0.8em;
                margin-left: 8px;
                transition: transform 0.3s ease;
            }
            .article-title.active:after {
                transform: rotate(180deg);
            }
            .article-source {
                color: #666;
                font-size: 0.9em;
                margin-bottom: 8px;
            }
            .article-description {
                margin-bottom: 10px;
                color: #555;
                font-style: italic;
            }
            .article-content {
                display: none;
                background-color: #f9f9f9;
                border-top: 1px solid #eaeaea;
                padding: 15px;
                margin-top: 15px;
                font-size: 0.95em;
                max-height: 500px;
                overflow-y: auto;
                line-height: 1.7;
            }
            .article-content p {
                margin-bottom: 15px;
                text-align: justify;
            }
            .article-content h4 {
                margin-top: 20px;
                margin-bottom: 10px;
                color: #2c3e50;
            }
            .theme-meta {
                display: flex;
                justify-content: space-between;
                margin-bottom: 20px;
                font-size: 0.9em;
                color: #666;
            }
            .theme-count {
                background-color: #3498db;
                color: white;
                border-radius: 20px;
                padding: 3px 10px;
                font-size: 0.8em;
            }
            footer {
                text-align: center;
                margin-top: 50px;
                padding-top: 20px;
                border-top: 1px solid #eaeaea;
                color: #777;
                font-size: 0.9em;
            }
            a {
                color: #3498db;
                text-decoration: none;
            }
            a:hover {
                text-decoration: underline;
            }
        </style>
        
            
        <script>
            document.addEventListener('DOMContentLoaded', function() {
                const articleTitles = document.querySelectorAll('.article-title');
                
                articleTitles.forEach(title => {
                    title.addEventListener('click', function() {
                        const content = this.parentNode.querySelector('.article-content');
                        if (content.style.display === 'block') {
                            content.style.display = 'none';
                            this.classList.remove('active');
                        } else {
                            content.style.display = 'block';
                            this.classList.add('active');
                        }
                    });
                });
            });
        </script>
        
        </head>
        <body>
            <div class="container">
                <header>
                    <h1>News Article Analysis</h1>
                    <p>Clustered by themes and topics - Generated on 2025-03-26 23:30</p>
                    <div class="theme-meta">
                        <span>Total Articles: 39</span>
                        <span>Themes Identified: <span class="theme-count">7</span></span>
                    </div>
                </header>
                
                <section id="themes">
        
                    <section id="theme-1">
                        <h2>Artificial Intelligence <small>(30 articles)</small></h2>
                        <p>Articles related to advancements, applications, and impact of artificial intelligence</p>
                        
                        <h3>Articles in this Theme</h3>
            
                        <div class="article-card">
                            <div class="article-title">Beyond transformers: Nvidia’s MambaVision aims to unlock faster, cheaper enterprise computer vision</div>
                            <div class="article-source">VentureBeat - March 25, 2025 3:35 PM</div>
                            <div class="article-description">Nvidia is updating its computer vision models with new versions of MambaVision that combine the best of Mamba and transformers to improve efficiency.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Transformer-based large language models (LLMs) are the foundation of the modern generative AI landscape. Transformers aren’t the only way to do gen AI, though. Over the course of the last year, Mamba, an approach that uses Structured State Space Models (SSM), has also picked up adoption as an alternative approach from multiple vendors, including AI21 and AI silicon giant Nvidia. Nvidia first discussed the concept of Mamba-powered models in 2024 when it initially released the MambaVision research and some early models. This week, Nvidia is expanding on its initial effort with a series of updated MambaVision models available on Hugging Face. MambaVision, as the name implies, is a Mamba-based model family for computer vision and image recognition tasks. The promise of MambaVision for enterprise is that it could improve the efficiency and accuracy of vision operations, at potentially lower costs, thanks to lower computational requirements. SSMs are a neural network architecture class that processes sequential data differently from traditional transformers. While transformers use attention mechanisms to process all tokens in relation to each other, SSMs model sequence data as a continuous dynamic system. Mamba is a specific SSM implementation developed to address the limitations of earlier SSM models. It introduces selective state space modelling that dynamically adapts to input data and hardware-aware design for efficient GPU utilization. Mamba aims to provide comparable performance to transformers on many tasks while using fewer computational resources Traditional Vision Transformers (ViT) have dominated high-performance computer vision for the last several years, but at significant computational cost. Pure Mamba-based approaches, while more efficient, have struggled to match Transformer performance on complex vision tasks requiring global context understanding. MambaVision bridges this gap by adopting a hybrid approach. Nvidia’s MambaVision is a hybrid model that strategically combines Mamba’s efficiency with the Transformer’s modelling power. The architecture’s innovation lies in its redesigned Mamba formulation specifically engineered for visual feature modeling, augmented by strategic placement of self-attention blocks in the final layers to capture complex spatial dependencies. Unlike conventional vision models that rely exclusively on either attention mechanisms or convolutional approaches, MambaVision’s hierarchical architecture employs both paradigms simultaneously. The model processes visual information through sequential scan-based operations from Mamba while leveraging self-attention to model global context — effectively getting the best of both worlds. The new set of MambaVision models released on Hugging Face is available under the Nvidia Source Code License-NC, which is an open license. The initial variants of MambaVision released in 2024 include the T and T2 variants, which were trained on the ImageNet-1K library. The new models released this week include the L/L2 and L3 variants, which are scaled-up models. “Since the initial release, we’ve significantly enhanced MambaVision, scaling it up to an impressive 740 million parameters,” Ali Hatamizadeh, Senior Research Scientist at Nvidia wrote in a Hugging Face discussion post. “We’ve also expanded our training approach by utilizing the larger ImageNet-21K dataset and have introduced native support for higher resolutions, now handling images at 256 and 512 pixels compared to the original 224 pixels.” According to Nvidia, the improved scale in the new MambaVision models also improves performance. Independent AI consultant Alex Fazio explained to VentureBeat that the new MambaVision models’ training on larger datasets makes them much better at handling more diverse and complex tasks. He noted that the new models include high-resolution variants perfect for detailed image analysis. Fazio said that the lineup has also expanded with advanced configurations offering more flexibility and scalability for different workloads. “In terms of benchmarks, the 2025 models are expected to outperform the 2024 ones because they generalize better across larger datasets and tasks, Fazio said. For enterprises building computer vision applications, MambaVision’s balance of performance and efficiency opens new possibilities Reduced inference costs: The improved throughput means lower GPU compute requirements for similar performance levels compared to Transformer-only models. Edge deployment potential: While still large, MambaVision’s architecture is more amenable to optimization for edge devices than pure Transformer approaches. Improved downstream task performance: The gains on complex tasks like object detection and segmentation translate directly to better performance for real-world applications like inventory management, quality control, and autonomous systems. Simplified deployment: NVIDIA has released MambaVision with Hugging Face integration, making implementation straightforward with just a few lines of code for both classification and feature extraction. MambaVision represents an opportunity for enterprises to deploy more efficient computer vision systems that maintain high accuracy. The model’s strong performance means that it can potentially serve as a versatile foundation for multiple computer vision applications across industries. MambaVision is still somewhat of an early effort, but it does represent a glimpse into the future of computer vision models. MambaVision highlights how architectural innovation—not just scale—continues to drive meaningful improvements in AI capabilities. Understanding these architectural advances is becoming increasingly crucial for technical decision-makers to make informed AI deployment choices. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Alibaba launches new open-source AI model for &#x27;cost-effective AI agents&#x27;</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20258:41 PM EDT</div>
                            <div class="article-description">Alibaba Cloud has launched its latest AI model in its &quot;Qwen series,&quot; as large language model competition in China continues to heat up. </div>
                            <div class="article-content"><p>Alibaba Cloud launched Thursday its latest AI model in its &quot;Qwen series,&quot; as large language model competition in China continues to heat up following the &quot;DeepSeek moment.&quot; The new &quot;Qwen2.5-Omni-7B&quot; is a multimodal model, which means it can process inputs, including text, images, audio and videos, while generating real-time text and natural speech responses, according to an announcement on Alibaba Cloud&#x27;s website. The company says that the model can be deployed on edge devices like mobile phones, offering high efficiency without compromising performance. &quot;This unique combination makes it the perfect foundation for developing agile, cost-effective AI agents that deliver tangible value, especially intelligent voice applications,&quot; Alibaba said. For example, it could be used to help a visually impaired person navigate their environment through real-time audio description, the company added. The new model is open-sourced on the platforms Hugging Face and Github, following a growing trend in China after DeepSeek made its breakthrough R1 model open-source. Open-source generally refers to software in which the source code is made freely available on the web for possible modification and redistribution. Over the past years, Alibaba Cloud says it has open-sourced over 200 generative AI models. Amid China&#x27;s AI fervor accelerated by DeepSeek, Alibaba and other generative AI competitors have been releasing new, cost-effective models and products at an unprecedented pace. Last week, Chinese tech giant Baidu released a new multimodal foundational model and its first reasoning-focused model. Alibaba, meanwhile, debuted its updated Qwen 2.5 artificial intelligence model in late January and released a new version of its AI assistant tool Quark earlier this month. The company has strongly committed to its AI strategy, announcing last month a plan to invest $53 billion in its cloud computing and AI infrastructure over the next three years, exceeding what it spent in the space over the past decade. Kai Wang, Asia senior equity analyst at Morningstar, told CNBC that large Chinese tech players such as Alibaba, which build data centers to meet the computing needs of AI in addition to building their own LLMs, are well positioned to benefit from China&#x27;s post-DeepSeek AI boom. Alibaba secured a major win for its AI business last month when it confirmed that the company was partnering with Apple to roll out AI integration for iPhones sold in China. On Wednesday, the group also reported an expanded strategic partnership with BMW to accelerate the integration of its AI into the carmaker&#x27;s next-generation intelligent vehicles.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Amazon is testing shopping, health assistants as it pushes deeper into generative AI</div>
                            <div class="article-source">CNBC - Published Tue, Mar 25 202510:04 PM EDT</div>
                            <div class="article-description">With CEO Andy Jassy pushing employees to build AI apps across the company, Amazon is testing new shopping and health assistants.</div>
                            <div class="article-content"><p>In this article Amazon, in an effort to infuse generative artificial intelligence across a wider swath of its e-commerce universe, recently began testing a shopping assistant and a health-focused chatbot with a subset of users. AI has become a major area of investment across Amazon, including in its retail, cloud computing, devices and health-care businesses. Within the retail business, Amazon has already launched a shopping chatbot, an AI assistant for sellers and AI shopping guides. The new services Amazon is testing appeared on its app or website in recent weeks. An Amazon spokesperson confirmed the features are being tested in beta with some customers. The shopping tool, called Interests AI, prompts users to describe an interest &quot;using your own words,&quot; and then it generates a curated selection of products. The feature lets consumers browse for products using more conversational language and is separate from the main search bar on Amazon&#x27;s website. Within its core app, Amazon has a landing page for the feature. &quot;Describe your interest, like &#x27;coffee brewing gadgets&#x27; or &#x27;latest pickleball accessories&#x27; — and we&#x27;ll find relevant products for you,&quot; the page says. Other suggested searches include &quot;children books about persistence and dealing with failure,&quot; and &quot;brain teasers that are not too hard, made out of wood or metal.&quot; The Amazon spokesperson said Interests uses large language models to translate everyday words or phrases into queries and attributes that traditional search engines can turn into product recommendations. It&#x27;s unclear what models Interests relies on. Amazon said in a blog post after publication of this article that it expects to make the feature available to all U.S. users in the coming months. Amazon CEO Andy Jassy said last month that employees have built or are in the process of building roughly 1,000 generative AI applications across the company. Its cloud unit offers a chatbot for businesses, called Q. In commerce, the company has rolled out services for consumers as well as its millions of third-party sellers. Amazon is also exploring ways that artificial intelligence can address medical needs. The company is testing a chatbot on its website and mobile app called &quot;Health AI,&quot; which can answer health and wellness questions, &quot;provide common care options for health care needs,&quot; and suggest products. While Rufus, Amazon&#x27;s shopping chatbot, can suggest products like ice packs and ibuprofen, Health AI goes further, providing users with medical guidance and care tips, such as how to deal with cold symptoms or the flu. The site says the service can&#x27;t provide personalized medical advice. Some responses feature a &quot;clinically verified&quot; badge, which denotes information that&#x27;s been &quot;reviewed by US-based licensed clinicians,&quot; Amazon says. Health AI also steers users to Amazon&#x27;s online pharmacy, along with clinical services offered by One Medical, the primary care provider it acquired for roughly $3.9 billion in 2022. Amazon&#x27;s spokesperson said the health assistant uses Bedrock, a service launched by Amazon&#x27;s cloud unit that accesses AI models from the company and third parties. &quot;We are collecting feedback from customers, and plan to introduce new features to enhance the experience in the future,&quot; the spokesperson said in a statement. More consumers are embracing generative AI as a shopping tool, and with features like Health AI and Interests AI, Amazon wants shoppers to use its own services over rivals like OpenAI&#x27;s ChatGPT. With enough use, Amazon could gain valuable insights on the ways that people are interacting with AI assistants as the company prepares to overhaul Alexa, the digital assistant it launched more than a decade ago. Amazon announced Alexa+, a new version of the technology embedded with generative AI, late last month. The company says that Alexa+, which has yet to roll out, is capable of handling more complex tasks and can serve as an &quot;agent&quot; by taking actions for users without their direct involvement. Andrew Bell, an Amazon e-commerce manager for the National Fire Protection Association who also publishes research on Amazon&#x27;s patent filings and AI development, came across the new shopping and health features and recently posted about them on LinkedIn. Bell said in an interview that Alexa+ could potentially draw upon models developed for Amazon applications like Health AI to answer queries. &quot;If there&#x27;s a health-related question, Alexa+ is going to maybe call on Health AI,&quot; Bell said. &quot;If there&#x27;s a product-related question, Alexa+ can call on Rufus.&quot; WATCH: Amazon&#x27;s SVP of Devices on Alexa+</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Google releases ‘most intelligent model to date,’ Gemini 2.5 Pro</div>
                            <div class="article-source">VentureBeat - March 25, 2025 1:17 PM</div>
                            <div class="article-description">Gemini 2.5 Pro is now available for Gemini Advanced users and is Google&#x27;s most capable model with a 1 million token context window.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Just a few months after releasing Gemini 2.0 and the rise of DeepSeek, Google announced its “most intelligent model” yet, Gemini 2.5, capable of reasoning and with better performance and accuracy. Gemini 2.5 comes three months after Google released its previously most intelligent model family, Gemini 2.0 which introduced reasoning and agentic use cases. This new model is available as Gemini 2.5 Pro (experimental) on Google’s AI Studio and for Gemini Advanced users on the Gemini chat interface. It will be available on Vertex AI soon. Koray Kavukcuoglu, CTO at Google DeepMind, said in a blog post that Gemini 2.5 represents the next step in Google’s goal of making “AI smarter and more capable of reasoning.” “Now, with Gemini 2.5, we’ve achieved a new level of performance by combining a significantly enhanced base model with improved post-training,” Kavukcuoglu wrote. “Going forward, we’re building these thinking capabilities directly into all of our models, so they can handle more complex problems and support even more capable, context-aware agents.” Like Gemini 2.0 and Gemini 2.0 Flash Thinking, Gemini 2.5 Pro “thinks” before it responds. The new model can handle multimodal input from text, audio, images, videos and large datasets. Gemini 2.5 Pro can also understand entire code repositories for coding projects. Gemini 2.5 Pro offers some of the largest context windows available for experimental models on Gemini. It ships with a 1 million token context window but will expand to 2 million tokens soon. Google AI Studio product manager Logan Kilpatrick posted on X that Gemini 2.5 Pro is “the first experimental model with higher rate limits + billing.” Google plans to release pricing for Gemini 2.5 models soon. Google said the model leads in advanced reasoning benchmark tests. The company said Gemini 2.5 Pro “leads in match and science benchmarks like GPQA and AIME 2025.” Kavukcuoglu said the model also scored “a state-of-the-art 18.8% across models without tool use on Humanity’s Last Exam,” a dataset aiming to capture human knowledge and reasoning. Gemini 2.5 Pro also performs strongly on coding tasks and scored better than Gemini 2.0 in specific benchmarks. Google noted the new model “excels at creating visually compelling web apps and agentic code applications, along with code transformation and editing.” Gemini 2.5 Pro enters the reasoning model fray in a significantly changed environment than Gemini 2.0 did in December. The release of DeepSeek’s reasoning large language model (LLM) DeepSeek-R1 showed that powerful models can perform well at a fraction of the training and compute cost. Furthermore, DeepSeek showed that open-source models can compete with more closed-source LLMs, such as OpenAI’s o1 and o3 models. Besides DeepSeek’s ever-expanding model offerings, Google has to compete with OpenAI’s reasoning models. While the newest model from OpenAI was GPT-4.5 —not a reasoning model—the company is still expected to develop more reasoning models soon. Gemini 2.5 is Google’s second new model this month. In March, the company released the latest version of its small language model, Gemma 3, which offered a 128,000 token context model and was best for use in on-the-go devices. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Providing consistent, reliable customer experiences with CQRS at scale</div>
                            <div class="article-source">VentureBeat - March 25, 2025 6:50 AM</div>
                            <div class="article-description">To maintain seamless customer experiences, Chase explains their transition to a modern, always-on ecosystem that scales with customer demand.</div>
                            <div class="article-content"><p>Presented by JPMorganChase As more Chase card customers embrace digital services, we’ve seen a surge in transaction-related inquiries. This increase has put pressure on our distributed backend systems to maintain seamless customer experiences, even during system outages at Systems of Record (SORs) or other layers. To address these challenges, we embarked on a modernization journey. Our goal was to transition from siloed environments to a modern, always-on ecosystem that scales with customer demand. This transformation allows customers to manage accounts, conduct transactions and access financial services through our online and mobile platforms without friction. The result? A significant technological advancement in distributed systems and big data, supporting Chase’s journey to unlock and accelerate new value for our business and customers. SORs which were predominantly mainframe-based and eventually evolved to modern technology stacks were designed to ensure reliability of command traffic. Data was ingested into data warehouses, the primary destination for most queries. With the emergence of real-time traffic through digital experiences, SORs began exposing their data to queries through APIs. Over time, the volume of query traffic grew significantly, often surpassing command traffic. Nowadays, it’s not uncommon for queries to constitute up to 90% of total SOR read volume. These strategic shifts have had profound effects on the cost, scalability and reliability of SORs, often contributing to operational issues. In our quest to mitigate operational issues posed by SORs and provide exceptional customer experiences, Chase adopted Command Query Responsibility Segregation (CQRS), a software architectural pattern that separates the responsibilities of handling commands (write operations) and queries (read operations) into distinct parts. Introduced by Greg Young around 2010, CQRS has gained significant traction in the software development community due to its ability to enhance scalability, performance and maintainability in complex systems. At Chase we built and implemented standards to achieve business continuity three to five times faster and improved customer experience, speed to market with new experiences and reliability. These standards include: The read layer in the CQRS pattern played a pivotal role in building a common data product by providing a unified and optimized view of data tailored to various user needs. By decoupling the read operations from the write processes, it allowed for the creation of specialized read models that can aggregate and present data from multiple sources, enabling consistent and efficient access to data across different applications and services. Embracing the challenge of adopting CQRS was immensely gratifying, and we successfully accomplished the following objectives on our journey: While CQRS offers numerous benefits it also introduces additional complexity, especially in terms of system design, implementation and operational overhead. We spent significant time carefully evaluating trade-offs, considering key factors such as eventual consistency, asynchronous communication and aligning with the principles of domain-driven design (DDD). Adopting CQRS has allowed us to operate at scale and with autonomy. It requires careful consideration of trade-offs and may not suit every project or team. As with any architectural pattern, understanding the principles and applying them judiciously is key to realizing the benefits of CQRS in software development. Like what you’re reading? about the Chase Product, Experience and Technology teams on Next at Chase. Amit Meshram is Executive Director, Principal Software Engineer at Chase.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Ex-Meta VP is raising back-to-back mega funding rounds from the same investors who made a fortune when Google bought Wiz</div>
                            <div class="article-source">Business Insider - 2025-03-24T20:57:48Z</div>
                            <div class="article-description">Roi Tiger, former vice-president of engineering at Meta, is in the process of closing a $55 million Series A funding round for a new cybersecurity startup.</div>
                            <div class="article-content"><p>How Twitter panic took down Silicon Valley Bank</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">‘Studio Ghibli’ AI image trend overwhelms OpenAI’s new GPT-4o feature, delaying free tier</div>
                            <div class="article-source">VentureBeat - March 26, 2025 4:14 PM</div>
                            <div class="article-description">The new feature has been widely embraced by users of X, but it raises copyright concerns and goes against Studio Ghibli&#x27;s creator.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More If you’ve been on the internet — or, at least, on the social network X — in the last day or so, you’ve likely come across colorful, smooth anime-style images of famous photographs rendered in the style of the Japanese studio, Studio Ghibli (the one that made Princess Mononoke, The Boy and the Crane, and My Neighbor Totoro, among many other classic animated films). In fact, some users are complaining because their feeds seem to be filled with nearly exclusively these types of images. Whether it’s current President Trump, the iconic image of the “Tank Man” during the 1989 pro-Democracy Tiananmen Square protests, Osama Bin Laden, Jeffrey Epstein, or even other pop culture moments and characters like Sam Rockwell’s iconic cameo on The White Lotus and many popular memes of yore, people have been making and sharing these images at a rapid clip. Much of that is thanks to OpenAI’s new update to the GPT-4o model behind ChatGPT for Pro, Plus, and Team subscription tiers, which turns on “native image generation.” While ChatGPT previously allowed users to create images from text prompts, it did so by routing them to another, separate OpenAI model, DALL-E 3. But OpenAI’s GPT-4o model is so named with an “o” because it is an “omni” model — the company trained it not only on text and code, but also on imagery and presumably, video and audio as well, allowing it to be able to understand all these forms of media and their similarities and differences, conceive of ideas across them (an “apple” is not just a word, but also something that can be drawn as a red or yellow or green fruit), and accurately produce said media given text prompts by a user without connecting to any external models. As a consequence, like rival Google AI Studio’s recent update to include a Gemini 2.0 Flash experimental image creation model, the new OpenAI GPT-4o can also accept image uploads of any pre-existing image in your camera roll or that you’ve screenshotted or saved off the web. First, navigate to Chat.com or ChatGPT.com and ensure you’re logged in with your ChatGPT Plus, Pro, or Team account and that the AI model selector (located in the left corner of the session window) is showing “GPT-4o” as the chosen model (you can click it to drop down and select the proper model between the available options). Once you do that, you can upload an image to ChatGPT using the “+” button in the lower left hand corner of the prompt entry text box, you can now ask the new GPT-4o with image creation model to render your pre-existing image in a new style. If you want, you can try it by uploading a photo of yourself and friends and typing “make all these people in the style of a Studio Ghibli animation.” And after a few seconds, it will do so with some pretty convincing and amusing results. It even supports attaching multiple images and combining them into a single piece. OpenAI initially said it would also enable this feature for free (non-paying users of ChatGPT), but unfortunately for them, co-founder and CEO Sam Altman today posted that the feature will be delayed due to the overwhelming demand by existing paying subscribers to ChatGPT Plus, Pro, and Team tiers. As he wrote on X: “images in chatgpt are wayyyy more popular than we expected (and we had pretty high expectations). rollout to our free tier is unfortunately going to be delayed for awhile.“ Meanwhile, those who do have access will likely continue cranking out image edits in this and other recognizable or novel styles. Of course, not everyone is a fan of OpenAI’s work here. In fact, Studio Ghibli creator Hayao Miyazaki himself appeared in a documentary back in 2016 — and one of the most memorable moments from it still referenced to this day is him reacting with overwhelming disgust and revulsion to an early example of AI-powered animation and physics by, you guessed it, an OpenAI model. As with many generative AI products and services, OpenAI’s training data for this new image generation capability remains under wraps, but is widely speculated to contain copyrighted material — and while imitating a style is generally not considered copyright infringement in the U.S., it is rubbing some fans of the original animation the wrong way. For now, those brands and enterprises looking to play with this style should do so with caution and after serious consideration, given the possible negative blowback among some users. But for those who are unabashedly pro-AI tools or with more forgiving and fun-loving fanbases, it’s clear that OpenAI has yet another hit on its hands. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Agentic AI is changing online meeting platforms: Moving from silent observer to active participant</div>
                            <div class="article-source">VentureBeat - March 25, 2025 6:00 AM</div>
                            <div class="article-description">Agentic AI is changing online meeting platforms and now thanks to Otter AI and other leading vendors it’s poised to change it even more.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Online meetings used to be much the same as their physical world counterparts. With the introduction of generative AI, online meeting platforms began to add new insights, including voice transcription services. Platform vendors, including Microsoft Teams, Google Meet and Cisco WebEx, have steadily integrated capabilities that go beyond what in-person physical meetings can provide. Now in the emerging era of agentic AI, online meetings are poised to diverge even further with a new wave of innovations. Zoom recently announced its agentic AI efforts which aims to create the paradigm shift from meetings to milestones. Microsoft has added copilot actions which can integrate with its Microsoft Teams service helping users inside of meetings to get insight and connect with other Microsoft services. Cisco has been steadily expanding its AI capabilities in Webex, and announced a Webex AI agent at the end of 2024 that helps with contact center deployments. Another firm that has been particularly active in the space is Otter AI, which is somewhat differentiated in that it is not directly tethered to any specific online meeting vendor platform. While Otter first made its mark as an AI-powered voice transcription service, it has added an AI assistant called Otter Pilot, an AI chat assistant, and a series of meeting capabilities known as Meeting GenAI. Today, Otter is going a step further, with its foray into agentic AI. While some of the agentic AI features that Otter is adding are not unique, it is doing at least one thing that isn’t yet part of every agentic AI meeting technology. Otter AI is now being integrated as an entity inside of a meeting that can actually respond by voice to queries. No longer is the AI an external participant accessible just via a chat window; AI is now a live entity that is literally part of the meeting. For the last several years, AI meeting assistants have been passive observers—transcribing conversations, creating summaries and allowing post-meeting queries. Otter is now changing this dynamic with its AI Meeting Agent, which can actively participate in conversations when summoned. “The new AI meeting agent we’re building will be able to help you with voice in real-time meetings,” Sam Liang, CEO of Otter AI told VentureBeat. “During the meeting, you can say, ‘Hey Otter,’ and ask it questions.” In a live demonstration with VentureBeat, Liang showed how the agent could answer factual questions, provide meeting summaries and even schedule follow-up meetings—all through voice commands during an active conversation. What makes this particularly powerful is the agent’s ability to connect to a company’s knowledge ecosystem. “This agent can become a domain expert,” Liang noted. “When you’re having a meeting, it has almost infinite knowledge from the internet, but this agent also has knowledge about your enterprise.” Taking agentic capability a step further, Otter is also launching an autonomous SDR (Sales Development Representative) agent that can independently conduct entire meetings without human intervention. This agent greets website visitors, conducts product demonstrations and schedules follow-up meetings with human sales representatives. “We cannot hire a million human agents to answer questions, but we built this Otter SDR agent that functions like a sales development representative who can greet every single visitor and give them a live demo,” Liang said. The use of chatbots and avatar-based systems is not new. Liang argued that what distinguishes his company’s technology from existing avatar-based solutions is its ability to conduct multimedia product demonstrations in real time. The autonomous agent can share screens, demonstrate product features and respond to specific questions about functionality and pricing. Agentic AI is an overloaded and somewhat overhyped term in the industry today overall. Functionally agentic AI is about enabling actions, which can be done by combining multiple models with a tool like LangChain or by using the function-calling capabilities present in many models. Liang has an even more nuanced definition for agentic AI. “Agents in general are a more sophisticated AI system that can break down a large and complicated task into smaller tasks,” he said. “It can do reasoning and it can do some planning to perform a task.” Otter is not using LangChain but has developed its own custom technology specifically designed for the challenges of multi-speaker voice environments. The technical architecture combines both public knowledge retrieval and proprietary enterprise information through a custom RAG (Retrieval-Augmented Generation) implementation. This enables the agent to understand company-specific information like employee names, project terminology and internal acronyms. What agentic AI is bringing to online meeting platforms represents a powerful new set of capabilities for organizational efficiency. For decades, organizational efficiency experts have warned about the risks of wasted time in meetings. Modern AI-powered platforms are changing that risk. Last week Zoom’s CTO told me that his goal was to move the technology from meetings to milestones, where the output of a meeting isn’t just another meeting but actionable things that will benefit the organization. While agentic AI can create workflows, there is still also benefit in regular AI assistants that are not actually agentic. There will still be standalone AI assistants and fully agentic ones and that’s a good thing, according to Anurag Dhingra, SVP &amp; GM, Enterprise Connectivity and Collaboration at Cisco. “While AI agents act as autonomous do-ers and AI assistants serve as prompted helpers, both offer benefits in boosting productivity and enhancing overall collaboration,” Dhingra told VentureBeat. “It’s not a matter of choosing one over the other but rather leveraging their combined strengths to create environments where teams can focus on innovation and strategic decision-making.” What is also starting to happen is more interoperability across different platforms. For example, Cisco’s AI Assistant will work with workflow applications such as Salesforce, ServiceNow and Outlook. For enterprises evaluating their AI adoption roadmap, Otter’s approach to meeting agents represents a strategic inflection point. Rather than implementing general-purpose AI and hoping for ROI, organizations should consider how domain-specific meeting agents can address concrete pain points with measurable impacts. As AI continues to evolve from tools we use, to colleagues we work with, the differentiation will increasingly be found not in the underlying models but in how effectively they’re trained to understand specific business contexts and domain knowledge. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">The concern with CoreWeave’s 250,000 Nvidia chips ahead of its IPO</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20255:00 PM EDT</div>
                            <div class="article-description">CoreWeave sells access to Nvidia graphics processors as a service, allowing developers to rent chips by the hour so they can develop more advanced AI models.</div>
                            <div class="article-content"><p>In this article With 250,000 highly-desired Nvidia graphics processors, CoreWeave has become one of the most prominent &quot;GPU clouds,&quot; a status it hopes investors will value when it debuts on the public markets. But the world of artificial intelligence hardware is moving so quickly that it raises questions about how long those chips will remain on the cutting edge and in demand. It&#x27;s a concern that could impact investor demand for shares of CoreWeave, one of the most anticipated IPOs in years. CoreWeave, which rents out remote access to computers based on Nvidia AI chips, said in a financial filing this month that most of its AI chips are from Nvidia&#x27;s Hopper generation. Those chips, such as the H100, were state-of-the-art in 2023 and 2024. They were scarce as AI companies bought or rented all the chips they could get in the wake of OpenAI ushering in the generative AI age with the release of ChatGPT in late 2022. But these days, Nvidia CEO Jensen Huang says that his company&#x27;s Hopper chips are getting blown out of the water by their successors – the Blackwell generation of GPUs, which have been shipping since late 2024. Hopper chips are &quot;fine&quot; for some circumstances but &quot;not many,&quot; Huang joked at Nvidia&#x27;s GTC conference last week. &quot;In a reasoning model, Blackwell is 40 times the performance of Hopper. Straight up. Pretty amazing,&quot; Huang said. &quot;I said before that when Blackwell starts shipping in volume, you couldn&#x27;t give Hoppers away.&quot; That&#x27;s great for Nvidia, which needs to find ways to keep selling chips to the companies committed to the AI race, but it&#x27;s bad news for GPU clouds like CoreWeave. That&#x27;s because the New Jersey company models the future trajectory of its business based on how much it anticipates being able to rent Nvidia chips out for over the next five to six years. Huang may have been kidding, but Nvidia spent much of its event detailing just how much better its Blackwell chips are. In Nvidia&#x27;s view, the best way to decrease the high cost of serving AI is by buying faster chips. Blackwell systems are in full production and shipping to customers, and Nvidia plans to introduce an upgraded version of Blackwell in late 2026. When new chips come out, the older chips — the kind CoreWeave has a quarter of a million of — go down in price, Huang said. So too does the price of renting them. Older chips don&#x27;t just stop working when new ones come out. Most companies, including CoreWeave, plan to use Hopper chips for six years. But Nvidia is telling customers that its newer, faster chips are capable of producing more AI content, which leads to more revenues at a better margin for clouds. An H100 would have to be priced 65% lower per hour than an Nvidia Blackwell GB200 NVL system for the two systems to be competitive in price per output to a renter. Put another way, the H100 would have to rent at 98 cents per hour to match the price per output of a Blackwell rack system priced at $2.20 per hour per GPU, SemiAnalysis estimated, speaking generally about AI rentals. H100s rented for as much as $8 per hour back in 2023 and often required long commitments and lead times, but now, usage of those chips can be summoned in minutes with a credit card. Some services now offer rented H100 access for under $2 per hour. The industry could be entering a period where the useful life of AI chips is reduced, Barclays analyst Ross Sandler wrote in a note on Friday. He was focused on hyperscalers — Meta, Google and Amazon — but the trend affects smaller cloud providers like CoreWeave, too. &quot;These assets are becoming obsolete at a much more rapid pace given how much innovation and speed improvements happen with each generation,&quot; Sandler wrote. This threatens company earnings if they end up depreciating older equipment faster, he said. CoreWeave says that if there were to be changes to the &quot;significant&quot; assumptions it makes about the useful lifetime of its AI infrastructure, it could hurt its business or future prospects. CoreWeave has also borrowed nearly $8 billion to buy Nvidia chips and build its data centers, sometimes using the GPUs it amassed as collateral. Analysts and investors are also increasingly asking questions about the useful lifespan of these new AI systems and whether their financial depreciation schedules should be accelerated because the technology is improving so fast. CoreWeave says in its filing that it seeks to offer state-of-the-art infrastructure and says it will continue spending to expand and improve its data centers. &quot;Part of this process entails cycling out outdated components of our infrastructure and replacing them with the latest technology available,&quot; the New Jersey company said. &quot;This requires us to make certain estimates with respect to the useful life of the components of our infrastructure and to maximize the value of the components of our infrastructure, including our GPUs, to the fullest extent possible.&quot; CoreWeave and Nvidia maintain a good relationship. CoreWeave will certainly buy more chips from Nvidia, which owns more than 5% of the New Jersey company. &quot;We&#x27;re super proud of them,&quot; Huang said last week. But Nvidia&#x27;s road map for releasing new chips that it proudly touts will make their predecessors obsolete is a threat to CoreWeave&#x27;s ambitions. WATCH: CoreWeave begins marketing IPO, targeting price range of $47-$55 per share: Report</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">U.S. blacklists over 50 Chinese companies in bid to curb Beijing&#x27;s AI, chip capabilities</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 202512:56 AM EDT</div>
                            <div class="article-description">The export restrictions come at a time when tensions between Washington and Beijing have been rising with the Trump administration ratcheting up tariffs against China.</div>
                            <div class="article-content"><p>The U.S. on Tuesday added dozens of Chinese tech companies to its export blacklist in its first such effort under the Donald Trump administration, as it doubles down on curtailing Beijing&#x27;s artificial intelligence and advanced computing capabilities. The U.S. Department of Commerce&#x27;s Bureau of Industry and Security added 80 organizations to an &quot;entity list,&quot; with more than 50 from China, barring American companies from supplying to those on the list without government permits. The companies were blacklisted for allegedly acting contrary to U.S. national security and foreign policy interests, the agency said, as part of its efforts to further restrict Beijing&#x27;s access to exascale computing tech, which can process vast amounts of data at very high speeds, as well as quantum technologies. Dozens of Chinese entities were targeted for their alleged involvement in developing advanced AI, supercomputers and high-performance AI chips for military purposes, the Commerce Department said, adding that two firms were supplying to sanctioned entities such as Huawei and its affiliated chipmaker HiSilicon. It blacklisted 27 Chinese entities for acquiring U.S.-origin items to support China&#x27;s military modernization and seven firms for helping advance China&#x27;s quantum technology capabilities. Among the organizations in the &quot;entity list&quot; were also six subsidiaries of Chinese cloud-computing firm Inspur Group, which had been blacklisted by the Joe Biden administration in 2023. China&#x27;s foreign ministry said late Wednesday it &quot;strongly condemns&quot; the export restrictions while urging the U.S. to &quot;stop generalizing national security,&quot; Reuters reported. The latest additions &quot;cast an ever-widening net aimed at third countries, transit points and intermediaries,&quot; said Alex Capri, a senior lecturer at National University of Singapore and author of &quot;Techno-Nationalism: How It&#x27;s Reshaping Trade, Geopolitics and Society.&quot; Chinese firms have managed to gain access to U.S. strategic dual-use technologies via certain third parties, he said, referring to loopholes that have allowed Chinese companies access to U.S. technologies despite restrictions. &quot;U.S. officials will continue to step up tracking and tracing operations aimed at the smuggling of advanced semiconductors made by Nvidia and Advanced Micro Devices,&quot; he said. The expanded export restrictions come at a time when tensions between Washington and Beijing have been rising with the Trump administration ratcheting up tariffs against China. The rapid rise of Chinese AI startup DeepSeek has boosted the adoption of open-source low-cost AI models in China, putting pressure on leading U.S. competitors with higher-cost, proprietary models. The Biden administration imposed sweeping export controls against China, encompassing everything from semiconductors to supercomputers under the so-called &quot;small yard, high fence&quot; policy. The approach aims to place restrictions on a small number of technologies with significant military potential while maintaining normal economic exchange in other areas. Under Secretary of Commerce for Industry and Security Jeffrey I. Kessler said the agency was &quot;sending a clear, resounding message&quot; that the Trump administration will prevent U.S. technologies from &quot;being misused for high performance computing, hypersonic missiles, military aircraft training, and UAVs (unmanned aerial vehicle) that threaten our national security.&quot; &quot;The entity list is one of many powerful tools at our disposal to identify and cut off foreign adversaries seeking to exploit American technology for malign purposes,&quot; he added. Inspur Group and Huawei did not immediately respond to CNBC&#x27;s requests for comment.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Trust Wallet Reaches 200 Million Downloads Milestone</div>
                            <div class="article-source">VentureBeat - March 26, 2025 9:25 AM</div>
                            <div class="article-description">Press Release Trust Wallet, the world’s leading self-custody Web3 wallet, has surpassed 200 million total downloads, marking a game-changing milestone in the industry. Trust Wallet stands as the most widely used non-custodial wallet globally for onchain users, cementing…</div>
                            <div class="article-content"><p>With this milestone, Trust Wallet cements its position as the #1 crypto wallet  DUBAI, United Arab Emirates–(BUSINESS WIRE)–March 26, 2025– Trust Wallet, the world’s leading self-custody Web3 wallet, has surpassed 200 million total downloads, marking a game-changing milestone in the industry. Trust Wallet stands as the most widely used non-custodial wallet globally for onchain users, cementing its role as a key gateway to Web3. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20250325066589/en/ Trust Wallet Reaches 200 Million Downloads Milestone Since its launch in 2017, Trust Wallet has played a pivotal role in onboarding millions into crypto. Initially introduced as an Ethereum wallet, it has evolved into a chain-agnostic, multi-chain Web3 hub, now supporting over 10 millions assets across 100+ blockchains, along with a suite of features that empower users to navigate their entire Web3 journey-from buying their first cryptocurrency to swapping, staking, exploring the decentralized web, and beyond. Eowyn Chen, CEO of Trust Wallet, commented on the achievement: “Reaching 200 million downloads is a real testament to the trust from the users. In a rapidly evolving industry, our mission has remained the same: empower people with freedom to own and access opportunities. We’re proud of this milestone, but even more humbled and excited about the future as we have many things on the roadmap for our global community. We got to work harder.” Trust Wallet has carved out a significant space for itself in the competitive landscape of cryptocurrency wallets. This success can be attributed to a combination of core principles that focus on user experience, community, trust and security. What’s Fuelling Trust Wallet’s Growth? With millions of users worldwide and a fast-growing community, Trust Wallet continues to expand its reach through compelling features, product innovations, and user-centric initiatives. Its recent growth and success points to a relentless focus on usability, innovation, and security. The wallet strikes a balance between onboarding new users and offering advanced tools for experienced users. Examples of Trust Wallet’s innovations include: Building a Future-Proof Web3: Trust Wallet’s Vision and Beyond As the on-chain economy evolves and AI-driven innovations take shape, Trust Wallet is focused on bridging the gap between Web2 simplicity and Web3 autonomy. The goal is to make decentralized finance (DeFi) and digital ownership more intuitive, secure, and accessible for millions of users. Web3 isn’t just about holding assets-it’s about seamless, intelligent, and secure interactions across decentralized applications (dApps), finance, gaming, and beyond. Trust Wallet continues to expand its capabilities to give users the tools and insights needed to navigate the decentralized world with confidence. Key Focus Areas for 2025: By improving usability, security, and intelligence, Trust Wallet is ensuring that more people can explore and benefit from the decentralized economy with confidence. About Trust Wallet Trust Wallet is the secure, self-custody Web3 wallet and gateway for people who want to fully own, control, and leverage the power of their digital assets. From beginners to experienced users, Trust Wallet makes it easier, safer, and convenient for millions of people around the world to experience Web3, access dApps securely, store and manage their crypto and NFTs, as well as buy, sell, and stake crypto to earn rewards – all in one place and without limits.  View source version on businesswire.com: https://www.businesswire.com/news/home/20250325066589/en/ For media enquiries, contact:press@trustwallet.com If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Meet a decades-old software company hitching a ride on the Nvidia rocket ship</div>
                            <div class="article-source">Business Insider - 2025-03-26T09:00:01Z</div>
                            <div class="article-description">DDN was invited on the rocket ship that is Nvidia just a few years ago — and everything changed.</div>
                            <div class="article-content"><p>Volkswagen is using AI to speed up and scale marketing, while also integrating ChatGPT into its vehicles, says CMO Susanne Franz</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Former Intel CEO Pat Gelsinger jumps to venture capital, joins Playground Global</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20257:00 PM EDT</div>
                            <div class="article-description">After retiring as Intel&#x27;s CEO in December, Pat Gelsinger is becoming a general partner at early-stage venture firm Playground Global.</div>
                            <div class="article-content"><p>In this article After a tumultuous four years running Intel, Pat Gelsinger is going into venture capital. Gelsinger, who was ousted by the chipmaker in December, has joined Playground Global as a general partner. Started in 2015 by a group that included Android founder Andy Rubin, Playground focuses on early-stage investments in deep technology. Gelsinger told CNBC in an interview that he considered starting a venture firm with someone else, but opted to go with a structure that was already up and running. &quot;It&#x27;s about scale,&quot; Gelsinger said, adding that starting from scratch would require &quot;10 hard years to get it.&quot; Before joining Playground, Gelsinger made a handful of private investments in startups including church outreach software startup Gloo, wearable maker Oura and artificial intelligence chip developer Fractile. At Playground, he&#x27;ll join the board of portfolio company xLight, which is developing lasers for semiconductor manufacturing. Gelsinger is entering VC after 45 years in the technology industry. He spent three decades at Intel, becoming its first chief technology officer, and left in 2009 for data center hardware maker EMC. He later led server virtualization company VMware. In 2021, with Intel struggling from delays in releasing new generations of processors, he rejoined the company as CEO. Under Gelsinger, Intel focused on semiconductor fabrication, pouring money into an effort to develop chips for other companies. In 2024, the Biden administration awarded Intel up to $8.5 billion in CHIPS and Science Act funding as part of a plan to bring chip manufacturing back to the U.S. But Intel lost market share and got trounced by Nvidia in AI, prompting a massive selloff in its stock price. Intel&#x27;s market cap plummeted by 60% in 2024, its worst performance in over five decades as a public company. In December, Intel announced Gelsinger&#x27;s retirement. Earlier this month, the company said Lip-Bu Tan, a former CEO of Cadence Design Systems, will take over as CEO. Gelsinger isn&#x27;t the first ex-Intel CEO to find his way to venture. His predecessor, Bob Swan, became a growth operating partner at venture firm Andreessen Horowitz in 2021, a few months after leaving the chipmaker. Gelsinger said he&#x27;s looking forward to seeing this week&#x27;s stock market debut of CoreWeave, which rents out Nvidia graphics processing units (GPUs) to Microsoft, Nvidia and OpenAI. &quot;Obviously they&#x27;ve been able to ride the wave of at-scale data centers for AI computing,&quot; Gelsinger said. &quot;There are multiple participants who are trying to do it. They did the best in that. The question is, what&#x27;s their sustainable differentiation?&quot; Another technology of interest, Gelsinger said, is quantum computing. Unlike classical computers that store data in bits that are either on or off, quantum computers operate with quantum qubits, or qubits, that can be in both states at the same time. The hope among quantum bulls is that the technology might be able to perform certain calculations that have stymied today&#x27;s machines. Amazon and Microsoft have both had their latest claims published in the journal Nature. Gelsinger said he looks forward to working on quantum computing with PsiQuantum, a Playground portfolio company. PsiQuantum is raising $750 million or more in fresh capital at a $6 billion valuation, with BlackRock planning to lead the round, CNBC confirmed. Reuters reported about the fundraising efforts on Monday. Quantum computers will be &quot;materially impacting computing structures before the end of this decade,&quot; Gelsinger said. In February, the U.S. Defense Advanced Research Projects Agency (DARPA) said it will evaluate whether quantum systems from PsiQuantum and Microsoft will be more valuable than they cost by 2033. The release didn&#x27;t mention Intel, which announced its inaugural quantum chip, codenamed Tunnel Falls, in 2023. Gelsinger said he wishes the best to his former employer and Tan, its new leader. &quot;I certainly believe that Intel is critical for the semiconductor industry,&quot; he said. &quot;You need to design and manufacture leading-edge technology.&quot; — CNBC&#x27;s Kate Rooney contributed to this report. WATCH: Intel shares fall after CEO leaves</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Googlers grilled leaders about smaller pay bumps in a recent all-hands</div>
                            <div class="article-source">Business Insider - 2025-03-26T23:12:06Z</div>
                            <div class="article-description">At a recent Google all-hands meeting, employees question leadership on 2025 compensation packages.</div>
                            <div class="article-content"><p>How tech layoffs could affect the economy</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Trump says he may reduce China tariffs to help close a TikTok deal</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20257:06 PM EDT</div>
                            <div class="article-description">&quot;Maybe I&#x27;ll give them a little reduction in tariffs or something to get it done,&quot; President Donald Trump said about a deal involving TikTok&#x27;s U.S. operations</div>
                            <div class="article-content"><p>President Donald Trump said he may reduce tariffs on China to facilitate a deal that would result in ByteDance selling the U.S. operations of TikTok. China &quot;is going to have to play a role&quot; in approving a TikTok-related divestiture, Trump said in a press conference Wednesday. &quot;Maybe I&#x27;ll give them a little reduction in tariffs or something to get it done,&quot; Trump said. &quot;TikTok is big, but every point in tariffs is worth more than TikTok.&quot; Although a national security law requires ByteDance to divest TikTok&#x27;s U.S. operations or face an effective ban in the country, Trump in January signed an executive order that delayed the deadline for a deal to April 5. Trump has previously said that he wants the U.S. to maintain a 50% ownership position in TikTok via a joint venture. It&#x27;s possible he will extend the TikTok deadline again, Trump said Wednesday. &quot;We&#x27;re going to have a form of a deal, but if it&#x27;s not finished, it&#x27;s not a big deal,&quot; Trump said. &quot;We&#x27;ll just extend it.&quot; Vice President JD Vance told NBC News earlier this month that he was confident that a TikTok-related deal would happen by the April deadline. &quot;There will almost certainly be a high-level agreement that I think satisfies our national security concerns, allows there to be a distinct American TikTok enterprise,&quot; Vance said. WATCH: TikTok bid is &#x27;in active dialogue&#x27; with Trump administration.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Former Intel CEO Pat Gelsinger jumps to venture capital, joins Playground Global</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20257:00 PM EDT</div>
                            <div class="article-description">After retiring as Intel&#x27;s CEO in December, Pat Gelsinger is becoming a general partner at early-stage venture firm Playground Global.</div>
                            <div class="article-content"><p>In this article After a tumultuous four years running Intel, Pat Gelsinger is going into venture capital. Gelsinger, who was ousted by the chipmaker in December, has joined Playground Global as a general partner. Started in 2015 by a group that included Android founder Andy Rubin, Playground focuses on early-stage investments in deep technology. Gelsinger told CNBC in an interview that he considered starting a venture firm with someone else, but opted to go with a structure that was already up and running. &quot;It&#x27;s about scale,&quot; Gelsinger said, adding that starting from scratch would require &quot;10 hard years to get it.&quot; Before joining Playground, Gelsinger made a handful of private investments in startups including church outreach software startup Gloo, wearable maker Oura and artificial intelligence chip developer Fractile. At Playground, he&#x27;ll join the board of portfolio company xLight, which is developing lasers for semiconductor manufacturing. Gelsinger is entering VC after 45 years in the technology industry. He spent three decades at Intel, becoming its first chief technology officer, and left in 2009 for data center hardware maker EMC. He later led server virtualization company VMware. In 2021, with Intel struggling from delays in releasing new generations of processors, he rejoined the company as CEO. Under Gelsinger, Intel focused on semiconductor fabrication, pouring money into an effort to develop chips for other companies. In 2024, the Biden administration awarded Intel up to $8.5 billion in CHIPS and Science Act funding as part of a plan to bring chip manufacturing back to the U.S. But Intel lost market share and got trounced by Nvidia in AI, prompting a massive selloff in its stock price. Intel&#x27;s market cap plummeted by 60% in 2024, its worst performance in over five decades as a public company. In December, Intel announced Gelsinger&#x27;s retirement. Earlier this month, the company said Lip-Bu Tan, a former CEO of Cadence Design Systems, will take over as CEO. Gelsinger isn&#x27;t the first ex-Intel CEO to find his way to venture. His predecessor, Bob Swan, became a growth operating partner at venture firm Andreessen Horowitz in 2021, a few months after leaving the chipmaker. Gelsinger said he&#x27;s looking forward to seeing this week&#x27;s stock market debut of CoreWeave, which rents out Nvidia graphics processing units (GPUs) to Microsoft, Nvidia and OpenAI. &quot;Obviously they&#x27;ve been able to ride the wave of at-scale data centers for AI computing,&quot; Gelsinger said. &quot;There are multiple participants who are trying to do it. They did the best in that. The question is, what&#x27;s their sustainable differentiation?&quot; Another technology of interest, Gelsinger said, is quantum computing. Unlike classical computers that store data in bits that are either on or off, quantum computers operate with quantum qubits, or qubits, that can be in both states at the same time. The hope among quantum bulls is that the technology might be able to perform certain calculations that have stymied today&#x27;s machines. Amazon and Microsoft have both had their latest claims published in the journal Nature. Gelsinger said he looks forward to working on quantum computing with PsiQuantum, a Playground portfolio company. PsiQuantum is raising $750 million or more in fresh capital at a $6 billion valuation, with BlackRock planning to lead the round, CNBC confirmed. Reuters reported about the fundraising efforts on Monday. Quantum computers will be &quot;materially impacting computing structures before the end of this decade,&quot; Gelsinger said. In February, the U.S. Defense Advanced Research Projects Agency (DARPA) said it will evaluate whether quantum systems from PsiQuantum and Microsoft will be more valuable than they cost by 2033. The release didn&#x27;t mention Intel, which announced its inaugural quantum chip, codenamed Tunnel Falls, in 2023. Gelsinger said he wishes the best to his former employer and Tan, its new leader. &quot;I certainly believe that Intel is critical for the semiconductor industry,&quot; he said. &quot;You need to design and manufacture leading-edge technology.&quot; — CNBC&#x27;s Kate Rooney contributed to this report. WATCH: Intel shares fall after CEO leaves</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">23andMe co-founder lashes out at CEO Wojcicki after bankruptcy filing, says board lacked oversight</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20255:24 PM EDT</div>
                            <div class="article-description">23andMe co-founder Linda Avey expressed frustration about the fall of the genetic testing company in a social media post.</div>
                            <div class="article-content"><p>In this article 23andMe co-founder Linda Avey took to social media on Wednesday to express frustration about the fate of the once-thriving genetic testing company that spiraled into Chapter 11 bankruptcy protection this week. Avey helped launch 23andMe in 2006 alongside Paul Cusenza and Anne Wojcicki, who resigned as CEO on Friday. The company went mainstream due to its popular at-home DNA testing kits, but struggled in recent years to generate recurring revenue, stand up viable therapeutics and research businesses and assuage privacy concerns. &quot;My time at the company was cut short in 2009, when my co-founder Anne convinced the board that she should run the company,&quot; Avey wrote in a post on social media site X. &quot;And I must be honest, I was frustrated with the direction the company took after that point.&quot; 23andMe, which reached a peak market cap of about $6 billion, was worth around $14 million as of market close on Wednesday. &quot;Without continued consumer-focused product development, and without governance, 23andMe lost its way, and society missed a key opportunity in furthering the idea of personalized health,&quot; Avey wrote. Last March, 23andMe&#x27;s independent directors formed a special committee to evaluate the company&#x27;s potential paths forward. All seven members resigned from the board in September and said they disagreed with Wojcicki about the &quot;strategic direction for the company.&quot; &quot;After my departure, she architected a majority vote for herself that eliminated board governance, even as it expanded over the following funding rounds,&quot; Avey said. &quot;For better or worse, the buck stopped with her. It came as no surprise when the board resigned last year.&quot; Wojcicki submitted multiple proposals to take the company private herself, but all were rejected, even after the company appointed new board members. The special committee &quot;unanimously determined to reject&quot; Wojcicki&#x27;s most recent proposal earlier this month. If 23andMe&#x27;s Chapter 11 plan is approved by the court, the company will &quot;actively solicit qualified bids&quot; over a 45-day process. Wojcicki still plans to pursue the company as an independent bidder, she said in a post on X on Monday. &quot;There are many cautionary tales buried in the 23andMe story,&quot; Avey said. &quot;Striking a balance between the desire for founder control and board oversight is essential; otherwise, why have a board at all?&quot; 23andMe did not immediately respond to CNBC&#x27;s request for comment. WATCH: The rise and fall of 23andMe</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Trump says Tesla CEO Elon Musk didn’t advise on auto tariffs &#x27;because he may have a conflict&#x27;</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20257:31 PM EDT</div>
                            <div class="article-description">When asked by reporters whether the new tariffs would be good for Musk&#x27;s autos business, Tesla, President Donald Trump said they may be &quot;net neutral or they may be good.&quot;</div>
                            <div class="article-content"><p>In this article After President Donald Trump said on Wednesday he would impose 25% tariffs on &quot;all cars that are not made in the United States,&quot; he said his key advisor, Tesla CEO Elon Musk, had not weighed in on the matter, &quot;because he may have a conflict.&quot; He added that Musk had never &quot;asked me for a favor in business whatsoever.&quot; Musk serves as a senior advisor to Trump, having earlier contributed $290 million to propel him back to the White House. While Musk remains at the helm of his companies, including SpaceX and Tesla, he is also leading the Department of Government Efficiency (DOGE), which is an effort to slash federal government spending, personnel and consolidate or eliminate various federal agencies and services. Earlier this month, Trump turned the South Lawn of the White House into a temporary Tesla showroom. The company delivered five of its electric vehicles there for the president to inspect after he had declared, in a post on Truth Social, that he would buy a Tesla to show support for Musk and the business. Musk stood by his side while Trump called the vehicles &quot;beautiful&quot; and praised the unorthodox design of the angular, steel Tesla Cybertruck. When asked by reporters whether the new tariffs would be good for Musk&#x27;s autos business, Tesla, Trump said they may be &quot;net neutral or they may be good.&quot; He pointed to Tesla&#x27;s vehicle assembly plants in Austin, Texas and Fremont, California and opined that, &quot;anybody that has plants in the United States — it&#x27;s going to be good for them.&quot; Tesla recently wrote, in a letter to the U.S. Trade Representative, that &quot;even with aggressive localization&quot; of its supply chain domestically, &quot;certain parts and components are difficult or impossible to source within the United States.&quot; The company urged the USTR to &quot;consider the downstream impacts of certain proposed actions taken to address unfair trade practices.&quot; Tesla and other automakers commonly buy headlamps, automotive glass, brakes, body panels, suspension parts, and printed circuit boards for various electrical systems in their vehicles from foreign suppliers in Mexico, Canada and China, especially. Musk and Tesla did not immediately respond to a request for comment about how the new 25% tariffs may impact their business. Tesla faces an onslaught of competition with more automakers selling fully electric models than ever before. However, the company&#x27;s most formidable rival in battery electric vehicles, BYD in China, has never been authorized to sell its electric cars in the United States. Domestic automakers including General Motors, Ford, Rivian and Tesla saw shares declining slightly after hours following the latest tariffs announcement.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Groq and PlayAI just made voice AI sound way more human — here’s how</div>
                            <div class="article-source">VentureBeat - March 26, 2025 8:30 AM</div>
                            <div class="article-description">Groq partners with PlayAI to deliver Dialog, an emotionally intelligent text-to-speech model that runs 10x faster than real-time speech, including the Middle East&#x27;s first Arabic voice AI model.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Groq and PlayAI announced a partnership today to bring Dialog, an advanced text-to-speech model, to market through Groq’s high-speed inference platform. The partnership combines PlayAI’s expertise in voice AI with Groq’s specialized processing infrastructure, creating what the companies claim is one of the most natural-sounding and responsive text-to-speech systems available. “Groq provides a complete, low latency system for automatic speech recognition (ASR), GenAI, and text-to-speech, all in one place,” said Ian Andrews, Chief Revenue Officer at Groq, in an exclusive interview with VentureBeat. “With Dialog now running on GroqCloud, this means customers won’t have to use multiple providers for a single use case — Groq is a one stop solution.” Dialog is notable for being available in both English and Arabic, with the Arabic version representing the first voice AI specifically designed for the Middle East region. The inclusion of Arabic as one of the initial offerings was strategic for both companies. “Arabic is the fourth most spoken language globally — by partnering with PlayAI to offer an Arabic TTS model, Groq is unlocking a key global market and enabling broader access to fast AI inference,” Andrews told VentureBeat. The companies claim their solution addresses key shortcomings in existing voice AI technologies, particularly around natural speech patterns and response speed. According to benchmark testing conducted by third-party evaluator Podonos, Dialog was preferred by users at a rate of 10:1 versus ElevenLabs v2.5 Turbo and over 3:1 against ElevenLabs Multilingual v2.0. What sets Dialog apart is its sophisticated approach to context. Rather than treating each vocalization as an isolated event, the system maintains awareness of the entire conversation flow. “We built a novel architecture that we call an ‘adaptive speech contextualizer‘ (ASC), which allows the model to use the full context and history of a conversation,” said Mahmoud Felfel, co-founder and CEO of PlayAI, in an interview with VentureBeat. “This means that every response isn’t just a standalone output; it’s enriched with appropriate prosody, tone, and emotion that reflect the flow of the conversation.” For enterprises looking to implement conversational AI, latency — the delay between request and response — has been a persistent challenge. Groq’s specialized Language Processing Units (LPUs) appear to provide a significant advantage in this area. “Based on initial internal testing, Groq is delivering up to 140 characters per second on PlayAI’s Dialog model, a significant boost compared to the same model running on GPUs at 86 characters per second,” explained Andrews. “That means that Dialog generates text up to 10 times faster than real-time.” The partnership comes at a time of significant expansion for Groq, which recently secured a $1.5 billion commitment from Saudi Arabia to fund additional infrastructure. The company has established a data center in Dammam, which it describes as “the region’s largest inference cluster.” “Partnering with Groq was a no-brainer; they’re the industry leader in advanced AI inference infrastructure,” said Felfel. “With TTS and agents, low latency is key. We’ve already optimized Dialog for these real-time applications, but partnering with Groq allows us to deliver the lowest latency voice model on the market.” The voice AI market has seen rapid growth as businesses look to automate customer interactions while maintaining a natural, human-like experience. Applications range from customer service and sales automation to voice-overs and accessibility features for the visually impaired. “Beyond customer service, other enterprise use cases include automating sales and appointment scheduling, on-boarding and personal assistants, creating voice overs to existing content, translating English audio and video content into Arabic, increasing website and static content accessibility for the visually impaired, and more,” Andrews said. For PlayAI, which was founded by entrepreneurs from the Middle East and North Africa region, the inclusion of Arabic language capabilities was particularly meaningful. “As MENA founders, we know the region is heavily investing in AI capabilities and infrastructure as inflected in investments like Groq, but also world-leading adoption,” said Felfel. “Arabic is a global business language and one that we grew up speaking, so it was a natural choice as one of our core languages.” The companies have made the Dialog technology available through GroqCloud’s tiered service model, which includes both free and paid options. This approach allows developers to experiment with the technology before committing to larger implementations. “GroqCloud offers both free and paid plans. Anyone can create an account and create an API code for free,” Andrews explained. “Our paid Developer Tier is self-serve, meaning anyone with a credit card can sign up themselves.” As voice becomes an increasingly important interface for AI systems, this partnership positions both companies to capitalize on the growing demand for more natural and responsive conversational experiences. By addressing the technical challenges of latency and natural speech patterns, Groq and PlayAI may have removed significant barriers to wider adoption of voice AI in enterprise settings. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Powerful Medical Receives FDA Breakthrough Device Designation for PMcardio STEMI AI ECG Model</div>
                            <div class="article-source">VentureBeat - March 25, 2025 10:25 AM</div>
                            <div class="article-description">Press Release Powerful Medical, a leader in AI-driven cardiovascular diagnostics, announces that its PMcardio STEMI AI ECG model has been granted Breakthrough Device Designation by the US Food and Drug Administration (FDA). This designation recognizes PMcardio as a…</div>
                            <div class="article-content"><p>NEW YORK–(BUSINESS WIRE)–March 25, 2025– Powerful Medical, a leader in AI-driven cardiovascular diagnostics, announces that its PMcardio STEMI AI ECG model has been granted Breakthrough Device Designation by the US Food and Drug Administration (FDA). This designation recognizes PMcardio as a breakthrough technology for the detection of ST-elevation myocardial infarction (STEMI) and STEMI equivalents-a life-threatening cardiac condition requiring immediate intervention. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20250325333487/en/ Powerful Medical Receives FDA Breakthrough Device Designation for PMcardio STEMI AI ECG Model Every 40 seconds, someone in the US suffers from a heart attack¹, where rapid diagnosis is crucial to saving heart muscle and preventing long-term damage, often leading to higher mortality rates. The ECG remains the primary tool for rapid diagnostics, yet the standard of care often fails to detect heart attacks accurately and timely, resulting in critical delays in treatment. PMcardio is the only solution capable of detecting both STEMI and STEMI equivalents on the ECG-aligning with the emerging emphasis on a paradigm shift towards Occlusion Myocardial Infarction (OMI) diagnosis and bridging a vital gap in early, precise heart attack diagnosis. “For the last 20 years, life-saving treatment exists for heart attack patients, yet far too many still don’t receive the urgent care they need due to delays in diagnosis and inefficient triage,” said Robert Herman, MD, PhD, Chief Medical Officer of Powerful Medical. This is especially critical in settings where immediate specialist evaluation isn’t available-only 17% of patients presenting to rural centers make it to the catheterization lab in time for intervention.² Dr. Herman added, “By equipping physicians and allied providers with an AI-powered tool for accurate and immediate STEMI detection, available around the clock, we can bridge this gap, ensure timely treatment, and improve patient outcomes, often preventing avoidable deaths”. The FDA’s Breakthrough Device Designation provides PMcardio with an expedited review process and close collaboration with the agency on its path toward market authorization. This designation is reserved for technologies that offer significant advantages over existing solutions and address unmet medical needs. This recognition underscores the FDA’s acknowledgment of Powerful Medical’s STEMI AI ECG Model, dubbed “Queen of Hearts”, to set a new standard in frontline heart attack detection and triage, ultimately enhancing care quality, accelerating treatment decisions, and saving lives through earlier and more accurate diagnosis. “FDA Breakthrough Device Designation is a pivotal milestone in our effort to revolutionize heart attack detection and ensure every patient receives immediate, life-saving care,” said Felix Bauer, COO of Powerful Medical. “We’re committed to bringing this life-saving technology to the US, the largest healthcare market in the world. This recognition by the FDA validates the impact of our innovation and brings us closer to transforming emergency cardiac care on a global scale,” added Martin Herman, CEO. With this designation, Powerful Medical not only works closely with the FDA on market approval but also gains improved access to CMS reimbursement mechanisms to bring the PMcardio STEMI AI ECG Model to healthcare providers nationwide, serving US public health. About Powerful Medical Powerful Medical is a pioneering health technology company specializing in AI-driven cardiovascular diagnostics. Its flagship product, PMcardio, harnesses AI to enhance ECG interpretation, streamline patient triage, and support clinical decision-making. With a mission to bridge the gap between innovation and clinical practice, Powerful Medical is committed to ensuring every patient receives the highest standard of care. References   View source version on businesswire.com: https://www.businesswire.com/news/home/20250325333487/en/ Lucia Bojkovskapr@powerfulmedical.com If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">METASCALE improves LLM reasoning with adaptive strategies</div>
                            <div class="article-source">VentureBeat - March 25, 2025 3:14 PM</div>
                            <div class="article-description">METASCALE uses a three-stage approach to dynamically choose the right reasoning technique for each promblem.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More A new framework called METASCALE enables large language models (LLMs) to dynamically adapt their reasoning mode at inference time. This framework addresses one of LLMs’ shortcomings, which is using the same reasoning strategy for all types of problems. Introduced in a paper by researchers at the University of California, Davis, the University of Southern California and Microsoft Research, METASCALE uses “meta-thoughts”—adaptive thinking strategies tailored to each task—to improve LLM performance and generalization across various tasks. This approach can offer enterprises a way to enhance the accuracy and efficiency of their LLM applications without changing models or engaging in expensive fine-tuning efforts. One of the main challenges of LLM applications is their fixed and inflexible reasoning behavior. Unlike humans, who can consciously choose different approaches to solve problems, LLMs often rely on pattern matching from their training data, which may not always align with sound reasoning principles that humans use. Current methods for adjusting the reasoning process of LLMs, such as chain-of-thought (CoT) prompting, self-verification and reverse thinking, are often designed for specific tasks, limiting their adaptability and effectiveness across diverse scenarios. The researchers point out that “these approaches impose fixed thinking structures rather than enabling LLMs to adaptively determine the most effective task-specific strategy, potentially limiting their performance.” To address this limitation, the researchers propose the concept of “meta-thinking.” This process allows LLMs to reflect on their approach before generating a response. Meta-thoughts guide the reasoning process through two components inspired by human cognition: Cognitive mindset: The perspective, expertise, or role the model adopts to approach the task. Problem-solving strategy: A structured pattern used to formulate a solution for the task based on the chosen mindset. Instead of directly tackling a problem, the LLM first determines how to think, selecting the most appropriate cognitive strategy. For example, when faced with a complex software problem, the LLM might first think about the kind of professional who would solve it (e.g., a software engineer) and choose a strategy to approach the problem (e.g., using design patterns to break down the problem or using a micro-services approach to simplify the deployment). “By incorporating this meta-thinking step, LLMs can dynamically adapt their reasoning process to different tasks, rather than relying on rigid, predefined heuristics,” the researchers write. Building upon meta-thoughts, the researchers introduce METASCALE, a test-time framework that can be applied to any model through prompt engineering. “The goal is to enable LLMs to explore different thinking strategies, and generate the most effective response for a given input,” they state. METASCALE operates in three phases: Initialization: METASCALE generates a diverse pool of reasoning strategies based on the input prompt. It does this by prompting the LLM to self-compose strategies and leveraging instruction-tuning datasets containing reasoning templates for different types of problems. This combination creates a rich initial pool of meta-thoughts. Selection: A Multi-Armed Bandit (MAB) algorithm selects the most promising meta-thought for each iteration. MAB is a problem framework where an agent must repeatedly choose between multiple options, or “arms,” each with unknown reward distributions. The core challenge lies in balancing “exploration” (e.g., trying different reasoning strategies) and “exploitation” (consistently selecting the reasoning strategy that previously provided the best responses). In METASCALE, each meta-thought is treated as an arm, and the goal is to maximize the reward (response quality) based on the selected meta-thought. Evolution: A genetic algorithm refines and expands the pool of cognitive strategies iteratively. METASCALE uses high-performing meta-thoughts as “parents” to produce new “child” meta-thoughts. The LLM is prompted to develop refined meta-thoughts that integrate and improve upon the selected parents. To remain efficient, METASCALE operates within a fixed sampling budget when generating meta-thoughts. The researchers evaluated METASCALE on mathematical reasoning benchmarks (GSM8K), knowledge and language understanding (MMLU-Pro), and Arena-Hard, comparing it to four baseline inference methods: direct responses (single-pass inference), CoT, Best-of-N (sampling multiple responses and choosing the best one), and Best-of-N with CoT. They used GPT-4o and Llama-3.1-8B-Instruct as the backbone models for their experiments. The results show that METASCALE significantly enhances LLM problem-solving capabilities across diverse tasks, consistently outperforming baseline methods. METASCALE achieved equal or superior performance compared to all baselines, regardless of whether they used CoT prompting. Notably, GPT-4o with METASCALE outperformed o1-mini under style control. “These results demonstrate that integrating meta-thoughts enables LLMs to scale more effectively during test time as the number of samples increases,” the researchers state. As the number of candidate solutions increased, METASCALE showed significantly higher gains than other baselines, indicating that it is a more effective scaling strategy. As a test-time technique, METASCALE can help enterprises improve the quality of LLM reasoning through smart prompt engineering without the need to fine-tune or switch models. It also doesn’t require building complex software scaffolding on top of models, as the logic is completely provided by the LLM itself. By dynamically adjusting the reasoning strategies of LLMs, METASCALE is also practical for real-world applications that handle various reasoning tasks. It is also a black-box method, which can be applied to open-source models running on the enterprise cloud or closed models running behind third-party APIs. It shows promising capabilities of test-time scaling techniques for reasoning tasks. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">The open source Model Context Protocol was just updated — here’s why it’s a big deal</div>
                            <div class="article-source">VentureBeat - March 26, 2025 6:51 PM</div>
                            <div class="article-description">An updated version of the MCP spec introduced key upgrades to make AI agents more secure, capable and interoperable.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More The Model Context Protocol (MCP)—a rising open standard designed to help AI agents interact seamlessly with tools, data, and interfaces—just hit a significant milestone. Today, developers behind the initiative finalized an updated version of the MCP spec, introducing key upgrades to make AI agents more secure, capable and interoperable. [Update: In a very significant move, OpenAI, the industry leader in generative AI, followed the MCP announcement today by saying it also is adding support for MCP across its products. CEO Sam Altman said the support is available today in OpenAI’s Agents SDK and that support for ChatGPT’s desktop app and the Responses API would be coming soon.] In a notable move, Microsoft also recently announced support for MCP alongside this release, including launching a new Playwright-MCP server that allows AI agents like Claude to browse the web and interact with sites using the Chrome accessibility tree. “This new version is a major leap forward for agent-tool communication,” Alex Albert, a key contributor to the MCP project, said in a post on Twitter. “And having Microsoft building real-world infrastructure on top of it shows how quickly this ecosystem is evolving.” The March 26 update brings several important protocol-level changes: Figure 1: Claude Desktop using Playwright-MCP to navigate and describe datasette.io, demonstrating web automation powered by the Model Context Protocol. The protocol uses a modular JSON-RPC 2.0 base, with a layered architecture separating core transport, lifecycle management, server features (like resources and prompts) and client features (like sampling or logging). Developers can pick and choose which components to implement, depending on their use case. A day before the MCP update, Microsoft released Playwright-MCP, a server that wraps its powerful browser automation tool in the MCP standard. This means AI agents like Claude can now do more than talk—they can click, type, browse and interact with the web like a real user. Built on the Chrome accessibility tree, the integration allows Claude to access and describe page contents in a human-readable form. The available toolset includes: This turns any compliant AI agent into a test automation bot, QA assistant or data navigator. people love MCP and we are excited to add support across our products. available today in the agents SDK and support for chatgpt desktop app + responses api coming soon! Setup is easy: users simply add Playwright as a command in claude_desktop_config.json, and the Claude Desktop app will recognize the tools at runtime. Figure 2: The modular design of MCP enables developers to implement only the layers they need, while maintaining compatibility. Anthropic first introduced MCP in late 2023 to solve a growing pain point: AI agents need to interact with real-world tools, but every app speaks a different “language.” MCP aims to fix that by providing a common protocol for describing and using tools across ecosystems. With backing from Anthropic, LangChain, and now Microsoft, MCP is emerging as a serious contender for becoming the standard layer of agent interconnectivity. Since MCP was launched first by Anthropic, questions lingered whether Anthropic’s largest competitor, OpenAI, would support the protocol. And of course, Microsoft, a big ally of OpenAI, was another question mark. The fact that both players have supported the protocol shows momentum is building among enterprise and open-source communities. OpenAI itself has been opening its ecosystem around agents, including with its latest Agents SDK announced a week ago — and the move has solidified support around OpenAI’s API formats becoming a standard, given that others like Anthropic and Google have fallen in line. So with OpenAI’s APIs and MCP both seeing support, standardization has seen a bit win over the past couple of weeks. “We’re entering the protocol era of AI,” tweeted Alexander Doria, the co-founder of AI startup Pleias. “This is how agents will actually do things.” With the release of MCP 0.2 and Microsoft’s tangible support, the groundwork is being laid for a new generation of agents who can think and act securely and flexibly across the stack. Figure 3: OAuth 2.1 Authorization Flow in Model Context Protocol (MCP) The big question now is: Will others follow? If Meta, Amazon, or Apple sign on, MCP could soon become the universal “language” of AI actions. For now, it’s a big day for the agent ecosystem—one that brings the promise of AI interoperability closer to reality. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">New analyst forecast says X&#x27;s advertising revenue is on the upswing, driven partly by &#x27;fear&#x27;</div>
                            <div class="article-source">Business Insider - 2025-03-26T18:41:37Z</div>
                            <div class="article-description">EMARKETER forecasts X&#x27;s US ad revenue will jump by 17.5% this year but cautions that &quot;fear is not a sustainable motivator.&quot;</div>
                            <div class="article-content"><p>Full content not available</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">The new best AI image generation model is here: say hello to Reve Image 1.0!</div>
                            <div class="article-source">VentureBeat - March 25, 2025 8:22 AM</div>
                            <div class="article-description">One of the model’s standout capabilities is its strong text rendering performance, addressing a common challenge in AI-generated imagery.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Reve AI, Inc., an AI startup based in Palo Alto, California, has officially launched Reve Image 1.0, an advanced text-to-image generation model designed to excel at prompt adherence, aesthetics, and typography. This marks the company’s first release, with future tools expected to follow. Reve Image is currently available for free preview at preview.reve.art, allowing users to generate images from text descriptions without requiring advanced prompt engineering. The company has not yet announced API access or long-term pricing plans, nor is it clear if the model will be proprietary or made open source, and if so, under what license. Reve Image differentiates itself by aiming for a deeper understanding of user intent. It allows users to not only generate images from text but also modify existing images with simple language commands. Example modifications include changing colors, adjusting text, and altering perspectives. The model also supports uploading reference images, enabling users to create visuals that match a specific style or inspiration. One of the model’s standout capabilities is its strong text rendering performance, addressing a common challenge in AI-generated imagery — and making it more directly competitive with text-focused image models such as Ideogram, which are more valuable to those designing logos and branding. Additionally, early user tests suggest that Reve Image handles multi-character prompts more effectively than previous models. Reve Image has already been evaluated by third-party AI model testing service Artificial Analysis. In the Artificial Analysis’s Image Arena, which ranks various image generation models based on user reviews and other quantitative metrics, Reve is currently in the lead at #1 for “image generation quality,” outperforming competitors such as Midjourney v6.1, Google’s Imagen 3, Recraft V3, and Black Forest Lab’s FLUX.1.1 [pro]. The benchmarking group highlighted Reve Image’s ability to generate clear and readable text within images, a historically difficult task for AI models. Before its official unveiling, Reve Image was known under the code name “Halfmoon” on social media, generating speculation and anticipation within the AI community. Reve describes itself as a “small team of passionate researchers, builders, designers, and storytellers with big ideas.” The company is focused on developing creative tooling that enhances how users interact with AI-powered visuals. On X, Michaël Gharbi, Co-Founder and Research Scientist at Reve, shared insights into the company’s long-term vision, emphasizing the goal of building AI models that understand creative intent rather than merely generating visually plausible outputs. “Capturing creative intent requires advanced machine understanding of natural language and other interactions,” Gharbi said. “Our vision is to build a new semantic intermediate representation that both a human and a machine can understand, reason about, and operate on.” Other team members, including engineer Hunter Loftis and researcher Taesung Park, echoed the importance of bringing logic to AI-generated visuals. Park compared current text-to-image models to early large language models (LLMs), stating that they often produce visually appealing but logically inconsistent results. Early user feedback on the AI-heavy subreddit r/singularity (on Reddit), has been largely positive, with many praising the model’s accurate prompt following, high-quality text rendering, and rapid generation speed. Some users have reported success in generating multi-character scenes and complex environments, areas where previous models often struggled. However, some challenges remain. Users have noted that Reve Image: Despite these hurdles, the team at Reve has been actively engaging with the user community and incorporating feedback into ongoing improvements. In my own brief hands on usage while drafting and creating the header image for this very article, I found Reve to be fairly intuitive and easy-to-use, with impressive visuals and prompt adherence. Like many AI-image generators, there’s a prompt entry textbox, though unlike Midjourney and Ideogram, Reve puts it at the bottom of the website and leaves your generated content up top to fill the majority of the space. In addition, the prompt entry textbox also contains four buttons below it for further fine adjustments to the image generation prompt sequence, including an aspect ratio adjuster (with standard sizing between 16:9 (widescreen landscape) and 9:16 (portrait, like a smartphone)… There’s another button selector for how many images you want to produce from each prompt (1, 2, 4, 8), a button to toggle on and off prompt text enhancement (it’s default toggled on, and this means that Reve will actually automatically edit the text you type in based on what it thinks you want to see in your image, adding lots more rich details and visual language than you might initially include) and a “seed” button for choosing if you want it to use a specific numeric string from a previous generated image to guide the generations going forward. It’s far fewer settings and doesn’t include any visual based editors like Midjourney, but the basics are there and it should be more than enough for most casual AI image users to get started. My brief tests also showed it was on-par or better than Ideogram at rendering legible text baked into images (and far surpassing Midjoruney), as well as on-par or exceeding the quality of rendering recognizable public figures as Grok (again, Midjourney and many other image generators prohibit this). While the model is currently only available via the company’s website, there is growing anticipation for API access or potential open-source options. Users have also expressed interest in additional features like custom model training, control tools for animation, and integration with creative software. For now, Reve Image remains freely accessible at preview.reve.art, allowing users to explore its capabilities firsthand. As Reve continues to refine its AI models and expand its offerings, the company is positioning itself as a major player in the evolving world of AI-powered creative tooling. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Microsoft infuses enterprise agents with deep reasoning, unveils data Analyst agent that outsmarts competitors</div>
                            <div class="article-source">VentureBeat - March 25, 2025 7:45 PM</div>
                            <div class="article-description">Microsoft announced Tuesday two significant additions to its Copilot Studio platform: deep reasoning capabilities that enable agents to tackle complex problems through careful, methodical thinking, and agent flows that combine AI flexibility with deterministic business process automation.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Microsoft has built the largest enterprise AI agent ecosystem, and is now extending its lead with powerful new capabilities that position the company ahead in one of enterprise tech’s most exciting segments. The company announced Tuesday evening two significant additions to its Copilot Studio platform: deep reasoning capabilities that enable agents to tackle complex problems through careful, methodical thinking, and agent flows that combine AI flexibility with deterministic business process automation. Microsoft also unveiled two specialized deep reasoning agents for Microsoft 365 Copilot: Researcher and Analyst. “We have customers with thousands of agents already,” Microsoft’s Corporate Vice President for Business and Industry Copilot Charles Lamanna, told VentureBeat in an exclusive interview on Monday. “You start to have this kind of agentic workforce where no matter what the job is, you probably have an agent that can help you get it done faster.” While the Researcher agent mirrors capabilities from competitors like OpenAI’s Deep Research and Google’s Deep Research, Microsoft’s Analyst agent represents a more differentiated offering. Designed to function like a personal data scientist, the Analyst agent can process diverse data sources, including Excel files, CSVs, and embedded tables in documents, generating insights through code execution and visualization. “This is not a base model off the shelf,” Lamanna emphasized. “This is quite a bit of extensions and tuning and training on top of the core models.” Microsoft has leveraged its deep understanding of Excel workflows and data analysis patterns to create an agent that aligns with how enterprise users actually work with data. The Analyst can automatically generate Python code to process uploaded data files, produce visualizations, and deliver business insights without requiring technical expertise from users. This makes it particularly valuable for financial analysis, budget forecasting and operational reporting use cases that typically require extensive data preparation. Microsoft’s deep reasoning capability extends agents’ abilities beyond simple task completion to complex judgment and analytical work. By integrating advanced reasoning models like OpenAI’s o1 and connecting them to enterprise data, these agents can tackle ambiguous business problems more methodically. The system dynamically determines when to invoke deeper reasoning, either implicitly based on task complexity or explicitly when users include prompts like “reason over this” or “think really hard about this.” Behind the scenes, the platform analyzes instructions, evaluates context, and selects appropriate tools based on the task requirements. This enables scenarios that were previously difficult to automate. For example, one large telecommunications company uses deep reasoning agents to generate complex RFP responses by assembling information from across multiple internal documents and knowledge sources, Lamanna told VentureBeat. Similarly, Thomson Reuters employs these capabilities for due diligence in mergers and acquisition reviews, processing unstructured documents to identify insights, he said. See an example of the agent reasoning at work in the video below: Microsoft has also introduced agent flows, which effectively evolve robotic process automation (RPA) by combining rule-based workflows with AI reasoning. This addresses customer demands for integrating deterministic business logic with flexible AI capabilities. “Sometimes they don’t want the model to freestyle. They don’t want the AI to make its own decisions. They want to have hard-coded business rules,” Lamanna explained. “Other times they do want the agent to freestyle and make judgment calls.” This hybrid approach enables scenarios like intelligent fraud prevention, where an agent flow might use conditional logic to route higher-value refund requests to an AI agent for deep analysis against policy documents. Pets at Home, a U.K.-based pet supplies retailer, has already deployed this technology for fraud prevention. Lamanna revealed the company has saved “over a million pounds” through the implementation. Similarly, Dow Chemical has realized “millions of dollars saved for transportation and freight management” through agent-based optimization. Below is a video showing the Agent Flows at work: Central to Microsoft’s agent strategy is its enterprise data integration through the Microsoft Graph, which is a comprehensive mapping of workplace relationships between people, documents, emails, calendar events, and business data. This provides agents with contextual awareness that generic models lack. “The lesser known secret capability of the Microsoft graph is that we’re able to improve relevance on the graph based on engagement and how tightly connected some files are,” Lamanna revealed. The system identifies which documents are most referenced, shared, or commented on, ensuring agents reference authoritative sources rather than outdated copies. This approach gives Microsoft a significant competitive advantage over standalone AI providers. While competitors may offer advanced models, Microsoft combines these with workplace context and fine-tuning optimized explicitly for enterprise use cases and Microsoft tools. Microsoft can leverage the same web data and model technology that competitors can, Lamanna noted, “but we then also have all the content inside the enterprise.” This creates a flywheel effect where each new agent interaction further enriches the graph’s understanding of workplace patterns. Microsoft has prioritized making these powerful capabilities accessible to organizations with varying technical resources, Lamanna said. The agents are exposed directly within Copilot, allowing users to interact through natural language without prompt engineering expertise. Meanwhile, Copilot Studio provides a low-code environment for custom agent development. “It’s in our DNA to have a tool for everybody, not just people who can boot up a Python SDK and make calls, but anybody can start to build these agents,” Lamanna emphasized. This accessibility approach has fueled rapid adoption. Microsoft previously revealed that over 100,000 organizations have used Copilot Studio and that more than 400,000 agents were created in the last quarter. While Microsoft appears to lead enterprise agent deployment today, competition is intensifying. Google has expanded its Gemini capabilities for agents and agentic coding, while OpenAI’s o1 model and Agents SDK provide powerful reasoning and agentic tools for developers. Big enterprise application companies like Salesforce, Oracle, ServiceNow, SAP and others have all launched agentic platforms for their customers over the last year. And also on Tuesday, Amazon’s AWS released an AI agent, called Amazon Q in Quicksight, to let employees to engage via natural language to perform data analysis without specialized skills. Employees can use natural language to perform expert-level data analysis, ask what-if questions, and get actionable recommendations, helping them unlock new insights and make decisions faster However, Microsoft’s advantage lies in its more comprehensive approach—a strong coupling with the leading reasoning model company, OpenAI, while also offering model choice, enterprise-grade infrastructure, extensive data integration across workplace tools, and a focus on business outcomes rather than raw AI capabilities. Microsoft has created an ecosystem that looks like best practice by combining personal copilots that understand individual work patterns with specialized agents for specific business processes. For enterprise decision-makers, the message is clear: agent technology has matured beyond experimentation to practical business applications with measurable ROI. The choice of platform increasingly depends on integration with existing tools and data. In this area, Microsoft holds an advantage in many application areas because of the number of users it has, for example, in Excel and Power Automate. Watch my full interview with Charles Lamanna embedded below to hear firsthand how Microsoft is driving its agent strategy, what these new capabilities mean for enterprise users, and how organizations are leveraging agents to deliver measurable business results: If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">TensorOpera AI and Samsung Electronics Showcase the Future of Generative AI on Mobile Devices</div>
                            <div class="article-source">VentureBeat - March 26, 2025 4:25 AM</div>
                            <div class="article-description">Press Release TensorOpera AI and Samsung Electronics announced a pioneering collaboration, which was showcased at the Consumer Electronics Show (CES) 2025 in Las Vegas, demonstrating the immense potential of deploying multi-modal generative AI applications directly…</div>
                            <div class="article-content"><p>PALO ALTO, Calif.–(BUSINESS WIRE)–March 26, 2025– TensorOpera AI and Samsung Electronics announced a pioneering collaboration, which was showcased at the Consumer Electronics Show (CES) 2025 in Las Vegas, demonstrating the immense potential of deploying multi-modal generative AI applications directly on mobile devices powered by Samsung Exynos processors. This milestone marks a significant step forward in redefining the role of mobile technology in advancing artificial intelligence. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20250318795401/en/ Bringing generative AI to mobile phones is a transformative innovation that addresses critical challenges such as privacy, personalization, and cost. Smartphones, which store vast amounts of personal data, offer an untapped opportunity to create AI agents uniquely tailored to enhance productivity, communication, and social interactions. With over 80 apps installed on the average smartphone-spanning self-care, productivity, financial planning, and more-the possibilities for personalized AI applications are endless. Imagine a future where your phone acts as a personal creativity coach, helping you brainstorm innovative ideas, sketch designs, compose music, or generate stunning visual art-all tailored to your unique style and preferences, while safeguarding your privacy. Picture a financial advisor that not only optimizes investments but continuously tracks market trends, predicts opportunities, and customizes strategies to align seamlessly with your goals. Envision a writing assistant that adapts to your voice, integrates effortlessly across your apps, drafts emails, creates presentations, and generates compelling content-all personalized to you. This is the transformative potential of bringing multimodal generative AI directly to mobile devices. TensorOpera implements Android applications and highly optimized C++ multi-modal inference pipeline to import and release the Apps on top of Exynos AI Stack. These Apps completely run on any smartphones equipped with Exynos processors with a reasonable inference latency. “Our collaborative demonstration at CES 2025 showcases how advanced multi-modal generative AI applications can seamlessly integrate into mobile devices, paving the way for smarter, more personalized AI solutions that respect user privacy-powered by Samsung’s Exynos mobile chips,” said Dr. Kee-Bong Song, SVP and Head of S.LSI US R&amp;D Center at Samsung Electronics. TensorOpera AI: Pioneering Hybrid Edge-Cloud AI Development TensorOpera AI’s end-to-end platform enables developers to design and deploy AI applications across cloud, mobile, and hybrid mobile-cloud environments. The future of generative AI lies in hybrid edge-cloud deployments, which combine the strengths of on-device processing with the scalability of cloud infrastructure. This hybrid approach balances efficiency and personalization. By processing sensitive data locally on mobile devices, users retain control over their privacy while accessing the immense computational power of the cloud when necessary. This ensures cost-effective deployment, minimizes reliance on centralized cloud infrastructures, and enhances scalability for adoption across industries and geographies. The benefits of hybrid mobile-cloud architectures extend far beyond individual devices. By reducing dependency on expensive cloud GPUs, this distributed model democratizes AI, making advanced models accessible to a broader range of users and developers. This innovation aligns with industry predictions that the market for AI assistants could surpass $6 trillion, with hybrid architectures forming the foundation of next-generation AI ecosystems. “Our collaboration with Samsung Electronics demonstrates the future of AI: smarter, more secure, and deeply personalized solutions powered by the synergy of mobile and cloud technologies,” said Salman Avestimehr, Chairman of TensorOpera AI. “As generative AI continues to evolve, we see hybrid deployments as the key to unlocking its full potential for users worldwide.” Building on their success with Samsung’s on-device multi-modal GenAI project, the founders of TensorOpera have once again demonstrated their prowess in Mobile AI through the ChainOpera AI Terminal App. They have developed LLM-based AI Agent Networks that seamlessly integrate cloud, edge, and device capabilities, serving over a million users at a cost 10 times lower than industry standard, and state-of-the-art LLM performance with a federated AI approach. About TensorOpera AI TensorOpera, Inc. (formerly FedML, Inc.) is an innovative AI company based in Silicon Valley, specifically Palo Alto, California. TensorOpera specializes in developing scalable and secure AI platforms tailored for enterprises and developers with two flagship products: (1) TensorOpera® AI Platform: Accessible at TensorOpera.ai, this platform serves as a comprehensive generative AI ecosystem. It features robust tools for enterprise AI platforms, model deployment, model serving, AI agent APIs, and more. It supports launching training and inference jobs on a serverless/decentralized GPU cloud, experimental tracking for distributed training, and enhanced security and privacy measures. (2) TensorOpera® FedML: Available at FedML.ai, this platform is a leader in federated learning and analytics, supporting zero-code implementation. It includes a lightweight, cross-platform Edge AI SDK suitable for edge GPUs, smartphones, and IoT devices. Additionally, it offers a user-friendly MLOps platform to streamline decentralized machine learning and deployment in real-world applications. Founded in February 2022, TensorOpera has quickly grown to support a large number of enterprises and developers worldwide.  View source version on businesswire.com: https://www.businesswire.com/news/home/20250318795401/en/ contact@tensoropera.com If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Trump says he may reduce China tariffs to help close a TikTok deal</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20257:06 PM EDT</div>
                            <div class="article-description">&quot;Maybe I&#x27;ll give them a little reduction in tariffs or something to get it done,&quot; President Donald Trump said about a deal involving TikTok&#x27;s U.S. operations</div>
                            <div class="article-content"><p>President Donald Trump said he may reduce tariffs on China to facilitate a deal that would result in ByteDance selling the U.S. operations of TikTok. China &quot;is going to have to play a role&quot; in approving a TikTok-related divestiture, Trump said in a press conference Wednesday. &quot;Maybe I&#x27;ll give them a little reduction in tariffs or something to get it done,&quot; Trump said. &quot;TikTok is big, but every point in tariffs is worth more than TikTok.&quot; Although a national security law requires ByteDance to divest TikTok&#x27;s U.S. operations or face an effective ban in the country, Trump in January signed an executive order that delayed the deadline for a deal to April 5. Trump has previously said that he wants the U.S. to maintain a 50% ownership position in TikTok via a joint venture. It&#x27;s possible he will extend the TikTok deadline again, Trump said Wednesday. &quot;We&#x27;re going to have a form of a deal, but if it&#x27;s not finished, it&#x27;s not a big deal,&quot; Trump said. &quot;We&#x27;ll just extend it.&quot; Vice President JD Vance told NBC News earlier this month that he was confident that a TikTok-related deal would happen by the April deadline. &quot;There will almost certainly be a high-level agreement that I think satisfies our national security concerns, allows there to be a distinct American TikTok enterprise,&quot; Vance said. WATCH: TikTok bid is &#x27;in active dialogue&#x27; with Trump administration.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">‘Insane’: OpenAI introduces GPT-4o native image generation and it’s already wowing users</div>
                            <div class="article-source">VentureBeat - March 25, 2025 12:10 PM</div>
                            <div class="article-description">As AI-generated images become more precise and accessible, GPT-4o represents a significant step forward in the space.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More We’re coming up on the one year anniversary since OpenAI released its first “omni” or multimodal model, GPT-4o back in May 2024, but that old standby still has some tricks up its sleeve. Case-in-point, today OpenAI finally turned on the native multimodal image generation capabilities of GPT-4o for users of its hit chatbot ChatGPT on the Plus, Pro, Team, and Free usage tiers. The company said it would also soon be made available for Enterprise, Edu, and through its application programming interface (API). Unlike the previous generative AI image model available in ChatGPT — OpenAI’s DALL-E 3, a classic diffusion transformer model that was trained to reconstruct images from text prompts by removing noise from pixels — this new image generator is part of the same model that spits out text and code, as OpenAI trained the entire model to understand all these forms of media at once. As a result, it is is much more accurate at interpreting a user’s prompts and matching corresponding imagery, the imagery is far more detailed and lifelike, and the user can go and back forth and request specific edits and changes in natural language that the model quickly implements in new generations. This has resulted in a much higher quality image generator that produces far more lifelike images and accurate text baked in, and it’s already impressing users — one of whom calls the quality “insane.” OpenAI president Greg Brockman had long ago previewed this native capability of GPT-4o back in May 2024, but for reasons that still remain unknown publicly, the company held onto it until now — following the public release of what many AI power users saw as a similar feature from Google AI Studio with its Gemini 2 Flash Experimental model. By the same token (pun intended), OpenAI still hasn’t said precisely what data GPT-4o’s image generation capabilities were trained on — and given the history of the company and other model providers, it likely includes many artworks scraped from the web, some of which are presumably copyrighted, which is likely to anger the artists behind them. OpenAI has long aimed to make image generation a core capability of its AI models. With GPT-4o, users can now generate images directly in ChatGPT, refining them through conversation and adjusting details on the fly. The model also integrates into Sora, OpenAI’s video-generation platform, further expanding multimodal capabilities. In an announcement on X, OpenAI confirmed that GPT-4o’s image generation is designed to: Users can describe an image in ChatGPT, specifying details such as aspect ratio, color schemes (hex codes), or transparency, and GPT-4o will generate it within a minute. As independent AI consultant Allie K. Miller wrote on X, it’s a “Huge leap in text generation,” and is “the best” AI image generation model she’s seen. GPT-4o is designed to make image generation not just visually stunning but also practical. Some of the key applications include: According to OpenAI’s official thread on X, GPT-4o introduces several improvements over previous models: Despite its advancements, GPT-4o still has some known challenges: OpenAI is actively addressing these issues through ongoing model refinements. As part of OpenAI’s commitment to responsible AI development, all GPT-4o-generated images include C2PA metadata, allowing users to verify their AI origin. Moreover, OpenAI has built an internal search tool to help detect AI-generated images. Strict safeguards are in place to block harmful content and prevent misuse, such as prohibiting explicit, deceptive, or harmful imagery. OpenAI also ensures that images featuring real people are subject to heightened restrictions. OpenAI CEO Sam Altman described the release as a “new high-water mark for creative freedom”, emphasizing that users will be able to create a wide range of visuals, with OpenAI observing and refining its approach based on real-world usage. As AI-generated images become more precise and accessible, GPT-4o represents a significant step forward in making text-to-image generation a mainstream tool for communication, creativity, and productivity. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Observe launches VoiceAI agents to automate customer call centers with realistic, humanlike voices that don’t interrupt</div>
                            <div class="article-source">VentureBeat - March 26, 2025 6:47 AM</div>
                            <div class="article-description"></div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Observe.AI has officially launched VoiceAI agents, a solution designed to automate routine customer interactions in contact centers. The latest addition to the company’s AI-driven conversational intelligence platform, VoiceAI agents aim to improve customer experience while reducing operational costs. With this release, Observe.AI is positioning itself as the only complete AI-powered platform that supports enterprises across the entire customer journey. The company’s suite of solutions now includes enterprise-grade VoiceAI agents, real-time agent assist tools, AutoQA for quality monitoring, agent coaching, and business insights. Observe.AI’s VoiceAI agents are built to handle a wide range of customer service inquiries, from frequently asked questions to more complex, multi-step conversations. They are built atop a combination of in-house AI models and partnerships with major AI providers like OpenAI and Anthropic for large language models (LLMs). “It’s an ensemble of multiple smaller models,” Jain explained. “For example, we have a specific model for number detection, a specific model for entity detection, a model for turn detection, and so on.” The goal is to alleviate the burden on human agents, allowing them to focus on higher-value interactions. As Swapnil Jain, CEO and co-founder of Observe.AI, told VentureBeat in a recent video call interview: “Enterprises are saying, ‘Do we really need human agents for these kinds of use cases?’” Jain said that companies often receive calls for basic tasks like checking an account balance or resetting a password—interactions that AI can now handle efficiently. For customers, this means eliminating long hold times and avoiding frustrating IVR menus that require pressing multiple buttons or repeatedly requesting a human agent. The voice AI space is becoming increasingly crowded with options ranging from proprietary models like OpenAI’s newly released GPT-4o-transcribe family and ElevenLabs to open source solutions as well. So why would someone pick Observe.AI’s agents over these? In a nutshell: specialization and ease-of-use. Instead of having to use raw voice AI models through providers’ APIs and building custom integrations with a business, or custom voice apps, Obseve.AI’s platform is already built to essentially “plug and play” with existing workflows and operations. So while, GPT-4o and other LLMs provide raw AI capabilities, Jain and Observe.AI’s contention is that they don’t offer a fully integrated solution for customer service workflows. In addition, unlike traditional voice AI assistants, Observe.AI’s VoiceAI agents are specifically designed for contact centers. The system combines various AI technologies, including: Jain noted that one of the key challenges AI agents face is knowing when a customer has actually finished speaking. “When do you know that the AI agent can start processing and the customer has stopped speaking?” he asked. “Sometimes I’m taking pauses because my sentence is over and I’m starting a new one. Sometimes I just stop speaking. How do you know the difference?” Observe.AI has developed custom in-house models that solve these nuances, ensuring smoother conversations between AI and customers. One of Observe.AI’s key advantages is its ability to integrate seamlessly with existing enterprise systems. Over time, the company has developed pre-built integrations with more than 250 platforms, including leading telephony, CRM, and workforce management tools such as Salesforce, Zendesk, and ServiceNow. This approach allows businesses to implement VoiceAI agents quickly. While AI deployments can sometimes take months, Observe.AI claims that its VoiceAI agents can go live in as little as one week, with minimal setup costs. “It’s not a professional services model where we take six months to customize something for you,” Jain said. “We come in, take two weeks to configure the product, and it works.” Given the sensitivity of customer interactions, Observe.AI has built its solution with enterprise-grade security. The company holds certifications including GDPR, HIPAA, HITRUST, SOC2, and ISO27001. While voice biometrics have been used in the past for authentication, Jain stated that Observe.AI does not rely on them due to security concerns. Instead, the system follows traditional authentication methods, such as verifying Social Security numbers or account details. Additionally, Observe.AI offers data redaction capabilities to remove personally identifiable information (PII) before storage, and customers can opt for private instances to ensure data remains isolated. “In today’s world, you cannot rely on individual speech patterns for authentication,” Jain said. “We work with businesses to configure the same security rules they use for their human agents into our AI agents.” Observe.AI’s pricing model is based on completed tasks rather than per-minute usage. The cost depends on the complexity of the interaction, with simpler tasks (such as routing a call) priced lower than more involved tasks (such as processing an insurance claim). According to Jain, businesses can expect to save between 70-80% on customer service costs compared to using human agents. Companies using VoiceAI agents are already seeing significant improvements. Emmanual Noyola, Director of Patient Services at Affordable Care, highlighted the impact on his team: “Beth, our VoiceAI agent, handles multiple intents with a 95% containment rate so our customer care team can focus on more complex cases.” By analyzing every conversation, Observe.AI’s platform continuously refines AI agent performance, ensuring accuracy and compliance. Businesses can also use AutoQA to evaluate both AI and human agents, identifying areas for improvement. One of the key challenges in AI-driven customer service is maintaining accuracy while preventing unintended responses. Jain acknowledged these concerns, referencing past AI missteps in customer service automation. “The core thesis behind making these enterprise-grade is having a very high bar on the confidence of the response,” he said. “If our response confidence is less than a certain threshold, it’s better for the AI agent to not even engage.” The launch of VoiceAI agents marks a major step toward what Observe.AI envisions as the autonomous contact center of the future. As AI continues to evolve, the company is focused on creating solutions that blend automation with human expertise, ensuring a seamless customer experience. “Since our founding seven years ago, we have created a platform that uniquely understands contact center conversations,” Jain said. “It is a logical next step to introduce our VoiceAI agents to automate interactions and ultimately support both human and AI agents in delivering consistent, secure, high-quality customer experience across every touchpoint.” If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">23andMe cofounder says company &#x27;lost its way&#x27; without &#x27;proper governance&#x27;</div>
                            <div class="article-source">Business Insider - 2025-03-26T23:20:33Z</div>
                            <div class="article-description">23andMe cofounder Linda Avey is mourning what the biotech company could have become. The company filed for Chapter 11 bankruptcy protection on Sunday.</div>
                            <div class="article-content"><p>5 ways Elon Musk shook up Twitter as CEO</p></div>
                        </div>
                
                    </section>
            
                    <section id="theme-2">
                        <h2>Technology Companies <small>(34 articles)</small></h2>
                        <p>Articles related to updates, developments, and activities of technology companies</p>
                        
                        <h3>Articles in this Theme</h3>
            
                        <div class="article-card">
                            <div class="article-title">Beyond transformers: Nvidia’s MambaVision aims to unlock faster, cheaper enterprise computer vision</div>
                            <div class="article-source">VentureBeat - March 25, 2025 3:35 PM</div>
                            <div class="article-description">Nvidia is updating its computer vision models with new versions of MambaVision that combine the best of Mamba and transformers to improve efficiency.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Transformer-based large language models (LLMs) are the foundation of the modern generative AI landscape. Transformers aren’t the only way to do gen AI, though. Over the course of the last year, Mamba, an approach that uses Structured State Space Models (SSM), has also picked up adoption as an alternative approach from multiple vendors, including AI21 and AI silicon giant Nvidia. Nvidia first discussed the concept of Mamba-powered models in 2024 when it initially released the MambaVision research and some early models. This week, Nvidia is expanding on its initial effort with a series of updated MambaVision models available on Hugging Face. MambaVision, as the name implies, is a Mamba-based model family for computer vision and image recognition tasks. The promise of MambaVision for enterprise is that it could improve the efficiency and accuracy of vision operations, at potentially lower costs, thanks to lower computational requirements. SSMs are a neural network architecture class that processes sequential data differently from traditional transformers. While transformers use attention mechanisms to process all tokens in relation to each other, SSMs model sequence data as a continuous dynamic system. Mamba is a specific SSM implementation developed to address the limitations of earlier SSM models. It introduces selective state space modelling that dynamically adapts to input data and hardware-aware design for efficient GPU utilization. Mamba aims to provide comparable performance to transformers on many tasks while using fewer computational resources Traditional Vision Transformers (ViT) have dominated high-performance computer vision for the last several years, but at significant computational cost. Pure Mamba-based approaches, while more efficient, have struggled to match Transformer performance on complex vision tasks requiring global context understanding. MambaVision bridges this gap by adopting a hybrid approach. Nvidia’s MambaVision is a hybrid model that strategically combines Mamba’s efficiency with the Transformer’s modelling power. The architecture’s innovation lies in its redesigned Mamba formulation specifically engineered for visual feature modeling, augmented by strategic placement of self-attention blocks in the final layers to capture complex spatial dependencies. Unlike conventional vision models that rely exclusively on either attention mechanisms or convolutional approaches, MambaVision’s hierarchical architecture employs both paradigms simultaneously. The model processes visual information through sequential scan-based operations from Mamba while leveraging self-attention to model global context — effectively getting the best of both worlds. The new set of MambaVision models released on Hugging Face is available under the Nvidia Source Code License-NC, which is an open license. The initial variants of MambaVision released in 2024 include the T and T2 variants, which were trained on the ImageNet-1K library. The new models released this week include the L/L2 and L3 variants, which are scaled-up models. “Since the initial release, we’ve significantly enhanced MambaVision, scaling it up to an impressive 740 million parameters,” Ali Hatamizadeh, Senior Research Scientist at Nvidia wrote in a Hugging Face discussion post. “We’ve also expanded our training approach by utilizing the larger ImageNet-21K dataset and have introduced native support for higher resolutions, now handling images at 256 and 512 pixels compared to the original 224 pixels.” According to Nvidia, the improved scale in the new MambaVision models also improves performance. Independent AI consultant Alex Fazio explained to VentureBeat that the new MambaVision models’ training on larger datasets makes them much better at handling more diverse and complex tasks. He noted that the new models include high-resolution variants perfect for detailed image analysis. Fazio said that the lineup has also expanded with advanced configurations offering more flexibility and scalability for different workloads. “In terms of benchmarks, the 2025 models are expected to outperform the 2024 ones because they generalize better across larger datasets and tasks, Fazio said. For enterprises building computer vision applications, MambaVision’s balance of performance and efficiency opens new possibilities Reduced inference costs: The improved throughput means lower GPU compute requirements for similar performance levels compared to Transformer-only models. Edge deployment potential: While still large, MambaVision’s architecture is more amenable to optimization for edge devices than pure Transformer approaches. Improved downstream task performance: The gains on complex tasks like object detection and segmentation translate directly to better performance for real-world applications like inventory management, quality control, and autonomous systems. Simplified deployment: NVIDIA has released MambaVision with Hugging Face integration, making implementation straightforward with just a few lines of code for both classification and feature extraction. MambaVision represents an opportunity for enterprises to deploy more efficient computer vision systems that maintain high accuracy. The model’s strong performance means that it can potentially serve as a versatile foundation for multiple computer vision applications across industries. MambaVision is still somewhat of an early effort, but it does represent a glimpse into the future of computer vision models. MambaVision highlights how architectural innovation—not just scale—continues to drive meaningful improvements in AI capabilities. Understanding these architectural advances is becoming increasingly crucial for technical decision-makers to make informed AI deployment choices. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Alibaba launches new open-source AI model for &#x27;cost-effective AI agents&#x27;</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20258:41 PM EDT</div>
                            <div class="article-description">Alibaba Cloud has launched its latest AI model in its &quot;Qwen series,&quot; as large language model competition in China continues to heat up. </div>
                            <div class="article-content"><p>Alibaba Cloud launched Thursday its latest AI model in its &quot;Qwen series,&quot; as large language model competition in China continues to heat up following the &quot;DeepSeek moment.&quot; The new &quot;Qwen2.5-Omni-7B&quot; is a multimodal model, which means it can process inputs, including text, images, audio and videos, while generating real-time text and natural speech responses, according to an announcement on Alibaba Cloud&#x27;s website. The company says that the model can be deployed on edge devices like mobile phones, offering high efficiency without compromising performance. &quot;This unique combination makes it the perfect foundation for developing agile, cost-effective AI agents that deliver tangible value, especially intelligent voice applications,&quot; Alibaba said. For example, it could be used to help a visually impaired person navigate their environment through real-time audio description, the company added. The new model is open-sourced on the platforms Hugging Face and Github, following a growing trend in China after DeepSeek made its breakthrough R1 model open-source. Open-source generally refers to software in which the source code is made freely available on the web for possible modification and redistribution. Over the past years, Alibaba Cloud says it has open-sourced over 200 generative AI models. Amid China&#x27;s AI fervor accelerated by DeepSeek, Alibaba and other generative AI competitors have been releasing new, cost-effective models and products at an unprecedented pace. Last week, Chinese tech giant Baidu released a new multimodal foundational model and its first reasoning-focused model. Alibaba, meanwhile, debuted its updated Qwen 2.5 artificial intelligence model in late January and released a new version of its AI assistant tool Quark earlier this month. The company has strongly committed to its AI strategy, announcing last month a plan to invest $53 billion in its cloud computing and AI infrastructure over the next three years, exceeding what it spent in the space over the past decade. Kai Wang, Asia senior equity analyst at Morningstar, told CNBC that large Chinese tech players such as Alibaba, which build data centers to meet the computing needs of AI in addition to building their own LLMs, are well positioned to benefit from China&#x27;s post-DeepSeek AI boom. Alibaba secured a major win for its AI business last month when it confirmed that the company was partnering with Apple to roll out AI integration for iPhones sold in China. On Wednesday, the group also reported an expanded strategic partnership with BMW to accelerate the integration of its AI into the carmaker&#x27;s next-generation intelligent vehicles.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Amazon is testing shopping, health assistants as it pushes deeper into generative AI</div>
                            <div class="article-source">CNBC - Published Tue, Mar 25 202510:04 PM EDT</div>
                            <div class="article-description">With CEO Andy Jassy pushing employees to build AI apps across the company, Amazon is testing new shopping and health assistants.</div>
                            <div class="article-content"><p>In this article Amazon, in an effort to infuse generative artificial intelligence across a wider swath of its e-commerce universe, recently began testing a shopping assistant and a health-focused chatbot with a subset of users. AI has become a major area of investment across Amazon, including in its retail, cloud computing, devices and health-care businesses. Within the retail business, Amazon has already launched a shopping chatbot, an AI assistant for sellers and AI shopping guides. The new services Amazon is testing appeared on its app or website in recent weeks. An Amazon spokesperson confirmed the features are being tested in beta with some customers. The shopping tool, called Interests AI, prompts users to describe an interest &quot;using your own words,&quot; and then it generates a curated selection of products. The feature lets consumers browse for products using more conversational language and is separate from the main search bar on Amazon&#x27;s website. Within its core app, Amazon has a landing page for the feature. &quot;Describe your interest, like &#x27;coffee brewing gadgets&#x27; or &#x27;latest pickleball accessories&#x27; — and we&#x27;ll find relevant products for you,&quot; the page says. Other suggested searches include &quot;children books about persistence and dealing with failure,&quot; and &quot;brain teasers that are not too hard, made out of wood or metal.&quot; The Amazon spokesperson said Interests uses large language models to translate everyday words or phrases into queries and attributes that traditional search engines can turn into product recommendations. It&#x27;s unclear what models Interests relies on. Amazon said in a blog post after publication of this article that it expects to make the feature available to all U.S. users in the coming months. Amazon CEO Andy Jassy said last month that employees have built or are in the process of building roughly 1,000 generative AI applications across the company. Its cloud unit offers a chatbot for businesses, called Q. In commerce, the company has rolled out services for consumers as well as its millions of third-party sellers. Amazon is also exploring ways that artificial intelligence can address medical needs. The company is testing a chatbot on its website and mobile app called &quot;Health AI,&quot; which can answer health and wellness questions, &quot;provide common care options for health care needs,&quot; and suggest products. While Rufus, Amazon&#x27;s shopping chatbot, can suggest products like ice packs and ibuprofen, Health AI goes further, providing users with medical guidance and care tips, such as how to deal with cold symptoms or the flu. The site says the service can&#x27;t provide personalized medical advice. Some responses feature a &quot;clinically verified&quot; badge, which denotes information that&#x27;s been &quot;reviewed by US-based licensed clinicians,&quot; Amazon says. Health AI also steers users to Amazon&#x27;s online pharmacy, along with clinical services offered by One Medical, the primary care provider it acquired for roughly $3.9 billion in 2022. Amazon&#x27;s spokesperson said the health assistant uses Bedrock, a service launched by Amazon&#x27;s cloud unit that accesses AI models from the company and third parties. &quot;We are collecting feedback from customers, and plan to introduce new features to enhance the experience in the future,&quot; the spokesperson said in a statement. More consumers are embracing generative AI as a shopping tool, and with features like Health AI and Interests AI, Amazon wants shoppers to use its own services over rivals like OpenAI&#x27;s ChatGPT. With enough use, Amazon could gain valuable insights on the ways that people are interacting with AI assistants as the company prepares to overhaul Alexa, the digital assistant it launched more than a decade ago. Amazon announced Alexa+, a new version of the technology embedded with generative AI, late last month. The company says that Alexa+, which has yet to roll out, is capable of handling more complex tasks and can serve as an &quot;agent&quot; by taking actions for users without their direct involvement. Andrew Bell, an Amazon e-commerce manager for the National Fire Protection Association who also publishes research on Amazon&#x27;s patent filings and AI development, came across the new shopping and health features and recently posted about them on LinkedIn. Bell said in an interview that Alexa+ could potentially draw upon models developed for Amazon applications like Health AI to answer queries. &quot;If there&#x27;s a health-related question, Alexa+ is going to maybe call on Health AI,&quot; Bell said. &quot;If there&#x27;s a product-related question, Alexa+ can call on Rufus.&quot; WATCH: Amazon&#x27;s SVP of Devices on Alexa+</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Google releases ‘most intelligent model to date,’ Gemini 2.5 Pro</div>
                            <div class="article-source">VentureBeat - March 25, 2025 1:17 PM</div>
                            <div class="article-description">Gemini 2.5 Pro is now available for Gemini Advanced users and is Google&#x27;s most capable model with a 1 million token context window.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Just a few months after releasing Gemini 2.0 and the rise of DeepSeek, Google announced its “most intelligent model” yet, Gemini 2.5, capable of reasoning and with better performance and accuracy. Gemini 2.5 comes three months after Google released its previously most intelligent model family, Gemini 2.0 which introduced reasoning and agentic use cases. This new model is available as Gemini 2.5 Pro (experimental) on Google’s AI Studio and for Gemini Advanced users on the Gemini chat interface. It will be available on Vertex AI soon. Koray Kavukcuoglu, CTO at Google DeepMind, said in a blog post that Gemini 2.5 represents the next step in Google’s goal of making “AI smarter and more capable of reasoning.” “Now, with Gemini 2.5, we’ve achieved a new level of performance by combining a significantly enhanced base model with improved post-training,” Kavukcuoglu wrote. “Going forward, we’re building these thinking capabilities directly into all of our models, so they can handle more complex problems and support even more capable, context-aware agents.” Like Gemini 2.0 and Gemini 2.0 Flash Thinking, Gemini 2.5 Pro “thinks” before it responds. The new model can handle multimodal input from text, audio, images, videos and large datasets. Gemini 2.5 Pro can also understand entire code repositories for coding projects. Gemini 2.5 Pro offers some of the largest context windows available for experimental models on Gemini. It ships with a 1 million token context window but will expand to 2 million tokens soon. Google AI Studio product manager Logan Kilpatrick posted on X that Gemini 2.5 Pro is “the first experimental model with higher rate limits + billing.” Google plans to release pricing for Gemini 2.5 models soon. Google said the model leads in advanced reasoning benchmark tests. The company said Gemini 2.5 Pro “leads in match and science benchmarks like GPQA and AIME 2025.” Kavukcuoglu said the model also scored “a state-of-the-art 18.8% across models without tool use on Humanity’s Last Exam,” a dataset aiming to capture human knowledge and reasoning. Gemini 2.5 Pro also performs strongly on coding tasks and scored better than Gemini 2.0 in specific benchmarks. Google noted the new model “excels at creating visually compelling web apps and agentic code applications, along with code transformation and editing.” Gemini 2.5 Pro enters the reasoning model fray in a significantly changed environment than Gemini 2.0 did in December. The release of DeepSeek’s reasoning large language model (LLM) DeepSeek-R1 showed that powerful models can perform well at a fraction of the training and compute cost. Furthermore, DeepSeek showed that open-source models can compete with more closed-source LLMs, such as OpenAI’s o1 and o3 models. Besides DeepSeek’s ever-expanding model offerings, Google has to compete with OpenAI’s reasoning models. While the newest model from OpenAI was GPT-4.5 —not a reasoning model—the company is still expected to develop more reasoning models soon. Gemini 2.5 is Google’s second new model this month. In March, the company released the latest version of its small language model, Gemma 3, which offered a 128,000 token context model and was best for use in on-the-go devices. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Providing consistent, reliable customer experiences with CQRS at scale</div>
                            <div class="article-source">VentureBeat - March 25, 2025 6:50 AM</div>
                            <div class="article-description">To maintain seamless customer experiences, Chase explains their transition to a modern, always-on ecosystem that scales with customer demand.</div>
                            <div class="article-content"><p>Presented by JPMorganChase As more Chase card customers embrace digital services, we’ve seen a surge in transaction-related inquiries. This increase has put pressure on our distributed backend systems to maintain seamless customer experiences, even during system outages at Systems of Record (SORs) or other layers. To address these challenges, we embarked on a modernization journey. Our goal was to transition from siloed environments to a modern, always-on ecosystem that scales with customer demand. This transformation allows customers to manage accounts, conduct transactions and access financial services through our online and mobile platforms without friction. The result? A significant technological advancement in distributed systems and big data, supporting Chase’s journey to unlock and accelerate new value for our business and customers. SORs which were predominantly mainframe-based and eventually evolved to modern technology stacks were designed to ensure reliability of command traffic. Data was ingested into data warehouses, the primary destination for most queries. With the emergence of real-time traffic through digital experiences, SORs began exposing their data to queries through APIs. Over time, the volume of query traffic grew significantly, often surpassing command traffic. Nowadays, it’s not uncommon for queries to constitute up to 90% of total SOR read volume. These strategic shifts have had profound effects on the cost, scalability and reliability of SORs, often contributing to operational issues. In our quest to mitigate operational issues posed by SORs and provide exceptional customer experiences, Chase adopted Command Query Responsibility Segregation (CQRS), a software architectural pattern that separates the responsibilities of handling commands (write operations) and queries (read operations) into distinct parts. Introduced by Greg Young around 2010, CQRS has gained significant traction in the software development community due to its ability to enhance scalability, performance and maintainability in complex systems. At Chase we built and implemented standards to achieve business continuity three to five times faster and improved customer experience, speed to market with new experiences and reliability. These standards include: The read layer in the CQRS pattern played a pivotal role in building a common data product by providing a unified and optimized view of data tailored to various user needs. By decoupling the read operations from the write processes, it allowed for the creation of specialized read models that can aggregate and present data from multiple sources, enabling consistent and efficient access to data across different applications and services. Embracing the challenge of adopting CQRS was immensely gratifying, and we successfully accomplished the following objectives on our journey: While CQRS offers numerous benefits it also introduces additional complexity, especially in terms of system design, implementation and operational overhead. We spent significant time carefully evaluating trade-offs, considering key factors such as eventual consistency, asynchronous communication and aligning with the principles of domain-driven design (DDD). Adopting CQRS has allowed us to operate at scale and with autonomy. It requires careful consideration of trade-offs and may not suit every project or team. As with any architectural pattern, understanding the principles and applying them judiciously is key to realizing the benefits of CQRS in software development. Like what you’re reading? about the Chase Product, Experience and Technology teams on Next at Chase. Amit Meshram is Executive Director, Principal Software Engineer at Chase.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Ex-Meta VP is raising back-to-back mega funding rounds from the same investors who made a fortune when Google bought Wiz</div>
                            <div class="article-source">Business Insider - 2025-03-24T20:57:48Z</div>
                            <div class="article-description">Roi Tiger, former vice-president of engineering at Meta, is in the process of closing a $55 million Series A funding round for a new cybersecurity startup.</div>
                            <div class="article-content"><p>How Twitter panic took down Silicon Valley Bank</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">‘Studio Ghibli’ AI image trend overwhelms OpenAI’s new GPT-4o feature, delaying free tier</div>
                            <div class="article-source">VentureBeat - March 26, 2025 4:14 PM</div>
                            <div class="article-description">The new feature has been widely embraced by users of X, but it raises copyright concerns and goes against Studio Ghibli&#x27;s creator.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More If you’ve been on the internet — or, at least, on the social network X — in the last day or so, you’ve likely come across colorful, smooth anime-style images of famous photographs rendered in the style of the Japanese studio, Studio Ghibli (the one that made Princess Mononoke, The Boy and the Crane, and My Neighbor Totoro, among many other classic animated films). In fact, some users are complaining because their feeds seem to be filled with nearly exclusively these types of images. Whether it’s current President Trump, the iconic image of the “Tank Man” during the 1989 pro-Democracy Tiananmen Square protests, Osama Bin Laden, Jeffrey Epstein, or even other pop culture moments and characters like Sam Rockwell’s iconic cameo on The White Lotus and many popular memes of yore, people have been making and sharing these images at a rapid clip. Much of that is thanks to OpenAI’s new update to the GPT-4o model behind ChatGPT for Pro, Plus, and Team subscription tiers, which turns on “native image generation.” While ChatGPT previously allowed users to create images from text prompts, it did so by routing them to another, separate OpenAI model, DALL-E 3. But OpenAI’s GPT-4o model is so named with an “o” because it is an “omni” model — the company trained it not only on text and code, but also on imagery and presumably, video and audio as well, allowing it to be able to understand all these forms of media and their similarities and differences, conceive of ideas across them (an “apple” is not just a word, but also something that can be drawn as a red or yellow or green fruit), and accurately produce said media given text prompts by a user without connecting to any external models. As a consequence, like rival Google AI Studio’s recent update to include a Gemini 2.0 Flash experimental image creation model, the new OpenAI GPT-4o can also accept image uploads of any pre-existing image in your camera roll or that you’ve screenshotted or saved off the web. First, navigate to Chat.com or ChatGPT.com and ensure you’re logged in with your ChatGPT Plus, Pro, or Team account and that the AI model selector (located in the left corner of the session window) is showing “GPT-4o” as the chosen model (you can click it to drop down and select the proper model between the available options). Once you do that, you can upload an image to ChatGPT using the “+” button in the lower left hand corner of the prompt entry text box, you can now ask the new GPT-4o with image creation model to render your pre-existing image in a new style. If you want, you can try it by uploading a photo of yourself and friends and typing “make all these people in the style of a Studio Ghibli animation.” And after a few seconds, it will do so with some pretty convincing and amusing results. It even supports attaching multiple images and combining them into a single piece. OpenAI initially said it would also enable this feature for free (non-paying users of ChatGPT), but unfortunately for them, co-founder and CEO Sam Altman today posted that the feature will be delayed due to the overwhelming demand by existing paying subscribers to ChatGPT Plus, Pro, and Team tiers. As he wrote on X: “images in chatgpt are wayyyy more popular than we expected (and we had pretty high expectations). rollout to our free tier is unfortunately going to be delayed for awhile.“ Meanwhile, those who do have access will likely continue cranking out image edits in this and other recognizable or novel styles. Of course, not everyone is a fan of OpenAI’s work here. In fact, Studio Ghibli creator Hayao Miyazaki himself appeared in a documentary back in 2016 — and one of the most memorable moments from it still referenced to this day is him reacting with overwhelming disgust and revulsion to an early example of AI-powered animation and physics by, you guessed it, an OpenAI model. As with many generative AI products and services, OpenAI’s training data for this new image generation capability remains under wraps, but is widely speculated to contain copyrighted material — and while imitating a style is generally not considered copyright infringement in the U.S., it is rubbing some fans of the original animation the wrong way. For now, those brands and enterprises looking to play with this style should do so with caution and after serious consideration, given the possible negative blowback among some users. But for those who are unabashedly pro-AI tools or with more forgiving and fun-loving fanbases, it’s clear that OpenAI has yet another hit on its hands. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Agentic AI is changing online meeting platforms: Moving from silent observer to active participant</div>
                            <div class="article-source">VentureBeat - March 25, 2025 6:00 AM</div>
                            <div class="article-description">Agentic AI is changing online meeting platforms and now thanks to Otter AI and other leading vendors it’s poised to change it even more.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Online meetings used to be much the same as their physical world counterparts. With the introduction of generative AI, online meeting platforms began to add new insights, including voice transcription services. Platform vendors, including Microsoft Teams, Google Meet and Cisco WebEx, have steadily integrated capabilities that go beyond what in-person physical meetings can provide. Now in the emerging era of agentic AI, online meetings are poised to diverge even further with a new wave of innovations. Zoom recently announced its agentic AI efforts which aims to create the paradigm shift from meetings to milestones. Microsoft has added copilot actions which can integrate with its Microsoft Teams service helping users inside of meetings to get insight and connect with other Microsoft services. Cisco has been steadily expanding its AI capabilities in Webex, and announced a Webex AI agent at the end of 2024 that helps with contact center deployments. Another firm that has been particularly active in the space is Otter AI, which is somewhat differentiated in that it is not directly tethered to any specific online meeting vendor platform. While Otter first made its mark as an AI-powered voice transcription service, it has added an AI assistant called Otter Pilot, an AI chat assistant, and a series of meeting capabilities known as Meeting GenAI. Today, Otter is going a step further, with its foray into agentic AI. While some of the agentic AI features that Otter is adding are not unique, it is doing at least one thing that isn’t yet part of every agentic AI meeting technology. Otter AI is now being integrated as an entity inside of a meeting that can actually respond by voice to queries. No longer is the AI an external participant accessible just via a chat window; AI is now a live entity that is literally part of the meeting. For the last several years, AI meeting assistants have been passive observers—transcribing conversations, creating summaries and allowing post-meeting queries. Otter is now changing this dynamic with its AI Meeting Agent, which can actively participate in conversations when summoned. “The new AI meeting agent we’re building will be able to help you with voice in real-time meetings,” Sam Liang, CEO of Otter AI told VentureBeat. “During the meeting, you can say, ‘Hey Otter,’ and ask it questions.” In a live demonstration with VentureBeat, Liang showed how the agent could answer factual questions, provide meeting summaries and even schedule follow-up meetings—all through voice commands during an active conversation. What makes this particularly powerful is the agent’s ability to connect to a company’s knowledge ecosystem. “This agent can become a domain expert,” Liang noted. “When you’re having a meeting, it has almost infinite knowledge from the internet, but this agent also has knowledge about your enterprise.” Taking agentic capability a step further, Otter is also launching an autonomous SDR (Sales Development Representative) agent that can independently conduct entire meetings without human intervention. This agent greets website visitors, conducts product demonstrations and schedules follow-up meetings with human sales representatives. “We cannot hire a million human agents to answer questions, but we built this Otter SDR agent that functions like a sales development representative who can greet every single visitor and give them a live demo,” Liang said. The use of chatbots and avatar-based systems is not new. Liang argued that what distinguishes his company’s technology from existing avatar-based solutions is its ability to conduct multimedia product demonstrations in real time. The autonomous agent can share screens, demonstrate product features and respond to specific questions about functionality and pricing. Agentic AI is an overloaded and somewhat overhyped term in the industry today overall. Functionally agentic AI is about enabling actions, which can be done by combining multiple models with a tool like LangChain or by using the function-calling capabilities present in many models. Liang has an even more nuanced definition for agentic AI. “Agents in general are a more sophisticated AI system that can break down a large and complicated task into smaller tasks,” he said. “It can do reasoning and it can do some planning to perform a task.” Otter is not using LangChain but has developed its own custom technology specifically designed for the challenges of multi-speaker voice environments. The technical architecture combines both public knowledge retrieval and proprietary enterprise information through a custom RAG (Retrieval-Augmented Generation) implementation. This enables the agent to understand company-specific information like employee names, project terminology and internal acronyms. What agentic AI is bringing to online meeting platforms represents a powerful new set of capabilities for organizational efficiency. For decades, organizational efficiency experts have warned about the risks of wasted time in meetings. Modern AI-powered platforms are changing that risk. Last week Zoom’s CTO told me that his goal was to move the technology from meetings to milestones, where the output of a meeting isn’t just another meeting but actionable things that will benefit the organization. While agentic AI can create workflows, there is still also benefit in regular AI assistants that are not actually agentic. There will still be standalone AI assistants and fully agentic ones and that’s a good thing, according to Anurag Dhingra, SVP &amp; GM, Enterprise Connectivity and Collaboration at Cisco. “While AI agents act as autonomous do-ers and AI assistants serve as prompted helpers, both offer benefits in boosting productivity and enhancing overall collaboration,” Dhingra told VentureBeat. “It’s not a matter of choosing one over the other but rather leveraging their combined strengths to create environments where teams can focus on innovation and strategic decision-making.” What is also starting to happen is more interoperability across different platforms. For example, Cisco’s AI Assistant will work with workflow applications such as Salesforce, ServiceNow and Outlook. For enterprises evaluating their AI adoption roadmap, Otter’s approach to meeting agents represents a strategic inflection point. Rather than implementing general-purpose AI and hoping for ROI, organizations should consider how domain-specific meeting agents can address concrete pain points with measurable impacts. As AI continues to evolve from tools we use, to colleagues we work with, the differentiation will increasingly be found not in the underlying models but in how effectively they’re trained to understand specific business contexts and domain knowledge. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">The concern with CoreWeave’s 250,000 Nvidia chips ahead of its IPO</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20255:00 PM EDT</div>
                            <div class="article-description">CoreWeave sells access to Nvidia graphics processors as a service, allowing developers to rent chips by the hour so they can develop more advanced AI models.</div>
                            <div class="article-content"><p>In this article With 250,000 highly-desired Nvidia graphics processors, CoreWeave has become one of the most prominent &quot;GPU clouds,&quot; a status it hopes investors will value when it debuts on the public markets. But the world of artificial intelligence hardware is moving so quickly that it raises questions about how long those chips will remain on the cutting edge and in demand. It&#x27;s a concern that could impact investor demand for shares of CoreWeave, one of the most anticipated IPOs in years. CoreWeave, which rents out remote access to computers based on Nvidia AI chips, said in a financial filing this month that most of its AI chips are from Nvidia&#x27;s Hopper generation. Those chips, such as the H100, were state-of-the-art in 2023 and 2024. They were scarce as AI companies bought or rented all the chips they could get in the wake of OpenAI ushering in the generative AI age with the release of ChatGPT in late 2022. But these days, Nvidia CEO Jensen Huang says that his company&#x27;s Hopper chips are getting blown out of the water by their successors – the Blackwell generation of GPUs, which have been shipping since late 2024. Hopper chips are &quot;fine&quot; for some circumstances but &quot;not many,&quot; Huang joked at Nvidia&#x27;s GTC conference last week. &quot;In a reasoning model, Blackwell is 40 times the performance of Hopper. Straight up. Pretty amazing,&quot; Huang said. &quot;I said before that when Blackwell starts shipping in volume, you couldn&#x27;t give Hoppers away.&quot; That&#x27;s great for Nvidia, which needs to find ways to keep selling chips to the companies committed to the AI race, but it&#x27;s bad news for GPU clouds like CoreWeave. That&#x27;s because the New Jersey company models the future trajectory of its business based on how much it anticipates being able to rent Nvidia chips out for over the next five to six years. Huang may have been kidding, but Nvidia spent much of its event detailing just how much better its Blackwell chips are. In Nvidia&#x27;s view, the best way to decrease the high cost of serving AI is by buying faster chips. Blackwell systems are in full production and shipping to customers, and Nvidia plans to introduce an upgraded version of Blackwell in late 2026. When new chips come out, the older chips — the kind CoreWeave has a quarter of a million of — go down in price, Huang said. So too does the price of renting them. Older chips don&#x27;t just stop working when new ones come out. Most companies, including CoreWeave, plan to use Hopper chips for six years. But Nvidia is telling customers that its newer, faster chips are capable of producing more AI content, which leads to more revenues at a better margin for clouds. An H100 would have to be priced 65% lower per hour than an Nvidia Blackwell GB200 NVL system for the two systems to be competitive in price per output to a renter. Put another way, the H100 would have to rent at 98 cents per hour to match the price per output of a Blackwell rack system priced at $2.20 per hour per GPU, SemiAnalysis estimated, speaking generally about AI rentals. H100s rented for as much as $8 per hour back in 2023 and often required long commitments and lead times, but now, usage of those chips can be summoned in minutes with a credit card. Some services now offer rented H100 access for under $2 per hour. The industry could be entering a period where the useful life of AI chips is reduced, Barclays analyst Ross Sandler wrote in a note on Friday. He was focused on hyperscalers — Meta, Google and Amazon — but the trend affects smaller cloud providers like CoreWeave, too. &quot;These assets are becoming obsolete at a much more rapid pace given how much innovation and speed improvements happen with each generation,&quot; Sandler wrote. This threatens company earnings if they end up depreciating older equipment faster, he said. CoreWeave says that if there were to be changes to the &quot;significant&quot; assumptions it makes about the useful lifetime of its AI infrastructure, it could hurt its business or future prospects. CoreWeave has also borrowed nearly $8 billion to buy Nvidia chips and build its data centers, sometimes using the GPUs it amassed as collateral. Analysts and investors are also increasingly asking questions about the useful lifespan of these new AI systems and whether their financial depreciation schedules should be accelerated because the technology is improving so fast. CoreWeave says in its filing that it seeks to offer state-of-the-art infrastructure and says it will continue spending to expand and improve its data centers. &quot;Part of this process entails cycling out outdated components of our infrastructure and replacing them with the latest technology available,&quot; the New Jersey company said. &quot;This requires us to make certain estimates with respect to the useful life of the components of our infrastructure and to maximize the value of the components of our infrastructure, including our GPUs, to the fullest extent possible.&quot; CoreWeave and Nvidia maintain a good relationship. CoreWeave will certainly buy more chips from Nvidia, which owns more than 5% of the New Jersey company. &quot;We&#x27;re super proud of them,&quot; Huang said last week. But Nvidia&#x27;s road map for releasing new chips that it proudly touts will make their predecessors obsolete is a threat to CoreWeave&#x27;s ambitions. WATCH: CoreWeave begins marketing IPO, targeting price range of $47-$55 per share: Report</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">U.S. blacklists over 50 Chinese companies in bid to curb Beijing&#x27;s AI, chip capabilities</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 202512:56 AM EDT</div>
                            <div class="article-description">The export restrictions come at a time when tensions between Washington and Beijing have been rising with the Trump administration ratcheting up tariffs against China.</div>
                            <div class="article-content"><p>The U.S. on Tuesday added dozens of Chinese tech companies to its export blacklist in its first such effort under the Donald Trump administration, as it doubles down on curtailing Beijing&#x27;s artificial intelligence and advanced computing capabilities. The U.S. Department of Commerce&#x27;s Bureau of Industry and Security added 80 organizations to an &quot;entity list,&quot; with more than 50 from China, barring American companies from supplying to those on the list without government permits. The companies were blacklisted for allegedly acting contrary to U.S. national security and foreign policy interests, the agency said, as part of its efforts to further restrict Beijing&#x27;s access to exascale computing tech, which can process vast amounts of data at very high speeds, as well as quantum technologies. Dozens of Chinese entities were targeted for their alleged involvement in developing advanced AI, supercomputers and high-performance AI chips for military purposes, the Commerce Department said, adding that two firms were supplying to sanctioned entities such as Huawei and its affiliated chipmaker HiSilicon. It blacklisted 27 Chinese entities for acquiring U.S.-origin items to support China&#x27;s military modernization and seven firms for helping advance China&#x27;s quantum technology capabilities. Among the organizations in the &quot;entity list&quot; were also six subsidiaries of Chinese cloud-computing firm Inspur Group, which had been blacklisted by the Joe Biden administration in 2023. China&#x27;s foreign ministry said late Wednesday it &quot;strongly condemns&quot; the export restrictions while urging the U.S. to &quot;stop generalizing national security,&quot; Reuters reported. The latest additions &quot;cast an ever-widening net aimed at third countries, transit points and intermediaries,&quot; said Alex Capri, a senior lecturer at National University of Singapore and author of &quot;Techno-Nationalism: How It&#x27;s Reshaping Trade, Geopolitics and Society.&quot; Chinese firms have managed to gain access to U.S. strategic dual-use technologies via certain third parties, he said, referring to loopholes that have allowed Chinese companies access to U.S. technologies despite restrictions. &quot;U.S. officials will continue to step up tracking and tracing operations aimed at the smuggling of advanced semiconductors made by Nvidia and Advanced Micro Devices,&quot; he said. The expanded export restrictions come at a time when tensions between Washington and Beijing have been rising with the Trump administration ratcheting up tariffs against China. The rapid rise of Chinese AI startup DeepSeek has boosted the adoption of open-source low-cost AI models in China, putting pressure on leading U.S. competitors with higher-cost, proprietary models. The Biden administration imposed sweeping export controls against China, encompassing everything from semiconductors to supercomputers under the so-called &quot;small yard, high fence&quot; policy. The approach aims to place restrictions on a small number of technologies with significant military potential while maintaining normal economic exchange in other areas. Under Secretary of Commerce for Industry and Security Jeffrey I. Kessler said the agency was &quot;sending a clear, resounding message&quot; that the Trump administration will prevent U.S. technologies from &quot;being misused for high performance computing, hypersonic missiles, military aircraft training, and UAVs (unmanned aerial vehicle) that threaten our national security.&quot; &quot;The entity list is one of many powerful tools at our disposal to identify and cut off foreign adversaries seeking to exploit American technology for malign purposes,&quot; he added. Inspur Group and Huawei did not immediately respond to CNBC&#x27;s requests for comment.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Trust Wallet Reaches 200 Million Downloads Milestone</div>
                            <div class="article-source">VentureBeat - March 26, 2025 9:25 AM</div>
                            <div class="article-description">Press Release Trust Wallet, the world’s leading self-custody Web3 wallet, has surpassed 200 million total downloads, marking a game-changing milestone in the industry. Trust Wallet stands as the most widely used non-custodial wallet globally for onchain users, cementing…</div>
                            <div class="article-content"><p>With this milestone, Trust Wallet cements its position as the #1 crypto wallet  DUBAI, United Arab Emirates–(BUSINESS WIRE)–March 26, 2025– Trust Wallet, the world’s leading self-custody Web3 wallet, has surpassed 200 million total downloads, marking a game-changing milestone in the industry. Trust Wallet stands as the most widely used non-custodial wallet globally for onchain users, cementing its role as a key gateway to Web3. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20250325066589/en/ Trust Wallet Reaches 200 Million Downloads Milestone Since its launch in 2017, Trust Wallet has played a pivotal role in onboarding millions into crypto. Initially introduced as an Ethereum wallet, it has evolved into a chain-agnostic, multi-chain Web3 hub, now supporting over 10 millions assets across 100+ blockchains, along with a suite of features that empower users to navigate their entire Web3 journey-from buying their first cryptocurrency to swapping, staking, exploring the decentralized web, and beyond. Eowyn Chen, CEO of Trust Wallet, commented on the achievement: “Reaching 200 million downloads is a real testament to the trust from the users. In a rapidly evolving industry, our mission has remained the same: empower people with freedom to own and access opportunities. We’re proud of this milestone, but even more humbled and excited about the future as we have many things on the roadmap for our global community. We got to work harder.” Trust Wallet has carved out a significant space for itself in the competitive landscape of cryptocurrency wallets. This success can be attributed to a combination of core principles that focus on user experience, community, trust and security. What’s Fuelling Trust Wallet’s Growth? With millions of users worldwide and a fast-growing community, Trust Wallet continues to expand its reach through compelling features, product innovations, and user-centric initiatives. Its recent growth and success points to a relentless focus on usability, innovation, and security. The wallet strikes a balance between onboarding new users and offering advanced tools for experienced users. Examples of Trust Wallet’s innovations include: Building a Future-Proof Web3: Trust Wallet’s Vision and Beyond As the on-chain economy evolves and AI-driven innovations take shape, Trust Wallet is focused on bridging the gap between Web2 simplicity and Web3 autonomy. The goal is to make decentralized finance (DeFi) and digital ownership more intuitive, secure, and accessible for millions of users. Web3 isn’t just about holding assets-it’s about seamless, intelligent, and secure interactions across decentralized applications (dApps), finance, gaming, and beyond. Trust Wallet continues to expand its capabilities to give users the tools and insights needed to navigate the decentralized world with confidence. Key Focus Areas for 2025: By improving usability, security, and intelligence, Trust Wallet is ensuring that more people can explore and benefit from the decentralized economy with confidence. About Trust Wallet Trust Wallet is the secure, self-custody Web3 wallet and gateway for people who want to fully own, control, and leverage the power of their digital assets. From beginners to experienced users, Trust Wallet makes it easier, safer, and convenient for millions of people around the world to experience Web3, access dApps securely, store and manage their crypto and NFTs, as well as buy, sell, and stake crypto to earn rewards – all in one place and without limits.  View source version on businesswire.com: https://www.businesswire.com/news/home/20250325066589/en/ For media enquiries, contact:press@trustwallet.com If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Meet a decades-old software company hitching a ride on the Nvidia rocket ship</div>
                            <div class="article-source">Business Insider - 2025-03-26T09:00:01Z</div>
                            <div class="article-description">DDN was invited on the rocket ship that is Nvidia just a few years ago — and everything changed.</div>
                            <div class="article-content"><p>Volkswagen is using AI to speed up and scale marketing, while also integrating ChatGPT into its vehicles, says CMO Susanne Franz</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Former Intel CEO Pat Gelsinger jumps to venture capital, joins Playground Global</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20257:00 PM EDT</div>
                            <div class="article-description">After retiring as Intel&#x27;s CEO in December, Pat Gelsinger is becoming a general partner at early-stage venture firm Playground Global.</div>
                            <div class="article-content"><p>In this article After a tumultuous four years running Intel, Pat Gelsinger is going into venture capital. Gelsinger, who was ousted by the chipmaker in December, has joined Playground Global as a general partner. Started in 2015 by a group that included Android founder Andy Rubin, Playground focuses on early-stage investments in deep technology. Gelsinger told CNBC in an interview that he considered starting a venture firm with someone else, but opted to go with a structure that was already up and running. &quot;It&#x27;s about scale,&quot; Gelsinger said, adding that starting from scratch would require &quot;10 hard years to get it.&quot; Before joining Playground, Gelsinger made a handful of private investments in startups including church outreach software startup Gloo, wearable maker Oura and artificial intelligence chip developer Fractile. At Playground, he&#x27;ll join the board of portfolio company xLight, which is developing lasers for semiconductor manufacturing. Gelsinger is entering VC after 45 years in the technology industry. He spent three decades at Intel, becoming its first chief technology officer, and left in 2009 for data center hardware maker EMC. He later led server virtualization company VMware. In 2021, with Intel struggling from delays in releasing new generations of processors, he rejoined the company as CEO. Under Gelsinger, Intel focused on semiconductor fabrication, pouring money into an effort to develop chips for other companies. In 2024, the Biden administration awarded Intel up to $8.5 billion in CHIPS and Science Act funding as part of a plan to bring chip manufacturing back to the U.S. But Intel lost market share and got trounced by Nvidia in AI, prompting a massive selloff in its stock price. Intel&#x27;s market cap plummeted by 60% in 2024, its worst performance in over five decades as a public company. In December, Intel announced Gelsinger&#x27;s retirement. Earlier this month, the company said Lip-Bu Tan, a former CEO of Cadence Design Systems, will take over as CEO. Gelsinger isn&#x27;t the first ex-Intel CEO to find his way to venture. His predecessor, Bob Swan, became a growth operating partner at venture firm Andreessen Horowitz in 2021, a few months after leaving the chipmaker. Gelsinger said he&#x27;s looking forward to seeing this week&#x27;s stock market debut of CoreWeave, which rents out Nvidia graphics processing units (GPUs) to Microsoft, Nvidia and OpenAI. &quot;Obviously they&#x27;ve been able to ride the wave of at-scale data centers for AI computing,&quot; Gelsinger said. &quot;There are multiple participants who are trying to do it. They did the best in that. The question is, what&#x27;s their sustainable differentiation?&quot; Another technology of interest, Gelsinger said, is quantum computing. Unlike classical computers that store data in bits that are either on or off, quantum computers operate with quantum qubits, or qubits, that can be in both states at the same time. The hope among quantum bulls is that the technology might be able to perform certain calculations that have stymied today&#x27;s machines. Amazon and Microsoft have both had their latest claims published in the journal Nature. Gelsinger said he looks forward to working on quantum computing with PsiQuantum, a Playground portfolio company. PsiQuantum is raising $750 million or more in fresh capital at a $6 billion valuation, with BlackRock planning to lead the round, CNBC confirmed. Reuters reported about the fundraising efforts on Monday. Quantum computers will be &quot;materially impacting computing structures before the end of this decade,&quot; Gelsinger said. In February, the U.S. Defense Advanced Research Projects Agency (DARPA) said it will evaluate whether quantum systems from PsiQuantum and Microsoft will be more valuable than they cost by 2033. The release didn&#x27;t mention Intel, which announced its inaugural quantum chip, codenamed Tunnel Falls, in 2023. Gelsinger said he wishes the best to his former employer and Tan, its new leader. &quot;I certainly believe that Intel is critical for the semiconductor industry,&quot; he said. &quot;You need to design and manufacture leading-edge technology.&quot; — CNBC&#x27;s Kate Rooney contributed to this report. WATCH: Intel shares fall after CEO leaves</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Googlers grilled leaders about smaller pay bumps in a recent all-hands</div>
                            <div class="article-source">Business Insider - 2025-03-26T23:12:06Z</div>
                            <div class="article-description">At a recent Google all-hands meeting, employees question leadership on 2025 compensation packages.</div>
                            <div class="article-content"><p>How tech layoffs could affect the economy</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Trump says he may reduce China tariffs to help close a TikTok deal</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20257:06 PM EDT</div>
                            <div class="article-description">&quot;Maybe I&#x27;ll give them a little reduction in tariffs or something to get it done,&quot; President Donald Trump said about a deal involving TikTok&#x27;s U.S. operations</div>
                            <div class="article-content"><p>President Donald Trump said he may reduce tariffs on China to facilitate a deal that would result in ByteDance selling the U.S. operations of TikTok. China &quot;is going to have to play a role&quot; in approving a TikTok-related divestiture, Trump said in a press conference Wednesday. &quot;Maybe I&#x27;ll give them a little reduction in tariffs or something to get it done,&quot; Trump said. &quot;TikTok is big, but every point in tariffs is worth more than TikTok.&quot; Although a national security law requires ByteDance to divest TikTok&#x27;s U.S. operations or face an effective ban in the country, Trump in January signed an executive order that delayed the deadline for a deal to April 5. Trump has previously said that he wants the U.S. to maintain a 50% ownership position in TikTok via a joint venture. It&#x27;s possible he will extend the TikTok deadline again, Trump said Wednesday. &quot;We&#x27;re going to have a form of a deal, but if it&#x27;s not finished, it&#x27;s not a big deal,&quot; Trump said. &quot;We&#x27;ll just extend it.&quot; Vice President JD Vance told NBC News earlier this month that he was confident that a TikTok-related deal would happen by the April deadline. &quot;There will almost certainly be a high-level agreement that I think satisfies our national security concerns, allows there to be a distinct American TikTok enterprise,&quot; Vance said. WATCH: TikTok bid is &#x27;in active dialogue&#x27; with Trump administration.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Former Intel CEO Pat Gelsinger jumps to venture capital, joins Playground Global</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20257:00 PM EDT</div>
                            <div class="article-description">After retiring as Intel&#x27;s CEO in December, Pat Gelsinger is becoming a general partner at early-stage venture firm Playground Global.</div>
                            <div class="article-content"><p>In this article After a tumultuous four years running Intel, Pat Gelsinger is going into venture capital. Gelsinger, who was ousted by the chipmaker in December, has joined Playground Global as a general partner. Started in 2015 by a group that included Android founder Andy Rubin, Playground focuses on early-stage investments in deep technology. Gelsinger told CNBC in an interview that he considered starting a venture firm with someone else, but opted to go with a structure that was already up and running. &quot;It&#x27;s about scale,&quot; Gelsinger said, adding that starting from scratch would require &quot;10 hard years to get it.&quot; Before joining Playground, Gelsinger made a handful of private investments in startups including church outreach software startup Gloo, wearable maker Oura and artificial intelligence chip developer Fractile. At Playground, he&#x27;ll join the board of portfolio company xLight, which is developing lasers for semiconductor manufacturing. Gelsinger is entering VC after 45 years in the technology industry. He spent three decades at Intel, becoming its first chief technology officer, and left in 2009 for data center hardware maker EMC. He later led server virtualization company VMware. In 2021, with Intel struggling from delays in releasing new generations of processors, he rejoined the company as CEO. Under Gelsinger, Intel focused on semiconductor fabrication, pouring money into an effort to develop chips for other companies. In 2024, the Biden administration awarded Intel up to $8.5 billion in CHIPS and Science Act funding as part of a plan to bring chip manufacturing back to the U.S. But Intel lost market share and got trounced by Nvidia in AI, prompting a massive selloff in its stock price. Intel&#x27;s market cap plummeted by 60% in 2024, its worst performance in over five decades as a public company. In December, Intel announced Gelsinger&#x27;s retirement. Earlier this month, the company said Lip-Bu Tan, a former CEO of Cadence Design Systems, will take over as CEO. Gelsinger isn&#x27;t the first ex-Intel CEO to find his way to venture. His predecessor, Bob Swan, became a growth operating partner at venture firm Andreessen Horowitz in 2021, a few months after leaving the chipmaker. Gelsinger said he&#x27;s looking forward to seeing this week&#x27;s stock market debut of CoreWeave, which rents out Nvidia graphics processing units (GPUs) to Microsoft, Nvidia and OpenAI. &quot;Obviously they&#x27;ve been able to ride the wave of at-scale data centers for AI computing,&quot; Gelsinger said. &quot;There are multiple participants who are trying to do it. They did the best in that. The question is, what&#x27;s their sustainable differentiation?&quot; Another technology of interest, Gelsinger said, is quantum computing. Unlike classical computers that store data in bits that are either on or off, quantum computers operate with quantum qubits, or qubits, that can be in both states at the same time. The hope among quantum bulls is that the technology might be able to perform certain calculations that have stymied today&#x27;s machines. Amazon and Microsoft have both had their latest claims published in the journal Nature. Gelsinger said he looks forward to working on quantum computing with PsiQuantum, a Playground portfolio company. PsiQuantum is raising $750 million or more in fresh capital at a $6 billion valuation, with BlackRock planning to lead the round, CNBC confirmed. Reuters reported about the fundraising efforts on Monday. Quantum computers will be &quot;materially impacting computing structures before the end of this decade,&quot; Gelsinger said. In February, the U.S. Defense Advanced Research Projects Agency (DARPA) said it will evaluate whether quantum systems from PsiQuantum and Microsoft will be more valuable than they cost by 2033. The release didn&#x27;t mention Intel, which announced its inaugural quantum chip, codenamed Tunnel Falls, in 2023. Gelsinger said he wishes the best to his former employer and Tan, its new leader. &quot;I certainly believe that Intel is critical for the semiconductor industry,&quot; he said. &quot;You need to design and manufacture leading-edge technology.&quot; — CNBC&#x27;s Kate Rooney contributed to this report. WATCH: Intel shares fall after CEO leaves</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">23andMe co-founder lashes out at CEO Wojcicki after bankruptcy filing, says board lacked oversight</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20255:24 PM EDT</div>
                            <div class="article-description">23andMe co-founder Linda Avey expressed frustration about the fall of the genetic testing company in a social media post.</div>
                            <div class="article-content"><p>In this article 23andMe co-founder Linda Avey took to social media on Wednesday to express frustration about the fate of the once-thriving genetic testing company that spiraled into Chapter 11 bankruptcy protection this week. Avey helped launch 23andMe in 2006 alongside Paul Cusenza and Anne Wojcicki, who resigned as CEO on Friday. The company went mainstream due to its popular at-home DNA testing kits, but struggled in recent years to generate recurring revenue, stand up viable therapeutics and research businesses and assuage privacy concerns. &quot;My time at the company was cut short in 2009, when my co-founder Anne convinced the board that she should run the company,&quot; Avey wrote in a post on social media site X. &quot;And I must be honest, I was frustrated with the direction the company took after that point.&quot; 23andMe, which reached a peak market cap of about $6 billion, was worth around $14 million as of market close on Wednesday. &quot;Without continued consumer-focused product development, and without governance, 23andMe lost its way, and society missed a key opportunity in furthering the idea of personalized health,&quot; Avey wrote. Last March, 23andMe&#x27;s independent directors formed a special committee to evaluate the company&#x27;s potential paths forward. All seven members resigned from the board in September and said they disagreed with Wojcicki about the &quot;strategic direction for the company.&quot; &quot;After my departure, she architected a majority vote for herself that eliminated board governance, even as it expanded over the following funding rounds,&quot; Avey said. &quot;For better or worse, the buck stopped with her. It came as no surprise when the board resigned last year.&quot; Wojcicki submitted multiple proposals to take the company private herself, but all were rejected, even after the company appointed new board members. The special committee &quot;unanimously determined to reject&quot; Wojcicki&#x27;s most recent proposal earlier this month. If 23andMe&#x27;s Chapter 11 plan is approved by the court, the company will &quot;actively solicit qualified bids&quot; over a 45-day process. Wojcicki still plans to pursue the company as an independent bidder, she said in a post on X on Monday. &quot;There are many cautionary tales buried in the 23andMe story,&quot; Avey said. &quot;Striking a balance between the desire for founder control and board oversight is essential; otherwise, why have a board at all?&quot; 23andMe did not immediately respond to CNBC&#x27;s request for comment. WATCH: The rise and fall of 23andMe</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">The best iPhone 16e cases: Top picks for protecting your new phone</div>
                            <div class="article-source">Business Insider - 2025-03-26T14:00:01Z</div>
                            <div class="article-description">The best iPhone 16e cases offer durable, practical, and visually appealing designs compatible with Qi wireless charging.</div>
                            <div class="article-content"><p>The best iPhone 16e cases offer durable, practical, and visually appealing designs compatible with Qi wireless charging. To help you decide which case best suits your needs, we&#x27;ve arranged our favorite available options from Apple and third-party brands like Zagg, Otterbox, Dbrand, Spigen, and others. Among the best iPhone 16e cases, our top pick is the Zagg Crystal Palace Snap case, a sturdy, clear model with an integrated array of magnets. Our favorite budget case is the affordable Spigen Tough Armor case, with its versatile design and raised edges for screen and camera protection. When you buy through our links, Business Insider may earn an affiliate commission. Learn more As noted in our iPhone 16e review, it&#x27;s wise to protect Apple&#x27;s latest budget phone with a high-quality case. Not even the best iPhone is impervious to accidental damage, and the aluminum edges and glass back of the iPhone 16e offer no exception. Coupled with a screen protector, a case remains the most cost-effective way to avoid the hassle of warranty repairs or the significant cost of replacing a phone outright. See our guide to the best iPhone 16e screen protectors for top options. Also, if you intend to use the iPhone 16e with magnetic accessories, you&#x27;ll need a case with a built-in magnetic array, as the phone notably lacks Apple&#x27;s MagSafe technology. As with any of the best phones, it&#x27;s important to consider cases that will provide adequate protection and functionality for your daily use of the iPhone 16e. Start by considering the level of protection that best compliments your life circumstances. Rugged cases with raised edges and shock-resistant materials are ideal for heavy-duty use or those prone to drops. In contrast, slim cases with moderate protection may be better suited to those looking for a lighter, less bulky option. It&#x27;s also worth ensuring that the case you choose will not interfere with features you want to use, such as Qi wireless charging. While the iPhone 16e lacks MagSafe, cases with integrated magnets can make the phone compatible with magnetic accessories and magnetic wireless chargers. However, the phone will wirelessly charge at slower speeds (around 7.5W) relative to the flagship iPhone 16 series and most recent iPhones, which feature the MagSafe charging standard (up to 25W). A case meant for the iPhone 16 will not fit the iPhone 16e, and vice versa, due to differences in their designs and dimensions. While the two phones share the same width and depth, the iPhone 16e is slightly shorter than the iPhone 16, has a distinct single-camera system, and lacks the flagship phone&#x27;s Camera Control button. See our guide to the best iPhone 16 cases to protect the flagship phone, and read our full iPhone 16 review or iPhone 16e vs. iPhone 16 guide for a comprehensive comparison of the models. You can purchase logo and accolade licensing to this story here.Disclosure: Written and researched by the Insider Reviews team. We highlight products and services you might find interesting. If you buy them, we may get a small share of the revenue from the sale from our partners. We may receive products free of charge from manufacturers to test. This does not drive our decision as to whether or not a product is featured or recommended. We operate independently from our advertising team. We welcome your feedback. Email us at reviews@businessinsider.com.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Dell&#x27;s staff numbers have dropped by 25,000 in just 2 years</div>
                            <div class="article-source">Business Insider - 2025-03-26T16:52:40Z</div>
                            <div class="article-description">Dell&#x27;s latest SEC filing shows that head count fell from 133,000 in 2023 to 108,000 now, a fall of 19%.</div>
                            <div class="article-content"><p>Nearly 50,000 tech workers have been laid off — but there&#x27;s a hack to avoid layoffs</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Trump says Tesla CEO Elon Musk didn’t advise on auto tariffs &#x27;because he may have a conflict&#x27;</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20257:31 PM EDT</div>
                            <div class="article-description">When asked by reporters whether the new tariffs would be good for Musk&#x27;s autos business, Tesla, President Donald Trump said they may be &quot;net neutral or they may be good.&quot;</div>
                            <div class="article-content"><p>In this article After President Donald Trump said on Wednesday he would impose 25% tariffs on &quot;all cars that are not made in the United States,&quot; he said his key advisor, Tesla CEO Elon Musk, had not weighed in on the matter, &quot;because he may have a conflict.&quot; He added that Musk had never &quot;asked me for a favor in business whatsoever.&quot; Musk serves as a senior advisor to Trump, having earlier contributed $290 million to propel him back to the White House. While Musk remains at the helm of his companies, including SpaceX and Tesla, he is also leading the Department of Government Efficiency (DOGE), which is an effort to slash federal government spending, personnel and consolidate or eliminate various federal agencies and services. Earlier this month, Trump turned the South Lawn of the White House into a temporary Tesla showroom. The company delivered five of its electric vehicles there for the president to inspect after he had declared, in a post on Truth Social, that he would buy a Tesla to show support for Musk and the business. Musk stood by his side while Trump called the vehicles &quot;beautiful&quot; and praised the unorthodox design of the angular, steel Tesla Cybertruck. When asked by reporters whether the new tariffs would be good for Musk&#x27;s autos business, Tesla, Trump said they may be &quot;net neutral or they may be good.&quot; He pointed to Tesla&#x27;s vehicle assembly plants in Austin, Texas and Fremont, California and opined that, &quot;anybody that has plants in the United States — it&#x27;s going to be good for them.&quot; Tesla recently wrote, in a letter to the U.S. Trade Representative, that &quot;even with aggressive localization&quot; of its supply chain domestically, &quot;certain parts and components are difficult or impossible to source within the United States.&quot; The company urged the USTR to &quot;consider the downstream impacts of certain proposed actions taken to address unfair trade practices.&quot; Tesla and other automakers commonly buy headlamps, automotive glass, brakes, body panels, suspension parts, and printed circuit boards for various electrical systems in their vehicles from foreign suppliers in Mexico, Canada and China, especially. Musk and Tesla did not immediately respond to a request for comment about how the new 25% tariffs may impact their business. Tesla faces an onslaught of competition with more automakers selling fully electric models than ever before. However, the company&#x27;s most formidable rival in battery electric vehicles, BYD in China, has never been authorized to sell its electric cars in the United States. Domestic automakers including General Motors, Ford, Rivian and Tesla saw shares declining slightly after hours following the latest tariffs announcement.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Groq and PlayAI just made voice AI sound way more human — here’s how</div>
                            <div class="article-source">VentureBeat - March 26, 2025 8:30 AM</div>
                            <div class="article-description">Groq partners with PlayAI to deliver Dialog, an emotionally intelligent text-to-speech model that runs 10x faster than real-time speech, including the Middle East&#x27;s first Arabic voice AI model.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Groq and PlayAI announced a partnership today to bring Dialog, an advanced text-to-speech model, to market through Groq’s high-speed inference platform. The partnership combines PlayAI’s expertise in voice AI with Groq’s specialized processing infrastructure, creating what the companies claim is one of the most natural-sounding and responsive text-to-speech systems available. “Groq provides a complete, low latency system for automatic speech recognition (ASR), GenAI, and text-to-speech, all in one place,” said Ian Andrews, Chief Revenue Officer at Groq, in an exclusive interview with VentureBeat. “With Dialog now running on GroqCloud, this means customers won’t have to use multiple providers for a single use case — Groq is a one stop solution.” Dialog is notable for being available in both English and Arabic, with the Arabic version representing the first voice AI specifically designed for the Middle East region. The inclusion of Arabic as one of the initial offerings was strategic for both companies. “Arabic is the fourth most spoken language globally — by partnering with PlayAI to offer an Arabic TTS model, Groq is unlocking a key global market and enabling broader access to fast AI inference,” Andrews told VentureBeat. The companies claim their solution addresses key shortcomings in existing voice AI technologies, particularly around natural speech patterns and response speed. According to benchmark testing conducted by third-party evaluator Podonos, Dialog was preferred by users at a rate of 10:1 versus ElevenLabs v2.5 Turbo and over 3:1 against ElevenLabs Multilingual v2.0. What sets Dialog apart is its sophisticated approach to context. Rather than treating each vocalization as an isolated event, the system maintains awareness of the entire conversation flow. “We built a novel architecture that we call an ‘adaptive speech contextualizer‘ (ASC), which allows the model to use the full context and history of a conversation,” said Mahmoud Felfel, co-founder and CEO of PlayAI, in an interview with VentureBeat. “This means that every response isn’t just a standalone output; it’s enriched with appropriate prosody, tone, and emotion that reflect the flow of the conversation.” For enterprises looking to implement conversational AI, latency — the delay between request and response — has been a persistent challenge. Groq’s specialized Language Processing Units (LPUs) appear to provide a significant advantage in this area. “Based on initial internal testing, Groq is delivering up to 140 characters per second on PlayAI’s Dialog model, a significant boost compared to the same model running on GPUs at 86 characters per second,” explained Andrews. “That means that Dialog generates text up to 10 times faster than real-time.” The partnership comes at a time of significant expansion for Groq, which recently secured a $1.5 billion commitment from Saudi Arabia to fund additional infrastructure. The company has established a data center in Dammam, which it describes as “the region’s largest inference cluster.” “Partnering with Groq was a no-brainer; they’re the industry leader in advanced AI inference infrastructure,” said Felfel. “With TTS and agents, low latency is key. We’ve already optimized Dialog for these real-time applications, but partnering with Groq allows us to deliver the lowest latency voice model on the market.” The voice AI market has seen rapid growth as businesses look to automate customer interactions while maintaining a natural, human-like experience. Applications range from customer service and sales automation to voice-overs and accessibility features for the visually impaired. “Beyond customer service, other enterprise use cases include automating sales and appointment scheduling, on-boarding and personal assistants, creating voice overs to existing content, translating English audio and video content into Arabic, increasing website and static content accessibility for the visually impaired, and more,” Andrews said. For PlayAI, which was founded by entrepreneurs from the Middle East and North Africa region, the inclusion of Arabic language capabilities was particularly meaningful. “As MENA founders, we know the region is heavily investing in AI capabilities and infrastructure as inflected in investments like Groq, but also world-leading adoption,” said Felfel. “Arabic is a global business language and one that we grew up speaking, so it was a natural choice as one of our core languages.” The companies have made the Dialog technology available through GroqCloud’s tiered service model, which includes both free and paid options. This approach allows developers to experiment with the technology before committing to larger implementations. “GroqCloud offers both free and paid plans. Anyone can create an account and create an API code for free,” Andrews explained. “Our paid Developer Tier is self-serve, meaning anyone with a credit card can sign up themselves.” As voice becomes an increasingly important interface for AI systems, this partnership positions both companies to capitalize on the growing demand for more natural and responsive conversational experiences. By addressing the technical challenges of latency and natural speech patterns, Groq and PlayAI may have removed significant barriers to wider adoption of voice AI in enterprise settings. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Powerful Medical Receives FDA Breakthrough Device Designation for PMcardio STEMI AI ECG Model</div>
                            <div class="article-source">VentureBeat - March 25, 2025 10:25 AM</div>
                            <div class="article-description">Press Release Powerful Medical, a leader in AI-driven cardiovascular diagnostics, announces that its PMcardio STEMI AI ECG model has been granted Breakthrough Device Designation by the US Food and Drug Administration (FDA). This designation recognizes PMcardio as a…</div>
                            <div class="article-content"><p>NEW YORK–(BUSINESS WIRE)–March 25, 2025– Powerful Medical, a leader in AI-driven cardiovascular diagnostics, announces that its PMcardio STEMI AI ECG model has been granted Breakthrough Device Designation by the US Food and Drug Administration (FDA). This designation recognizes PMcardio as a breakthrough technology for the detection of ST-elevation myocardial infarction (STEMI) and STEMI equivalents-a life-threatening cardiac condition requiring immediate intervention. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20250325333487/en/ Powerful Medical Receives FDA Breakthrough Device Designation for PMcardio STEMI AI ECG Model Every 40 seconds, someone in the US suffers from a heart attack¹, where rapid diagnosis is crucial to saving heart muscle and preventing long-term damage, often leading to higher mortality rates. The ECG remains the primary tool for rapid diagnostics, yet the standard of care often fails to detect heart attacks accurately and timely, resulting in critical delays in treatment. PMcardio is the only solution capable of detecting both STEMI and STEMI equivalents on the ECG-aligning with the emerging emphasis on a paradigm shift towards Occlusion Myocardial Infarction (OMI) diagnosis and bridging a vital gap in early, precise heart attack diagnosis. “For the last 20 years, life-saving treatment exists for heart attack patients, yet far too many still don’t receive the urgent care they need due to delays in diagnosis and inefficient triage,” said Robert Herman, MD, PhD, Chief Medical Officer of Powerful Medical. This is especially critical in settings where immediate specialist evaluation isn’t available-only 17% of patients presenting to rural centers make it to the catheterization lab in time for intervention.² Dr. Herman added, “By equipping physicians and allied providers with an AI-powered tool for accurate and immediate STEMI detection, available around the clock, we can bridge this gap, ensure timely treatment, and improve patient outcomes, often preventing avoidable deaths”. The FDA’s Breakthrough Device Designation provides PMcardio with an expedited review process and close collaboration with the agency on its path toward market authorization. This designation is reserved for technologies that offer significant advantages over existing solutions and address unmet medical needs. This recognition underscores the FDA’s acknowledgment of Powerful Medical’s STEMI AI ECG Model, dubbed “Queen of Hearts”, to set a new standard in frontline heart attack detection and triage, ultimately enhancing care quality, accelerating treatment decisions, and saving lives through earlier and more accurate diagnosis. “FDA Breakthrough Device Designation is a pivotal milestone in our effort to revolutionize heart attack detection and ensure every patient receives immediate, life-saving care,” said Felix Bauer, COO of Powerful Medical. “We’re committed to bringing this life-saving technology to the US, the largest healthcare market in the world. This recognition by the FDA validates the impact of our innovation and brings us closer to transforming emergency cardiac care on a global scale,” added Martin Herman, CEO. With this designation, Powerful Medical not only works closely with the FDA on market approval but also gains improved access to CMS reimbursement mechanisms to bring the PMcardio STEMI AI ECG Model to healthcare providers nationwide, serving US public health. About Powerful Medical Powerful Medical is a pioneering health technology company specializing in AI-driven cardiovascular diagnostics. Its flagship product, PMcardio, harnesses AI to enhance ECG interpretation, streamline patient triage, and support clinical decision-making. With a mission to bridge the gap between innovation and clinical practice, Powerful Medical is committed to ensuring every patient receives the highest standard of care. References   View source version on businesswire.com: https://www.businesswire.com/news/home/20250325333487/en/ Lucia Bojkovskapr@powerfulmedical.com If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Walt Disney had a surprising role in the creation of Bumble</div>
                            <div class="article-source">Business Insider - 2025-03-26T18:42:46Z</div>
                            <div class="article-description">Whitney Wolfe Herd has worked on two of the most popular dating apps, Bumble and Tinder. Entertainment legend Walt Disney helped her get there.</div>
                            <div class="article-content"><p>DeSantis vs. Disney: Who are the winners and losers?</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">The open source Model Context Protocol was just updated — here’s why it’s a big deal</div>
                            <div class="article-source">VentureBeat - March 26, 2025 6:51 PM</div>
                            <div class="article-description">An updated version of the MCP spec introduced key upgrades to make AI agents more secure, capable and interoperable.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More The Model Context Protocol (MCP)—a rising open standard designed to help AI agents interact seamlessly with tools, data, and interfaces—just hit a significant milestone. Today, developers behind the initiative finalized an updated version of the MCP spec, introducing key upgrades to make AI agents more secure, capable and interoperable. [Update: In a very significant move, OpenAI, the industry leader in generative AI, followed the MCP announcement today by saying it also is adding support for MCP across its products. CEO Sam Altman said the support is available today in OpenAI’s Agents SDK and that support for ChatGPT’s desktop app and the Responses API would be coming soon.] In a notable move, Microsoft also recently announced support for MCP alongside this release, including launching a new Playwright-MCP server that allows AI agents like Claude to browse the web and interact with sites using the Chrome accessibility tree. “This new version is a major leap forward for agent-tool communication,” Alex Albert, a key contributor to the MCP project, said in a post on Twitter. “And having Microsoft building real-world infrastructure on top of it shows how quickly this ecosystem is evolving.” The March 26 update brings several important protocol-level changes: Figure 1: Claude Desktop using Playwright-MCP to navigate and describe datasette.io, demonstrating web automation powered by the Model Context Protocol. The protocol uses a modular JSON-RPC 2.0 base, with a layered architecture separating core transport, lifecycle management, server features (like resources and prompts) and client features (like sampling or logging). Developers can pick and choose which components to implement, depending on their use case. A day before the MCP update, Microsoft released Playwright-MCP, a server that wraps its powerful browser automation tool in the MCP standard. This means AI agents like Claude can now do more than talk—they can click, type, browse and interact with the web like a real user. Built on the Chrome accessibility tree, the integration allows Claude to access and describe page contents in a human-readable form. The available toolset includes: This turns any compliant AI agent into a test automation bot, QA assistant or data navigator. people love MCP and we are excited to add support across our products. available today in the agents SDK and support for chatgpt desktop app + responses api coming soon! Setup is easy: users simply add Playwright as a command in claude_desktop_config.json, and the Claude Desktop app will recognize the tools at runtime. Figure 2: The modular design of MCP enables developers to implement only the layers they need, while maintaining compatibility. Anthropic first introduced MCP in late 2023 to solve a growing pain point: AI agents need to interact with real-world tools, but every app speaks a different “language.” MCP aims to fix that by providing a common protocol for describing and using tools across ecosystems. With backing from Anthropic, LangChain, and now Microsoft, MCP is emerging as a serious contender for becoming the standard layer of agent interconnectivity. Since MCP was launched first by Anthropic, questions lingered whether Anthropic’s largest competitor, OpenAI, would support the protocol. And of course, Microsoft, a big ally of OpenAI, was another question mark. The fact that both players have supported the protocol shows momentum is building among enterprise and open-source communities. OpenAI itself has been opening its ecosystem around agents, including with its latest Agents SDK announced a week ago — and the move has solidified support around OpenAI’s API formats becoming a standard, given that others like Anthropic and Google have fallen in line. So with OpenAI’s APIs and MCP both seeing support, standardization has seen a bit win over the past couple of weeks. “We’re entering the protocol era of AI,” tweeted Alexander Doria, the co-founder of AI startup Pleias. “This is how agents will actually do things.” With the release of MCP 0.2 and Microsoft’s tangible support, the groundwork is being laid for a new generation of agents who can think and act securely and flexibly across the stack. Figure 3: OAuth 2.1 Authorization Flow in Model Context Protocol (MCP) The big question now is: Will others follow? If Meta, Amazon, or Apple sign on, MCP could soon become the universal “language” of AI actions. For now, it’s a big day for the agent ecosystem—one that brings the promise of AI interoperability closer to reality. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">With 23andMe entering bankruptcy, here&#x27;s how to delete your genetic data</div>
                            <div class="article-source">CNBC - Published Tue, Mar 25 20252:37 PM EDT</div>
                            <div class="article-description">Now that 23andMe has filed for bankruptcy, another company could take control of its genetic database. </div>
                            <div class="article-content"><p>23andMe has officially filed for Chapter 11 bankruptcy protection, which means its assets — including its vast genetic database — will soon be up for sale. The company continues to sell its at-home DNA testing kits, allowing consumers to get insight into their family histories and genetic profiles. DNA data is particularly sensitive because each person&#x27;s sequence is unique, meaning it can never be fully anonymized, according to the National Human Genome Research Institute. If genetic data falls into the hands of bad actors, it could be used to facilitate identity theft, insurance fraud or other crimes. 23andMe has been plagued by privacy concerns in recent years after hackers accessed the information of nearly 7 million customers in October 2023. As part of the bankruptcy process, the company said it will seek a partner that shares its commitment to customer data privacy, and that there will be no changes to how it stores, manages and protects data through the sale process. &quot;Our users&#x27; privacy and data are important considerations in any transaction, and we remain committed to our users&#x27; privacy and to being transparent with our customers about how their data is managed,&quot; the company said in an FAQ page about the bankruptcy filing. &quot;Any buyer of 23andMe will be required to comply with applicable law with respect to the treatment of customer data.&quot; Still, experts and officials are urging 23andMe customers to proceed with caution. California Attorney General Rob Bonta on Friday issued a consumer alert, encouraging residents to consider deleting their genetic data from 23andMe, which is based in his home state. &quot;Given 23andMe&#x27;s reported financial distress, I remind Californians to consider invoking their rights and directing 23andMe to delete their data and destroy any samples of genetic material held by the company,&quot; Bonta said in the release. Adrianus Warmenhoven, who serves on the security advisory board at NordVPN, described genetic data as the &quot;blueprint of your entire biological profile.&quot; He encouraged consumers to delete their information and be mindful of the companies they chose to share it with going forward. &quot;Monitor your digital footprint regularly, and you can also sign up for credit monitoring or identity theft protection services,&quot; Warmenhoven said in a statement to CNBC. &quot;Revoke permissions you no longer require, shut down any account you don&#x27;t use, and learn about how your data is used.&quot; 23andMe said customers can still delete their account and accompanying data. Here&#x27;s how: At this point, your personal information and your account will be permanently deleted from 23andMe, according to the deletion email from the company. Additionally, your data will not be used in any future research projects, and any personal samples the company was storing will be discarded. Correction: A prior version of this story had incorrect information about what happens to downloaded data if you delete your account. WATCH: The rise and fall of 23andMe</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">New analyst forecast says X&#x27;s advertising revenue is on the upswing, driven partly by &#x27;fear&#x27;</div>
                            <div class="article-source">Business Insider - 2025-03-26T18:41:37Z</div>
                            <div class="article-description">EMARKETER forecasts X&#x27;s US ad revenue will jump by 17.5% this year but cautions that &quot;fear is not a sustainable motivator.&quot;</div>
                            <div class="article-content"><p>Full content not available</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Alibaba launches new open-source AI model for &#x27;cost-effective AI agents&#x27;</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20258:41 PM EDT</div>
                            <div class="article-description">Alibaba Cloud has launched its latest AI model in its &quot;Qwen series,&quot; as large language model competition in China continues to heat up. </div>
                            <div class="article-content"><p>Alibaba Cloud launched Thursday its latest AI model in its &quot;Qwen series,&quot; as large language model competition in China continues to heat up following the &quot;DeepSeek moment.&quot; The new &quot;Qwen2.5-Omni-7B&quot; is a multimodal model, which means it can process inputs, including text, images, audio and videos, while generating real-time text and natural speech responses, according to an announcement on Alibaba Cloud&#x27;s website. The company says that the model can be deployed on edge devices like mobile phones, offering high efficiency without compromising performance. &quot;This unique combination makes it the perfect foundation for developing agile, cost-effective AI agents that deliver tangible value, especially intelligent voice applications,&quot; Alibaba said. For example, it could be used to help a visually impaired person navigate their environment through real-time audio description, the company added. The new model is open-sourced on the platforms Hugging Face and Github, following a growing trend in China after DeepSeek made its breakthrough R1 model open-source. Open-source generally refers to software in which the source code is made freely available on the web for possible modification and redistribution. Over the past years, Alibaba Cloud says it has open-sourced over 200 generative AI models. Amid China&#x27;s AI fervor accelerated by DeepSeek, Alibaba and other generative AI competitors have been releasing new, cost-effective models and products at an unprecedented pace. Last week, Chinese tech giant Baidu released a new multimodal foundational model and its first reasoning-focused model. Alibaba, meanwhile, debuted its updated Qwen 2.5 artificial intelligence model in late January and released a new version of its AI assistant tool Quark earlier this month. The company has strongly committed to its AI strategy, announcing last month a plan to invest $53 billion in its cloud computing and AI infrastructure over the next three years, exceeding what it spent in the space over the past decade. Kai Wang, Asia senior equity analyst at Morningstar, told CNBC that large Chinese tech players such as Alibaba, which build data centers to meet the computing needs of AI in addition to building their own LLMs, are well positioned to benefit from China&#x27;s post-DeepSeek AI boom. Alibaba secured a major win for its AI business last month when it confirmed that the company was partnering with Apple to roll out AI integration for iPhones sold in China. On Wednesday, the group also reported an expanded strategic partnership with BMW to accelerate the integration of its AI into the carmaker&#x27;s next-generation intelligent vehicles.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">The new best AI image generation model is here: say hello to Reve Image 1.0!</div>
                            <div class="article-source">VentureBeat - March 25, 2025 8:22 AM</div>
                            <div class="article-description">One of the model’s standout capabilities is its strong text rendering performance, addressing a common challenge in AI-generated imagery.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Reve AI, Inc., an AI startup based in Palo Alto, California, has officially launched Reve Image 1.0, an advanced text-to-image generation model designed to excel at prompt adherence, aesthetics, and typography. This marks the company’s first release, with future tools expected to follow. Reve Image is currently available for free preview at preview.reve.art, allowing users to generate images from text descriptions without requiring advanced prompt engineering. The company has not yet announced API access or long-term pricing plans, nor is it clear if the model will be proprietary or made open source, and if so, under what license. Reve Image differentiates itself by aiming for a deeper understanding of user intent. It allows users to not only generate images from text but also modify existing images with simple language commands. Example modifications include changing colors, adjusting text, and altering perspectives. The model also supports uploading reference images, enabling users to create visuals that match a specific style or inspiration. One of the model’s standout capabilities is its strong text rendering performance, addressing a common challenge in AI-generated imagery — and making it more directly competitive with text-focused image models such as Ideogram, which are more valuable to those designing logos and branding. Additionally, early user tests suggest that Reve Image handles multi-character prompts more effectively than previous models. Reve Image has already been evaluated by third-party AI model testing service Artificial Analysis. In the Artificial Analysis’s Image Arena, which ranks various image generation models based on user reviews and other quantitative metrics, Reve is currently in the lead at #1 for “image generation quality,” outperforming competitors such as Midjourney v6.1, Google’s Imagen 3, Recraft V3, and Black Forest Lab’s FLUX.1.1 [pro]. The benchmarking group highlighted Reve Image’s ability to generate clear and readable text within images, a historically difficult task for AI models. Before its official unveiling, Reve Image was known under the code name “Halfmoon” on social media, generating speculation and anticipation within the AI community. Reve describes itself as a “small team of passionate researchers, builders, designers, and storytellers with big ideas.” The company is focused on developing creative tooling that enhances how users interact with AI-powered visuals. On X, Michaël Gharbi, Co-Founder and Research Scientist at Reve, shared insights into the company’s long-term vision, emphasizing the goal of building AI models that understand creative intent rather than merely generating visually plausible outputs. “Capturing creative intent requires advanced machine understanding of natural language and other interactions,” Gharbi said. “Our vision is to build a new semantic intermediate representation that both a human and a machine can understand, reason about, and operate on.” Other team members, including engineer Hunter Loftis and researcher Taesung Park, echoed the importance of bringing logic to AI-generated visuals. Park compared current text-to-image models to early large language models (LLMs), stating that they often produce visually appealing but logically inconsistent results. Early user feedback on the AI-heavy subreddit r/singularity (on Reddit), has been largely positive, with many praising the model’s accurate prompt following, high-quality text rendering, and rapid generation speed. Some users have reported success in generating multi-character scenes and complex environments, areas where previous models often struggled. However, some challenges remain. Users have noted that Reve Image: Despite these hurdles, the team at Reve has been actively engaging with the user community and incorporating feedback into ongoing improvements. In my own brief hands on usage while drafting and creating the header image for this very article, I found Reve to be fairly intuitive and easy-to-use, with impressive visuals and prompt adherence. Like many AI-image generators, there’s a prompt entry textbox, though unlike Midjourney and Ideogram, Reve puts it at the bottom of the website and leaves your generated content up top to fill the majority of the space. In addition, the prompt entry textbox also contains four buttons below it for further fine adjustments to the image generation prompt sequence, including an aspect ratio adjuster (with standard sizing between 16:9 (widescreen landscape) and 9:16 (portrait, like a smartphone)… There’s another button selector for how many images you want to produce from each prompt (1, 2, 4, 8), a button to toggle on and off prompt text enhancement (it’s default toggled on, and this means that Reve will actually automatically edit the text you type in based on what it thinks you want to see in your image, adding lots more rich details and visual language than you might initially include) and a “seed” button for choosing if you want it to use a specific numeric string from a previous generated image to guide the generations going forward. It’s far fewer settings and doesn’t include any visual based editors like Midjourney, but the basics are there and it should be more than enough for most casual AI image users to get started. My brief tests also showed it was on-par or better than Ideogram at rendering legible text baked into images (and far surpassing Midjoruney), as well as on-par or exceeding the quality of rendering recognizable public figures as Grok (again, Midjourney and many other image generators prohibit this). While the model is currently only available via the company’s website, there is growing anticipation for API access or potential open-source options. Users have also expressed interest in additional features like custom model training, control tools for animation, and integration with creative software. For now, Reve Image remains freely accessible at preview.reve.art, allowing users to explore its capabilities firsthand. As Reve continues to refine its AI models and expand its offerings, the company is positioning itself as a major player in the evolving world of AI-powered creative tooling. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Microsoft infuses enterprise agents with deep reasoning, unveils data Analyst agent that outsmarts competitors</div>
                            <div class="article-source">VentureBeat - March 25, 2025 7:45 PM</div>
                            <div class="article-description">Microsoft announced Tuesday two significant additions to its Copilot Studio platform: deep reasoning capabilities that enable agents to tackle complex problems through careful, methodical thinking, and agent flows that combine AI flexibility with deterministic business process automation.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Microsoft has built the largest enterprise AI agent ecosystem, and is now extending its lead with powerful new capabilities that position the company ahead in one of enterprise tech’s most exciting segments. The company announced Tuesday evening two significant additions to its Copilot Studio platform: deep reasoning capabilities that enable agents to tackle complex problems through careful, methodical thinking, and agent flows that combine AI flexibility with deterministic business process automation. Microsoft also unveiled two specialized deep reasoning agents for Microsoft 365 Copilot: Researcher and Analyst. “We have customers with thousands of agents already,” Microsoft’s Corporate Vice President for Business and Industry Copilot Charles Lamanna, told VentureBeat in an exclusive interview on Monday. “You start to have this kind of agentic workforce where no matter what the job is, you probably have an agent that can help you get it done faster.” While the Researcher agent mirrors capabilities from competitors like OpenAI’s Deep Research and Google’s Deep Research, Microsoft’s Analyst agent represents a more differentiated offering. Designed to function like a personal data scientist, the Analyst agent can process diverse data sources, including Excel files, CSVs, and embedded tables in documents, generating insights through code execution and visualization. “This is not a base model off the shelf,” Lamanna emphasized. “This is quite a bit of extensions and tuning and training on top of the core models.” Microsoft has leveraged its deep understanding of Excel workflows and data analysis patterns to create an agent that aligns with how enterprise users actually work with data. The Analyst can automatically generate Python code to process uploaded data files, produce visualizations, and deliver business insights without requiring technical expertise from users. This makes it particularly valuable for financial analysis, budget forecasting and operational reporting use cases that typically require extensive data preparation. Microsoft’s deep reasoning capability extends agents’ abilities beyond simple task completion to complex judgment and analytical work. By integrating advanced reasoning models like OpenAI’s o1 and connecting them to enterprise data, these agents can tackle ambiguous business problems more methodically. The system dynamically determines when to invoke deeper reasoning, either implicitly based on task complexity or explicitly when users include prompts like “reason over this” or “think really hard about this.” Behind the scenes, the platform analyzes instructions, evaluates context, and selects appropriate tools based on the task requirements. This enables scenarios that were previously difficult to automate. For example, one large telecommunications company uses deep reasoning agents to generate complex RFP responses by assembling information from across multiple internal documents and knowledge sources, Lamanna told VentureBeat. Similarly, Thomson Reuters employs these capabilities for due diligence in mergers and acquisition reviews, processing unstructured documents to identify insights, he said. See an example of the agent reasoning at work in the video below: Microsoft has also introduced agent flows, which effectively evolve robotic process automation (RPA) by combining rule-based workflows with AI reasoning. This addresses customer demands for integrating deterministic business logic with flexible AI capabilities. “Sometimes they don’t want the model to freestyle. They don’t want the AI to make its own decisions. They want to have hard-coded business rules,” Lamanna explained. “Other times they do want the agent to freestyle and make judgment calls.” This hybrid approach enables scenarios like intelligent fraud prevention, where an agent flow might use conditional logic to route higher-value refund requests to an AI agent for deep analysis against policy documents. Pets at Home, a U.K.-based pet supplies retailer, has already deployed this technology for fraud prevention. Lamanna revealed the company has saved “over a million pounds” through the implementation. Similarly, Dow Chemical has realized “millions of dollars saved for transportation and freight management” through agent-based optimization. Below is a video showing the Agent Flows at work: Central to Microsoft’s agent strategy is its enterprise data integration through the Microsoft Graph, which is a comprehensive mapping of workplace relationships between people, documents, emails, calendar events, and business data. This provides agents with contextual awareness that generic models lack. “The lesser known secret capability of the Microsoft graph is that we’re able to improve relevance on the graph based on engagement and how tightly connected some files are,” Lamanna revealed. The system identifies which documents are most referenced, shared, or commented on, ensuring agents reference authoritative sources rather than outdated copies. This approach gives Microsoft a significant competitive advantage over standalone AI providers. While competitors may offer advanced models, Microsoft combines these with workplace context and fine-tuning optimized explicitly for enterprise use cases and Microsoft tools. Microsoft can leverage the same web data and model technology that competitors can, Lamanna noted, “but we then also have all the content inside the enterprise.” This creates a flywheel effect where each new agent interaction further enriches the graph’s understanding of workplace patterns. Microsoft has prioritized making these powerful capabilities accessible to organizations with varying technical resources, Lamanna said. The agents are exposed directly within Copilot, allowing users to interact through natural language without prompt engineering expertise. Meanwhile, Copilot Studio provides a low-code environment for custom agent development. “It’s in our DNA to have a tool for everybody, not just people who can boot up a Python SDK and make calls, but anybody can start to build these agents,” Lamanna emphasized. This accessibility approach has fueled rapid adoption. Microsoft previously revealed that over 100,000 organizations have used Copilot Studio and that more than 400,000 agents were created in the last quarter. While Microsoft appears to lead enterprise agent deployment today, competition is intensifying. Google has expanded its Gemini capabilities for agents and agentic coding, while OpenAI’s o1 model and Agents SDK provide powerful reasoning and agentic tools for developers. Big enterprise application companies like Salesforce, Oracle, ServiceNow, SAP and others have all launched agentic platforms for their customers over the last year. And also on Tuesday, Amazon’s AWS released an AI agent, called Amazon Q in Quicksight, to let employees to engage via natural language to perform data analysis without specialized skills. Employees can use natural language to perform expert-level data analysis, ask what-if questions, and get actionable recommendations, helping them unlock new insights and make decisions faster However, Microsoft’s advantage lies in its more comprehensive approach—a strong coupling with the leading reasoning model company, OpenAI, while also offering model choice, enterprise-grade infrastructure, extensive data integration across workplace tools, and a focus on business outcomes rather than raw AI capabilities. Microsoft has created an ecosystem that looks like best practice by combining personal copilots that understand individual work patterns with specialized agents for specific business processes. For enterprise decision-makers, the message is clear: agent technology has matured beyond experimentation to practical business applications with measurable ROI. The choice of platform increasingly depends on integration with existing tools and data. In this area, Microsoft holds an advantage in many application areas because of the number of users it has, for example, in Excel and Power Automate. Watch my full interview with Charles Lamanna embedded below to hear firsthand how Microsoft is driving its agent strategy, what these new capabilities mean for enterprise users, and how organizations are leveraging agents to deliver measurable business results: If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Trump says he may reduce China tariffs to help close a TikTok deal</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20257:06 PM EDT</div>
                            <div class="article-description">&quot;Maybe I&#x27;ll give them a little reduction in tariffs or something to get it done,&quot; President Donald Trump said about a deal involving TikTok&#x27;s U.S. operations</div>
                            <div class="article-content"><p>President Donald Trump said he may reduce tariffs on China to facilitate a deal that would result in ByteDance selling the U.S. operations of TikTok. China &quot;is going to have to play a role&quot; in approving a TikTok-related divestiture, Trump said in a press conference Wednesday. &quot;Maybe I&#x27;ll give them a little reduction in tariffs or something to get it done,&quot; Trump said. &quot;TikTok is big, but every point in tariffs is worth more than TikTok.&quot; Although a national security law requires ByteDance to divest TikTok&#x27;s U.S. operations or face an effective ban in the country, Trump in January signed an executive order that delayed the deadline for a deal to April 5. Trump has previously said that he wants the U.S. to maintain a 50% ownership position in TikTok via a joint venture. It&#x27;s possible he will extend the TikTok deadline again, Trump said Wednesday. &quot;We&#x27;re going to have a form of a deal, but if it&#x27;s not finished, it&#x27;s not a big deal,&quot; Trump said. &quot;We&#x27;ll just extend it.&quot; Vice President JD Vance told NBC News earlier this month that he was confident that a TikTok-related deal would happen by the April deadline. &quot;There will almost certainly be a high-level agreement that I think satisfies our national security concerns, allows there to be a distinct American TikTok enterprise,&quot; Vance said. WATCH: TikTok bid is &#x27;in active dialogue&#x27; with Trump administration.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">How YouTube paranormal investigators Sam and Colby created a $20 million revenue empire</div>
                            <div class="article-source">Business Insider - 2025-03-26T14:52:21Z</div>
                            <div class="article-description">The multifaceted media business of YouTube duo Sam and Colby generated about $20 million in revenue last year, they told Business Insider.</div>
                            <div class="article-content"><p>How Sesame Street stays relevant to every generation, according to the brand&#x27;s marketing head</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Utah governor signs online child safety law requiring Apple, Google to verify user ages</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20255:43 PM EDT</div>
                            <div class="article-description">Utah&#x27;s signing of the law is a political win for Meta, which has been aiming to put more political and regulatory pressure on Apple. </div>
                            <div class="article-content"><p>Utah Gov. Spencer Cox on Wednesday signed a bill that requires Apple and Google&#x27;s mobile app stores to verify user ages and require parental permission for those under 18 to use certain apps, the governor&#x27;s spokesperson told CNBC. The law is the first of its kind in the nation and represents a significant shift in how user ages are verified online, and says it&#x27;s the responsibility of mobile app stores to verify ages — putting the onus on Apple and Google, instead of individual apps like Instagram, Snapchat and X, to do age checks. The App Store Accountability Act, or S.B. 142, could also kick off a wave of other states, including South Carolina and California, passing similar legislation. The law is designed to protect children, who may not understand apps&#x27; terms of services and, therefore, can&#x27;t agree to them, said Todd Weiler, a Republican state senator and the bill&#x27;s sponsor. &quot;For the past decade or longer, Instagram has rated itself as friendly for 12 year olds,&quot; Weiler said at a state senate committee hearing in January. &quot;It&#x27;s not.&quot; Apple and Google will need to request age verification checks when someone makes a new account in the state. That will most likely have to be done using credit cards, according to Weiler. If someone under 18 opens an app store account, Apple or Google will have to link it to a parent&#x27;s account or request additional documentation. Parents will have to consent to in-app purchases. Neither company immediately returned a request for comment on Wednesday. Meta, X and Snap said on Wednesday they applauded Cox and Utah for passing the bill, and encouraged other states to consider similar approaches. &quot;Parents want a one-stop-shop to oversee and approve the many apps their teens want to download, and Utah has led the way in centralizing it within a device&#x27;s app store,&quot; the companies said in a joint statement. &quot;This approach spares users from repeatedly submitting personal information to countless individual apps and online services.&quot; The Utah law is slated to take effect on May 7, but it is expected to be challenged in a legal fight over its validity. The state passed a similar age-verification law related to pornography in 2023, and arguments whether that law violates free speech were heard by the Supreme Court in January. Utah&#x27;s adoption of the law is also the latest shot in a long-running skirmish between Facebook-parent Meta and Apple. Meta, which supported the bill, argues that app stores are the best place to do age verification on minors, instead of on individual apps. Meta has recently shifted its policy strategy to seek strategic advantages for itself and shift antitrust scrutiny onto Apple, CNBC reported last month. Apple says it makes the most sense for apps themselves to do age verification, and that due to privacy reasons, it doesn&#x27;t want to collect the data needed for age verification. The &quot;right place to address the dangers of age-restricted content online is the limited set of websites and apps that host that kind of content,&quot; according to a paper Apple posted on its website last month. Utah&#x27;s bill raises privacy and safety risks, Google said in a blog post on March 12. &quot;There are a variety of fast-moving legislative proposals being pushed by Meta and other companies in an effort to offload their own responsibilities to keep kids safe to app stores,&quot; Google Director of Public Policy Kareem Ghanem wrote. &quot;These proposals introduce new risks to the privacy of minors, without actually addressing the harms that are inspiring lawmakers to act.&quot; The push for age verification comes after Meta CEO Mark Zuckerberg, X CEO Lina Yaccarino, Snap CEO Evan Spiegel and other social media CEOs appeared before Congress in January 2024 for a hearing focused on online child safety. There, lawmakers criticized the companies, saying they failed to stem online child sexual exploitation on social media apps and needed to do more. Zuckerberg appeared rattled during the hearing after senators told him he had &quot;blood on your hands.&quot; However, the legislation that came out of the meeting, the Kids Online Safety Act, failed to advance in Congress late last year. Meta has also been hit with a number of lawsuits filed by states relating to the well-being of children on Facebook and Instagram. -- CNBC&#x27;s Jonathan Vanian contributed to this report. WATCH: Apple delays AI improvements to Siri until 2026: Here&#x27;s what to know</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Observe launches VoiceAI agents to automate customer call centers with realistic, humanlike voices that don’t interrupt</div>
                            <div class="article-source">VentureBeat - March 26, 2025 6:47 AM</div>
                            <div class="article-description"></div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Observe.AI has officially launched VoiceAI agents, a solution designed to automate routine customer interactions in contact centers. The latest addition to the company’s AI-driven conversational intelligence platform, VoiceAI agents aim to improve customer experience while reducing operational costs. With this release, Observe.AI is positioning itself as the only complete AI-powered platform that supports enterprises across the entire customer journey. The company’s suite of solutions now includes enterprise-grade VoiceAI agents, real-time agent assist tools, AutoQA for quality monitoring, agent coaching, and business insights. Observe.AI’s VoiceAI agents are built to handle a wide range of customer service inquiries, from frequently asked questions to more complex, multi-step conversations. They are built atop a combination of in-house AI models and partnerships with major AI providers like OpenAI and Anthropic for large language models (LLMs). “It’s an ensemble of multiple smaller models,” Jain explained. “For example, we have a specific model for number detection, a specific model for entity detection, a model for turn detection, and so on.” The goal is to alleviate the burden on human agents, allowing them to focus on higher-value interactions. As Swapnil Jain, CEO and co-founder of Observe.AI, told VentureBeat in a recent video call interview: “Enterprises are saying, ‘Do we really need human agents for these kinds of use cases?’” Jain said that companies often receive calls for basic tasks like checking an account balance or resetting a password—interactions that AI can now handle efficiently. For customers, this means eliminating long hold times and avoiding frustrating IVR menus that require pressing multiple buttons or repeatedly requesting a human agent. The voice AI space is becoming increasingly crowded with options ranging from proprietary models like OpenAI’s newly released GPT-4o-transcribe family and ElevenLabs to open source solutions as well. So why would someone pick Observe.AI’s agents over these? In a nutshell: specialization and ease-of-use. Instead of having to use raw voice AI models through providers’ APIs and building custom integrations with a business, or custom voice apps, Obseve.AI’s platform is already built to essentially “plug and play” with existing workflows and operations. So while, GPT-4o and other LLMs provide raw AI capabilities, Jain and Observe.AI’s contention is that they don’t offer a fully integrated solution for customer service workflows. In addition, unlike traditional voice AI assistants, Observe.AI’s VoiceAI agents are specifically designed for contact centers. The system combines various AI technologies, including: Jain noted that one of the key challenges AI agents face is knowing when a customer has actually finished speaking. “When do you know that the AI agent can start processing and the customer has stopped speaking?” he asked. “Sometimes I’m taking pauses because my sentence is over and I’m starting a new one. Sometimes I just stop speaking. How do you know the difference?” Observe.AI has developed custom in-house models that solve these nuances, ensuring smoother conversations between AI and customers. One of Observe.AI’s key advantages is its ability to integrate seamlessly with existing enterprise systems. Over time, the company has developed pre-built integrations with more than 250 platforms, including leading telephony, CRM, and workforce management tools such as Salesforce, Zendesk, and ServiceNow. This approach allows businesses to implement VoiceAI agents quickly. While AI deployments can sometimes take months, Observe.AI claims that its VoiceAI agents can go live in as little as one week, with minimal setup costs. “It’s not a professional services model where we take six months to customize something for you,” Jain said. “We come in, take two weeks to configure the product, and it works.” Given the sensitivity of customer interactions, Observe.AI has built its solution with enterprise-grade security. The company holds certifications including GDPR, HIPAA, HITRUST, SOC2, and ISO27001. While voice biometrics have been used in the past for authentication, Jain stated that Observe.AI does not rely on them due to security concerns. Instead, the system follows traditional authentication methods, such as verifying Social Security numbers or account details. Additionally, Observe.AI offers data redaction capabilities to remove personally identifiable information (PII) before storage, and customers can opt for private instances to ensure data remains isolated. “In today’s world, you cannot rely on individual speech patterns for authentication,” Jain said. “We work with businesses to configure the same security rules they use for their human agents into our AI agents.” Observe.AI’s pricing model is based on completed tasks rather than per-minute usage. The cost depends on the complexity of the interaction, with simpler tasks (such as routing a call) priced lower than more involved tasks (such as processing an insurance claim). According to Jain, businesses can expect to save between 70-80% on customer service costs compared to using human agents. Companies using VoiceAI agents are already seeing significant improvements. Emmanual Noyola, Director of Patient Services at Affordable Care, highlighted the impact on his team: “Beth, our VoiceAI agent, handles multiple intents with a 95% containment rate so our customer care team can focus on more complex cases.” By analyzing every conversation, Observe.AI’s platform continuously refines AI agent performance, ensuring accuracy and compliance. Businesses can also use AutoQA to evaluate both AI and human agents, identifying areas for improvement. One of the key challenges in AI-driven customer service is maintaining accuracy while preventing unintended responses. Jain acknowledged these concerns, referencing past AI missteps in customer service automation. “The core thesis behind making these enterprise-grade is having a very high bar on the confidence of the response,” he said. “If our response confidence is less than a certain threshold, it’s better for the AI agent to not even engage.” The launch of VoiceAI agents marks a major step toward what Observe.AI envisions as the autonomous contact center of the future. As AI continues to evolve, the company is focused on creating solutions that blend automation with human expertise, ensuring a seamless customer experience. “Since our founding seven years ago, we have created a platform that uniquely understands contact center conversations,” Jain said. “It is a logical next step to introduce our VoiceAI agents to automate interactions and ultimately support both human and AI agents in delivering consistent, secure, high-quality customer experience across every touchpoint.” If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">23andMe cofounder says company &#x27;lost its way&#x27; without &#x27;proper governance&#x27;</div>
                            <div class="article-source">Business Insider - 2025-03-26T23:20:33Z</div>
                            <div class="article-description">23andMe cofounder Linda Avey is mourning what the biotech company could have become. The company filed for Chapter 11 bankruptcy protection on Sunday.</div>
                            <div class="article-content"><p>5 ways Elon Musk shook up Twitter as CEO</p></div>
                        </div>
                
                    </section>
            
                    <section id="theme-3">
                        <h2>Business and Finance <small>(11 articles)</small></h2>
                        <p>Articles related to business strategies, financial updates, and market trends</p>
                        
                        <h3>Articles in this Theme</h3>
            
                        <div class="article-card">
                            <div class="article-title">Providing consistent, reliable customer experiences with CQRS at scale</div>
                            <div class="article-source">VentureBeat - March 25, 2025 6:50 AM</div>
                            <div class="article-description">To maintain seamless customer experiences, Chase explains their transition to a modern, always-on ecosystem that scales with customer demand.</div>
                            <div class="article-content"><p>Presented by JPMorganChase As more Chase card customers embrace digital services, we’ve seen a surge in transaction-related inquiries. This increase has put pressure on our distributed backend systems to maintain seamless customer experiences, even during system outages at Systems of Record (SORs) or other layers. To address these challenges, we embarked on a modernization journey. Our goal was to transition from siloed environments to a modern, always-on ecosystem that scales with customer demand. This transformation allows customers to manage accounts, conduct transactions and access financial services through our online and mobile platforms without friction. The result? A significant technological advancement in distributed systems and big data, supporting Chase’s journey to unlock and accelerate new value for our business and customers. SORs which were predominantly mainframe-based and eventually evolved to modern technology stacks were designed to ensure reliability of command traffic. Data was ingested into data warehouses, the primary destination for most queries. With the emergence of real-time traffic through digital experiences, SORs began exposing their data to queries through APIs. Over time, the volume of query traffic grew significantly, often surpassing command traffic. Nowadays, it’s not uncommon for queries to constitute up to 90% of total SOR read volume. These strategic shifts have had profound effects on the cost, scalability and reliability of SORs, often contributing to operational issues. In our quest to mitigate operational issues posed by SORs and provide exceptional customer experiences, Chase adopted Command Query Responsibility Segregation (CQRS), a software architectural pattern that separates the responsibilities of handling commands (write operations) and queries (read operations) into distinct parts. Introduced by Greg Young around 2010, CQRS has gained significant traction in the software development community due to its ability to enhance scalability, performance and maintainability in complex systems. At Chase we built and implemented standards to achieve business continuity three to five times faster and improved customer experience, speed to market with new experiences and reliability. These standards include: The read layer in the CQRS pattern played a pivotal role in building a common data product by providing a unified and optimized view of data tailored to various user needs. By decoupling the read operations from the write processes, it allowed for the creation of specialized read models that can aggregate and present data from multiple sources, enabling consistent and efficient access to data across different applications and services. Embracing the challenge of adopting CQRS was immensely gratifying, and we successfully accomplished the following objectives on our journey: While CQRS offers numerous benefits it also introduces additional complexity, especially in terms of system design, implementation and operational overhead. We spent significant time carefully evaluating trade-offs, considering key factors such as eventual consistency, asynchronous communication and aligning with the principles of domain-driven design (DDD). Adopting CQRS has allowed us to operate at scale and with autonomy. It requires careful consideration of trade-offs and may not suit every project or team. As with any architectural pattern, understanding the principles and applying them judiciously is key to realizing the benefits of CQRS in software development. Like what you’re reading? about the Chase Product, Experience and Technology teams on Next at Chase. Amit Meshram is Executive Director, Principal Software Engineer at Chase.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">The concern with CoreWeave’s 250,000 Nvidia chips ahead of its IPO</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20255:00 PM EDT</div>
                            <div class="article-description">CoreWeave sells access to Nvidia graphics processors as a service, allowing developers to rent chips by the hour so they can develop more advanced AI models.</div>
                            <div class="article-content"><p>In this article With 250,000 highly-desired Nvidia graphics processors, CoreWeave has become one of the most prominent &quot;GPU clouds,&quot; a status it hopes investors will value when it debuts on the public markets. But the world of artificial intelligence hardware is moving so quickly that it raises questions about how long those chips will remain on the cutting edge and in demand. It&#x27;s a concern that could impact investor demand for shares of CoreWeave, one of the most anticipated IPOs in years. CoreWeave, which rents out remote access to computers based on Nvidia AI chips, said in a financial filing this month that most of its AI chips are from Nvidia&#x27;s Hopper generation. Those chips, such as the H100, were state-of-the-art in 2023 and 2024. They were scarce as AI companies bought or rented all the chips they could get in the wake of OpenAI ushering in the generative AI age with the release of ChatGPT in late 2022. But these days, Nvidia CEO Jensen Huang says that his company&#x27;s Hopper chips are getting blown out of the water by their successors – the Blackwell generation of GPUs, which have been shipping since late 2024. Hopper chips are &quot;fine&quot; for some circumstances but &quot;not many,&quot; Huang joked at Nvidia&#x27;s GTC conference last week. &quot;In a reasoning model, Blackwell is 40 times the performance of Hopper. Straight up. Pretty amazing,&quot; Huang said. &quot;I said before that when Blackwell starts shipping in volume, you couldn&#x27;t give Hoppers away.&quot; That&#x27;s great for Nvidia, which needs to find ways to keep selling chips to the companies committed to the AI race, but it&#x27;s bad news for GPU clouds like CoreWeave. That&#x27;s because the New Jersey company models the future trajectory of its business based on how much it anticipates being able to rent Nvidia chips out for over the next five to six years. Huang may have been kidding, but Nvidia spent much of its event detailing just how much better its Blackwell chips are. In Nvidia&#x27;s view, the best way to decrease the high cost of serving AI is by buying faster chips. Blackwell systems are in full production and shipping to customers, and Nvidia plans to introduce an upgraded version of Blackwell in late 2026. When new chips come out, the older chips — the kind CoreWeave has a quarter of a million of — go down in price, Huang said. So too does the price of renting them. Older chips don&#x27;t just stop working when new ones come out. Most companies, including CoreWeave, plan to use Hopper chips for six years. But Nvidia is telling customers that its newer, faster chips are capable of producing more AI content, which leads to more revenues at a better margin for clouds. An H100 would have to be priced 65% lower per hour than an Nvidia Blackwell GB200 NVL system for the two systems to be competitive in price per output to a renter. Put another way, the H100 would have to rent at 98 cents per hour to match the price per output of a Blackwell rack system priced at $2.20 per hour per GPU, SemiAnalysis estimated, speaking generally about AI rentals. H100s rented for as much as $8 per hour back in 2023 and often required long commitments and lead times, but now, usage of those chips can be summoned in minutes with a credit card. Some services now offer rented H100 access for under $2 per hour. The industry could be entering a period where the useful life of AI chips is reduced, Barclays analyst Ross Sandler wrote in a note on Friday. He was focused on hyperscalers — Meta, Google and Amazon — but the trend affects smaller cloud providers like CoreWeave, too. &quot;These assets are becoming obsolete at a much more rapid pace given how much innovation and speed improvements happen with each generation,&quot; Sandler wrote. This threatens company earnings if they end up depreciating older equipment faster, he said. CoreWeave says that if there were to be changes to the &quot;significant&quot; assumptions it makes about the useful lifetime of its AI infrastructure, it could hurt its business or future prospects. CoreWeave has also borrowed nearly $8 billion to buy Nvidia chips and build its data centers, sometimes using the GPUs it amassed as collateral. Analysts and investors are also increasingly asking questions about the useful lifespan of these new AI systems and whether their financial depreciation schedules should be accelerated because the technology is improving so fast. CoreWeave says in its filing that it seeks to offer state-of-the-art infrastructure and says it will continue spending to expand and improve its data centers. &quot;Part of this process entails cycling out outdated components of our infrastructure and replacing them with the latest technology available,&quot; the New Jersey company said. &quot;This requires us to make certain estimates with respect to the useful life of the components of our infrastructure and to maximize the value of the components of our infrastructure, including our GPUs, to the fullest extent possible.&quot; CoreWeave and Nvidia maintain a good relationship. CoreWeave will certainly buy more chips from Nvidia, which owns more than 5% of the New Jersey company. &quot;We&#x27;re super proud of them,&quot; Huang said last week. But Nvidia&#x27;s road map for releasing new chips that it proudly touts will make their predecessors obsolete is a threat to CoreWeave&#x27;s ambitions. WATCH: CoreWeave begins marketing IPO, targeting price range of $47-$55 per share: Report</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Trust Wallet Reaches 200 Million Downloads Milestone</div>
                            <div class="article-source">VentureBeat - March 26, 2025 9:25 AM</div>
                            <div class="article-description">Press Release Trust Wallet, the world’s leading self-custody Web3 wallet, has surpassed 200 million total downloads, marking a game-changing milestone in the industry. Trust Wallet stands as the most widely used non-custodial wallet globally for onchain users, cementing…</div>
                            <div class="article-content"><p>With this milestone, Trust Wallet cements its position as the #1 crypto wallet  DUBAI, United Arab Emirates–(BUSINESS WIRE)–March 26, 2025– Trust Wallet, the world’s leading self-custody Web3 wallet, has surpassed 200 million total downloads, marking a game-changing milestone in the industry. Trust Wallet stands as the most widely used non-custodial wallet globally for onchain users, cementing its role as a key gateway to Web3. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20250325066589/en/ Trust Wallet Reaches 200 Million Downloads Milestone Since its launch in 2017, Trust Wallet has played a pivotal role in onboarding millions into crypto. Initially introduced as an Ethereum wallet, it has evolved into a chain-agnostic, multi-chain Web3 hub, now supporting over 10 millions assets across 100+ blockchains, along with a suite of features that empower users to navigate their entire Web3 journey-from buying their first cryptocurrency to swapping, staking, exploring the decentralized web, and beyond. Eowyn Chen, CEO of Trust Wallet, commented on the achievement: “Reaching 200 million downloads is a real testament to the trust from the users. In a rapidly evolving industry, our mission has remained the same: empower people with freedom to own and access opportunities. We’re proud of this milestone, but even more humbled and excited about the future as we have many things on the roadmap for our global community. We got to work harder.” Trust Wallet has carved out a significant space for itself in the competitive landscape of cryptocurrency wallets. This success can be attributed to a combination of core principles that focus on user experience, community, trust and security. What’s Fuelling Trust Wallet’s Growth? With millions of users worldwide and a fast-growing community, Trust Wallet continues to expand its reach through compelling features, product innovations, and user-centric initiatives. Its recent growth and success points to a relentless focus on usability, innovation, and security. The wallet strikes a balance between onboarding new users and offering advanced tools for experienced users. Examples of Trust Wallet’s innovations include: Building a Future-Proof Web3: Trust Wallet’s Vision and Beyond As the on-chain economy evolves and AI-driven innovations take shape, Trust Wallet is focused on bridging the gap between Web2 simplicity and Web3 autonomy. The goal is to make decentralized finance (DeFi) and digital ownership more intuitive, secure, and accessible for millions of users. Web3 isn’t just about holding assets-it’s about seamless, intelligent, and secure interactions across decentralized applications (dApps), finance, gaming, and beyond. Trust Wallet continues to expand its capabilities to give users the tools and insights needed to navigate the decentralized world with confidence. Key Focus Areas for 2025: By improving usability, security, and intelligence, Trust Wallet is ensuring that more people can explore and benefit from the decentralized economy with confidence. About Trust Wallet Trust Wallet is the secure, self-custody Web3 wallet and gateway for people who want to fully own, control, and leverage the power of their digital assets. From beginners to experienced users, Trust Wallet makes it easier, safer, and convenient for millions of people around the world to experience Web3, access dApps securely, store and manage their crypto and NFTs, as well as buy, sell, and stake crypto to earn rewards – all in one place and without limits.  View source version on businesswire.com: https://www.businesswire.com/news/home/20250325066589/en/ For media enquiries, contact:press@trustwallet.com If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Meet a decades-old software company hitching a ride on the Nvidia rocket ship</div>
                            <div class="article-source">Business Insider - 2025-03-26T09:00:01Z</div>
                            <div class="article-description">DDN was invited on the rocket ship that is Nvidia just a few years ago — and everything changed.</div>
                            <div class="article-content"><p>Volkswagen is using AI to speed up and scale marketing, while also integrating ChatGPT into its vehicles, says CMO Susanne Franz</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Googlers grilled leaders about smaller pay bumps in a recent all-hands</div>
                            <div class="article-source">Business Insider - 2025-03-26T23:12:06Z</div>
                            <div class="article-description">At a recent Google all-hands meeting, employees question leadership on 2025 compensation packages.</div>
                            <div class="article-content"><p>How tech layoffs could affect the economy</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Dell&#x27;s staff numbers have dropped by 25,000 in just 2 years</div>
                            <div class="article-source">Business Insider - 2025-03-26T16:52:40Z</div>
                            <div class="article-description">Dell&#x27;s latest SEC filing shows that head count fell from 133,000 in 2023 to 108,000 now, a fall of 19%.</div>
                            <div class="article-content"><p>Nearly 50,000 tech workers have been laid off — but there&#x27;s a hack to avoid layoffs</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Powerful Medical Receives FDA Breakthrough Device Designation for PMcardio STEMI AI ECG Model</div>
                            <div class="article-source">VentureBeat - March 25, 2025 10:25 AM</div>
                            <div class="article-description">Press Release Powerful Medical, a leader in AI-driven cardiovascular diagnostics, announces that its PMcardio STEMI AI ECG model has been granted Breakthrough Device Designation by the US Food and Drug Administration (FDA). This designation recognizes PMcardio as a…</div>
                            <div class="article-content"><p>NEW YORK–(BUSINESS WIRE)–March 25, 2025– Powerful Medical, a leader in AI-driven cardiovascular diagnostics, announces that its PMcardio STEMI AI ECG model has been granted Breakthrough Device Designation by the US Food and Drug Administration (FDA). This designation recognizes PMcardio as a breakthrough technology for the detection of ST-elevation myocardial infarction (STEMI) and STEMI equivalents-a life-threatening cardiac condition requiring immediate intervention. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20250325333487/en/ Powerful Medical Receives FDA Breakthrough Device Designation for PMcardio STEMI AI ECG Model Every 40 seconds, someone in the US suffers from a heart attack¹, where rapid diagnosis is crucial to saving heart muscle and preventing long-term damage, often leading to higher mortality rates. The ECG remains the primary tool for rapid diagnostics, yet the standard of care often fails to detect heart attacks accurately and timely, resulting in critical delays in treatment. PMcardio is the only solution capable of detecting both STEMI and STEMI equivalents on the ECG-aligning with the emerging emphasis on a paradigm shift towards Occlusion Myocardial Infarction (OMI) diagnosis and bridging a vital gap in early, precise heart attack diagnosis. “For the last 20 years, life-saving treatment exists for heart attack patients, yet far too many still don’t receive the urgent care they need due to delays in diagnosis and inefficient triage,” said Robert Herman, MD, PhD, Chief Medical Officer of Powerful Medical. This is especially critical in settings where immediate specialist evaluation isn’t available-only 17% of patients presenting to rural centers make it to the catheterization lab in time for intervention.² Dr. Herman added, “By equipping physicians and allied providers with an AI-powered tool for accurate and immediate STEMI detection, available around the clock, we can bridge this gap, ensure timely treatment, and improve patient outcomes, often preventing avoidable deaths”. The FDA’s Breakthrough Device Designation provides PMcardio with an expedited review process and close collaboration with the agency on its path toward market authorization. This designation is reserved for technologies that offer significant advantages over existing solutions and address unmet medical needs. This recognition underscores the FDA’s acknowledgment of Powerful Medical’s STEMI AI ECG Model, dubbed “Queen of Hearts”, to set a new standard in frontline heart attack detection and triage, ultimately enhancing care quality, accelerating treatment decisions, and saving lives through earlier and more accurate diagnosis. “FDA Breakthrough Device Designation is a pivotal milestone in our effort to revolutionize heart attack detection and ensure every patient receives immediate, life-saving care,” said Felix Bauer, COO of Powerful Medical. “We’re committed to bringing this life-saving technology to the US, the largest healthcare market in the world. This recognition by the FDA validates the impact of our innovation and brings us closer to transforming emergency cardiac care on a global scale,” added Martin Herman, CEO. With this designation, Powerful Medical not only works closely with the FDA on market approval but also gains improved access to CMS reimbursement mechanisms to bring the PMcardio STEMI AI ECG Model to healthcare providers nationwide, serving US public health. About Powerful Medical Powerful Medical is a pioneering health technology company specializing in AI-driven cardiovascular diagnostics. Its flagship product, PMcardio, harnesses AI to enhance ECG interpretation, streamline patient triage, and support clinical decision-making. With a mission to bridge the gap between innovation and clinical practice, Powerful Medical is committed to ensuring every patient receives the highest standard of care. References   View source version on businesswire.com: https://www.businesswire.com/news/home/20250325333487/en/ Lucia Bojkovskapr@powerfulmedical.com If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Walt Disney had a surprising role in the creation of Bumble</div>
                            <div class="article-source">Business Insider - 2025-03-26T18:42:46Z</div>
                            <div class="article-description">Whitney Wolfe Herd has worked on two of the most popular dating apps, Bumble and Tinder. Entertainment legend Walt Disney helped her get there.</div>
                            <div class="article-content"><p>DeSantis vs. Disney: Who are the winners and losers?</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">New analyst forecast says X&#x27;s advertising revenue is on the upswing, driven partly by &#x27;fear&#x27;</div>
                            <div class="article-source">Business Insider - 2025-03-26T18:41:37Z</div>
                            <div class="article-description">EMARKETER forecasts X&#x27;s US ad revenue will jump by 17.5% this year but cautions that &quot;fear is not a sustainable motivator.&quot;</div>
                            <div class="article-content"><p>Full content not available</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">How YouTube paranormal investigators Sam and Colby created a $20 million revenue empire</div>
                            <div class="article-source">Business Insider - 2025-03-26T14:52:21Z</div>
                            <div class="article-description">The multifaceted media business of YouTube duo Sam and Colby generated about $20 million in revenue last year, they told Business Insider.</div>
                            <div class="article-content"><p>How Sesame Street stays relevant to every generation, according to the brand&#x27;s marketing head</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">23andMe cofounder says company &#x27;lost its way&#x27; without &#x27;proper governance&#x27;</div>
                            <div class="article-source">Business Insider - 2025-03-26T23:20:33Z</div>
                            <div class="article-description">23andMe cofounder Linda Avey is mourning what the biotech company could have become. The company filed for Chapter 11 bankruptcy protection on Sunday.</div>
                            <div class="article-content"><p>5 ways Elon Musk shook up Twitter as CEO</p></div>
                        </div>
                
                    </section>
            
                    <section id="theme-4">
                        <h2>Trade and Tariffs <small>(6 articles)</small></h2>
                        <p>Articles related to international trade, tariffs, and their impact on businesses</p>
                        
                        <h3>Articles in this Theme</h3>
            
                        <div class="article-card">
                            <div class="article-title">U.S. blacklists over 50 Chinese companies in bid to curb Beijing&#x27;s AI, chip capabilities</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 202512:56 AM EDT</div>
                            <div class="article-description">The export restrictions come at a time when tensions between Washington and Beijing have been rising with the Trump administration ratcheting up tariffs against China.</div>
                            <div class="article-content"><p>The U.S. on Tuesday added dozens of Chinese tech companies to its export blacklist in its first such effort under the Donald Trump administration, as it doubles down on curtailing Beijing&#x27;s artificial intelligence and advanced computing capabilities. The U.S. Department of Commerce&#x27;s Bureau of Industry and Security added 80 organizations to an &quot;entity list,&quot; with more than 50 from China, barring American companies from supplying to those on the list without government permits. The companies were blacklisted for allegedly acting contrary to U.S. national security and foreign policy interests, the agency said, as part of its efforts to further restrict Beijing&#x27;s access to exascale computing tech, which can process vast amounts of data at very high speeds, as well as quantum technologies. Dozens of Chinese entities were targeted for their alleged involvement in developing advanced AI, supercomputers and high-performance AI chips for military purposes, the Commerce Department said, adding that two firms were supplying to sanctioned entities such as Huawei and its affiliated chipmaker HiSilicon. It blacklisted 27 Chinese entities for acquiring U.S.-origin items to support China&#x27;s military modernization and seven firms for helping advance China&#x27;s quantum technology capabilities. Among the organizations in the &quot;entity list&quot; were also six subsidiaries of Chinese cloud-computing firm Inspur Group, which had been blacklisted by the Joe Biden administration in 2023. China&#x27;s foreign ministry said late Wednesday it &quot;strongly condemns&quot; the export restrictions while urging the U.S. to &quot;stop generalizing national security,&quot; Reuters reported. The latest additions &quot;cast an ever-widening net aimed at third countries, transit points and intermediaries,&quot; said Alex Capri, a senior lecturer at National University of Singapore and author of &quot;Techno-Nationalism: How It&#x27;s Reshaping Trade, Geopolitics and Society.&quot; Chinese firms have managed to gain access to U.S. strategic dual-use technologies via certain third parties, he said, referring to loopholes that have allowed Chinese companies access to U.S. technologies despite restrictions. &quot;U.S. officials will continue to step up tracking and tracing operations aimed at the smuggling of advanced semiconductors made by Nvidia and Advanced Micro Devices,&quot; he said. The expanded export restrictions come at a time when tensions between Washington and Beijing have been rising with the Trump administration ratcheting up tariffs against China. The rapid rise of Chinese AI startup DeepSeek has boosted the adoption of open-source low-cost AI models in China, putting pressure on leading U.S. competitors with higher-cost, proprietary models. The Biden administration imposed sweeping export controls against China, encompassing everything from semiconductors to supercomputers under the so-called &quot;small yard, high fence&quot; policy. The approach aims to place restrictions on a small number of technologies with significant military potential while maintaining normal economic exchange in other areas. Under Secretary of Commerce for Industry and Security Jeffrey I. Kessler said the agency was &quot;sending a clear, resounding message&quot; that the Trump administration will prevent U.S. technologies from &quot;being misused for high performance computing, hypersonic missiles, military aircraft training, and UAVs (unmanned aerial vehicle) that threaten our national security.&quot; &quot;The entity list is one of many powerful tools at our disposal to identify and cut off foreign adversaries seeking to exploit American technology for malign purposes,&quot; he added. Inspur Group and Huawei did not immediately respond to CNBC&#x27;s requests for comment.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Trump says he may reduce China tariffs to help close a TikTok deal</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20257:06 PM EDT</div>
                            <div class="article-description">&quot;Maybe I&#x27;ll give them a little reduction in tariffs or something to get it done,&quot; President Donald Trump said about a deal involving TikTok&#x27;s U.S. operations</div>
                            <div class="article-content"><p>President Donald Trump said he may reduce tariffs on China to facilitate a deal that would result in ByteDance selling the U.S. operations of TikTok. China &quot;is going to have to play a role&quot; in approving a TikTok-related divestiture, Trump said in a press conference Wednesday. &quot;Maybe I&#x27;ll give them a little reduction in tariffs or something to get it done,&quot; Trump said. &quot;TikTok is big, but every point in tariffs is worth more than TikTok.&quot; Although a national security law requires ByteDance to divest TikTok&#x27;s U.S. operations or face an effective ban in the country, Trump in January signed an executive order that delayed the deadline for a deal to April 5. Trump has previously said that he wants the U.S. to maintain a 50% ownership position in TikTok via a joint venture. It&#x27;s possible he will extend the TikTok deadline again, Trump said Wednesday. &quot;We&#x27;re going to have a form of a deal, but if it&#x27;s not finished, it&#x27;s not a big deal,&quot; Trump said. &quot;We&#x27;ll just extend it.&quot; Vice President JD Vance told NBC News earlier this month that he was confident that a TikTok-related deal would happen by the April deadline. &quot;There will almost certainly be a high-level agreement that I think satisfies our national security concerns, allows there to be a distinct American TikTok enterprise,&quot; Vance said. WATCH: TikTok bid is &#x27;in active dialogue&#x27; with Trump administration.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Trump says Tesla CEO Elon Musk didn’t advise on auto tariffs &#x27;because he may have a conflict&#x27;</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20257:31 PM EDT</div>
                            <div class="article-description">When asked by reporters whether the new tariffs would be good for Musk&#x27;s autos business, Tesla, President Donald Trump said they may be &quot;net neutral or they may be good.&quot;</div>
                            <div class="article-content"><p>In this article After President Donald Trump said on Wednesday he would impose 25% tariffs on &quot;all cars that are not made in the United States,&quot; he said his key advisor, Tesla CEO Elon Musk, had not weighed in on the matter, &quot;because he may have a conflict.&quot; He added that Musk had never &quot;asked me for a favor in business whatsoever.&quot; Musk serves as a senior advisor to Trump, having earlier contributed $290 million to propel him back to the White House. While Musk remains at the helm of his companies, including SpaceX and Tesla, he is also leading the Department of Government Efficiency (DOGE), which is an effort to slash federal government spending, personnel and consolidate or eliminate various federal agencies and services. Earlier this month, Trump turned the South Lawn of the White House into a temporary Tesla showroom. The company delivered five of its electric vehicles there for the president to inspect after he had declared, in a post on Truth Social, that he would buy a Tesla to show support for Musk and the business. Musk stood by his side while Trump called the vehicles &quot;beautiful&quot; and praised the unorthodox design of the angular, steel Tesla Cybertruck. When asked by reporters whether the new tariffs would be good for Musk&#x27;s autos business, Tesla, Trump said they may be &quot;net neutral or they may be good.&quot; He pointed to Tesla&#x27;s vehicle assembly plants in Austin, Texas and Fremont, California and opined that, &quot;anybody that has plants in the United States — it&#x27;s going to be good for them.&quot; Tesla recently wrote, in a letter to the U.S. Trade Representative, that &quot;even with aggressive localization&quot; of its supply chain domestically, &quot;certain parts and components are difficult or impossible to source within the United States.&quot; The company urged the USTR to &quot;consider the downstream impacts of certain proposed actions taken to address unfair trade practices.&quot; Tesla and other automakers commonly buy headlamps, automotive glass, brakes, body panels, suspension parts, and printed circuit boards for various electrical systems in their vehicles from foreign suppliers in Mexico, Canada and China, especially. Musk and Tesla did not immediately respond to a request for comment about how the new 25% tariffs may impact their business. Tesla faces an onslaught of competition with more automakers selling fully electric models than ever before. However, the company&#x27;s most formidable rival in battery electric vehicles, BYD in China, has never been authorized to sell its electric cars in the United States. Domestic automakers including General Motors, Ford, Rivian and Tesla saw shares declining slightly after hours following the latest tariffs announcement.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Trump says Tesla CEO Elon Musk didn’t advise on auto tariffs &#x27;because he may have a conflict&#x27;</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20257:31 PM EDT</div>
                            <div class="article-description">When asked by reporters whether the new tariffs would be good for Musk&#x27;s autos business, Tesla, President Donald Trump said they may be &quot;net neutral or they may be good.&quot;</div>
                            <div class="article-content"><p>In this article After President Donald Trump said on Wednesday he would impose 25% tariffs on &quot;all cars that are not made in the United States,&quot; he said his key advisor, Tesla CEO Elon Musk, had not weighed in on the matter, &quot;because he may have a conflict.&quot; He added that Musk had never &quot;asked me for a favor in business whatsoever.&quot; Musk serves as a senior advisor to Trump, having earlier contributed $290 million to propel him back to the White House. While Musk remains at the helm of his companies, including SpaceX and Tesla, he is also leading the Department of Government Efficiency (DOGE), which is an effort to slash federal government spending, personnel and consolidate or eliminate various federal agencies and services. Earlier this month, Trump turned the South Lawn of the White House into a temporary Tesla showroom. The company delivered five of its electric vehicles there for the president to inspect after he had declared, in a post on Truth Social, that he would buy a Tesla to show support for Musk and the business. Musk stood by his side while Trump called the vehicles &quot;beautiful&quot; and praised the unorthodox design of the angular, steel Tesla Cybertruck. When asked by reporters whether the new tariffs would be good for Musk&#x27;s autos business, Tesla, Trump said they may be &quot;net neutral or they may be good.&quot; He pointed to Tesla&#x27;s vehicle assembly plants in Austin, Texas and Fremont, California and opined that, &quot;anybody that has plants in the United States — it&#x27;s going to be good for them.&quot; Tesla recently wrote, in a letter to the U.S. Trade Representative, that &quot;even with aggressive localization&quot; of its supply chain domestically, &quot;certain parts and components are difficult or impossible to source within the United States.&quot; The company urged the USTR to &quot;consider the downstream impacts of certain proposed actions taken to address unfair trade practices.&quot; Tesla and other automakers commonly buy headlamps, automotive glass, brakes, body panels, suspension parts, and printed circuit boards for various electrical systems in their vehicles from foreign suppliers in Mexico, Canada and China, especially. Musk and Tesla did not immediately respond to a request for comment about how the new 25% tariffs may impact their business. Tesla faces an onslaught of competition with more automakers selling fully electric models than ever before. However, the company&#x27;s most formidable rival in battery electric vehicles, BYD in China, has never been authorized to sell its electric cars in the United States. Domestic automakers including General Motors, Ford, Rivian and Tesla saw shares declining slightly after hours following the latest tariffs announcement.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Powerful Medical Receives FDA Breakthrough Device Designation for PMcardio STEMI AI ECG Model</div>
                            <div class="article-source">VentureBeat - March 25, 2025 10:25 AM</div>
                            <div class="article-description">Press Release Powerful Medical, a leader in AI-driven cardiovascular diagnostics, announces that its PMcardio STEMI AI ECG model has been granted Breakthrough Device Designation by the US Food and Drug Administration (FDA). This designation recognizes PMcardio as a…</div>
                            <div class="article-content"><p>NEW YORK–(BUSINESS WIRE)–March 25, 2025– Powerful Medical, a leader in AI-driven cardiovascular diagnostics, announces that its PMcardio STEMI AI ECG model has been granted Breakthrough Device Designation by the US Food and Drug Administration (FDA). This designation recognizes PMcardio as a breakthrough technology for the detection of ST-elevation myocardial infarction (STEMI) and STEMI equivalents-a life-threatening cardiac condition requiring immediate intervention. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20250325333487/en/ Powerful Medical Receives FDA Breakthrough Device Designation for PMcardio STEMI AI ECG Model Every 40 seconds, someone in the US suffers from a heart attack¹, where rapid diagnosis is crucial to saving heart muscle and preventing long-term damage, often leading to higher mortality rates. The ECG remains the primary tool for rapid diagnostics, yet the standard of care often fails to detect heart attacks accurately and timely, resulting in critical delays in treatment. PMcardio is the only solution capable of detecting both STEMI and STEMI equivalents on the ECG-aligning with the emerging emphasis on a paradigm shift towards Occlusion Myocardial Infarction (OMI) diagnosis and bridging a vital gap in early, precise heart attack diagnosis. “For the last 20 years, life-saving treatment exists for heart attack patients, yet far too many still don’t receive the urgent care they need due to delays in diagnosis and inefficient triage,” said Robert Herman, MD, PhD, Chief Medical Officer of Powerful Medical. This is especially critical in settings where immediate specialist evaluation isn’t available-only 17% of patients presenting to rural centers make it to the catheterization lab in time for intervention.² Dr. Herman added, “By equipping physicians and allied providers with an AI-powered tool for accurate and immediate STEMI detection, available around the clock, we can bridge this gap, ensure timely treatment, and improve patient outcomes, often preventing avoidable deaths”. The FDA’s Breakthrough Device Designation provides PMcardio with an expedited review process and close collaboration with the agency on its path toward market authorization. This designation is reserved for technologies that offer significant advantages over existing solutions and address unmet medical needs. This recognition underscores the FDA’s acknowledgment of Powerful Medical’s STEMI AI ECG Model, dubbed “Queen of Hearts”, to set a new standard in frontline heart attack detection and triage, ultimately enhancing care quality, accelerating treatment decisions, and saving lives through earlier and more accurate diagnosis. “FDA Breakthrough Device Designation is a pivotal milestone in our effort to revolutionize heart attack detection and ensure every patient receives immediate, life-saving care,” said Felix Bauer, COO of Powerful Medical. “We’re committed to bringing this life-saving technology to the US, the largest healthcare market in the world. This recognition by the FDA validates the impact of our innovation and brings us closer to transforming emergency cardiac care on a global scale,” added Martin Herman, CEO. With this designation, Powerful Medical not only works closely with the FDA on market approval but also gains improved access to CMS reimbursement mechanisms to bring the PMcardio STEMI AI ECG Model to healthcare providers nationwide, serving US public health. About Powerful Medical Powerful Medical is a pioneering health technology company specializing in AI-driven cardiovascular diagnostics. Its flagship product, PMcardio, harnesses AI to enhance ECG interpretation, streamline patient triage, and support clinical decision-making. With a mission to bridge the gap between innovation and clinical practice, Powerful Medical is committed to ensuring every patient receives the highest standard of care. References   View source version on businesswire.com: https://www.businesswire.com/news/home/20250325333487/en/ Lucia Bojkovskapr@powerfulmedical.com If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Trump says he may reduce China tariffs to help close a TikTok deal</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20257:06 PM EDT</div>
                            <div class="article-description">&quot;Maybe I&#x27;ll give them a little reduction in tariffs or something to get it done,&quot; President Donald Trump said about a deal involving TikTok&#x27;s U.S. operations</div>
                            <div class="article-content"><p>President Donald Trump said he may reduce tariffs on China to facilitate a deal that would result in ByteDance selling the U.S. operations of TikTok. China &quot;is going to have to play a role&quot; in approving a TikTok-related divestiture, Trump said in a press conference Wednesday. &quot;Maybe I&#x27;ll give them a little reduction in tariffs or something to get it done,&quot; Trump said. &quot;TikTok is big, but every point in tariffs is worth more than TikTok.&quot; Although a national security law requires ByteDance to divest TikTok&#x27;s U.S. operations or face an effective ban in the country, Trump in January signed an executive order that delayed the deadline for a deal to April 5. Trump has previously said that he wants the U.S. to maintain a 50% ownership position in TikTok via a joint venture. It&#x27;s possible he will extend the TikTok deadline again, Trump said Wednesday. &quot;We&#x27;re going to have a form of a deal, but if it&#x27;s not finished, it&#x27;s not a big deal,&quot; Trump said. &quot;We&#x27;ll just extend it.&quot; Vice President JD Vance told NBC News earlier this month that he was confident that a TikTok-related deal would happen by the April deadline. &quot;There will almost certainly be a high-level agreement that I think satisfies our national security concerns, allows there to be a distinct American TikTok enterprise,&quot; Vance said. WATCH: TikTok bid is &#x27;in active dialogue&#x27; with Trump administration.</p></div>
                        </div>
                
                    </section>
            
                    <section id="theme-5">
                        <h2>Data Privacy and Security <small>(4 articles)</small></h2>
                        <p>Articles related to data privacy, security breaches, and protection measures</p>
                        
                        <h3>Articles in this Theme</h3>
            
                        <div class="article-card">
                            <div class="article-title">‘Studio Ghibli’ AI image trend overwhelms OpenAI’s new GPT-4o feature, delaying free tier</div>
                            <div class="article-source">VentureBeat - March 26, 2025 4:14 PM</div>
                            <div class="article-description">The new feature has been widely embraced by users of X, but it raises copyright concerns and goes against Studio Ghibli&#x27;s creator.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More If you’ve been on the internet — or, at least, on the social network X — in the last day or so, you’ve likely come across colorful, smooth anime-style images of famous photographs rendered in the style of the Japanese studio, Studio Ghibli (the one that made Princess Mononoke, The Boy and the Crane, and My Neighbor Totoro, among many other classic animated films). In fact, some users are complaining because their feeds seem to be filled with nearly exclusively these types of images. Whether it’s current President Trump, the iconic image of the “Tank Man” during the 1989 pro-Democracy Tiananmen Square protests, Osama Bin Laden, Jeffrey Epstein, or even other pop culture moments and characters like Sam Rockwell’s iconic cameo on The White Lotus and many popular memes of yore, people have been making and sharing these images at a rapid clip. Much of that is thanks to OpenAI’s new update to the GPT-4o model behind ChatGPT for Pro, Plus, and Team subscription tiers, which turns on “native image generation.” While ChatGPT previously allowed users to create images from text prompts, it did so by routing them to another, separate OpenAI model, DALL-E 3. But OpenAI’s GPT-4o model is so named with an “o” because it is an “omni” model — the company trained it not only on text and code, but also on imagery and presumably, video and audio as well, allowing it to be able to understand all these forms of media and their similarities and differences, conceive of ideas across them (an “apple” is not just a word, but also something that can be drawn as a red or yellow or green fruit), and accurately produce said media given text prompts by a user without connecting to any external models. As a consequence, like rival Google AI Studio’s recent update to include a Gemini 2.0 Flash experimental image creation model, the new OpenAI GPT-4o can also accept image uploads of any pre-existing image in your camera roll or that you’ve screenshotted or saved off the web. First, navigate to Chat.com or ChatGPT.com and ensure you’re logged in with your ChatGPT Plus, Pro, or Team account and that the AI model selector (located in the left corner of the session window) is showing “GPT-4o” as the chosen model (you can click it to drop down and select the proper model between the available options). Once you do that, you can upload an image to ChatGPT using the “+” button in the lower left hand corner of the prompt entry text box, you can now ask the new GPT-4o with image creation model to render your pre-existing image in a new style. If you want, you can try it by uploading a photo of yourself and friends and typing “make all these people in the style of a Studio Ghibli animation.” And after a few seconds, it will do so with some pretty convincing and amusing results. It even supports attaching multiple images and combining them into a single piece. OpenAI initially said it would also enable this feature for free (non-paying users of ChatGPT), but unfortunately for them, co-founder and CEO Sam Altman today posted that the feature will be delayed due to the overwhelming demand by existing paying subscribers to ChatGPT Plus, Pro, and Team tiers. As he wrote on X: “images in chatgpt are wayyyy more popular than we expected (and we had pretty high expectations). rollout to our free tier is unfortunately going to be delayed for awhile.“ Meanwhile, those who do have access will likely continue cranking out image edits in this and other recognizable or novel styles. Of course, not everyone is a fan of OpenAI’s work here. In fact, Studio Ghibli creator Hayao Miyazaki himself appeared in a documentary back in 2016 — and one of the most memorable moments from it still referenced to this day is him reacting with overwhelming disgust and revulsion to an early example of AI-powered animation and physics by, you guessed it, an OpenAI model. As with many generative AI products and services, OpenAI’s training data for this new image generation capability remains under wraps, but is widely speculated to contain copyrighted material — and while imitating a style is generally not considered copyright infringement in the U.S., it is rubbing some fans of the original animation the wrong way. For now, those brands and enterprises looking to play with this style should do so with caution and after serious consideration, given the possible negative blowback among some users. But for those who are unabashedly pro-AI tools or with more forgiving and fun-loving fanbases, it’s clear that OpenAI has yet another hit on its hands. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">The concern with CoreWeave’s 250,000 Nvidia chips ahead of its IPO</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20255:00 PM EDT</div>
                            <div class="article-description">CoreWeave sells access to Nvidia graphics processors as a service, allowing developers to rent chips by the hour so they can develop more advanced AI models.</div>
                            <div class="article-content"><p>In this article With 250,000 highly-desired Nvidia graphics processors, CoreWeave has become one of the most prominent &quot;GPU clouds,&quot; a status it hopes investors will value when it debuts on the public markets. But the world of artificial intelligence hardware is moving so quickly that it raises questions about how long those chips will remain on the cutting edge and in demand. It&#x27;s a concern that could impact investor demand for shares of CoreWeave, one of the most anticipated IPOs in years. CoreWeave, which rents out remote access to computers based on Nvidia AI chips, said in a financial filing this month that most of its AI chips are from Nvidia&#x27;s Hopper generation. Those chips, such as the H100, were state-of-the-art in 2023 and 2024. They were scarce as AI companies bought or rented all the chips they could get in the wake of OpenAI ushering in the generative AI age with the release of ChatGPT in late 2022. But these days, Nvidia CEO Jensen Huang says that his company&#x27;s Hopper chips are getting blown out of the water by their successors – the Blackwell generation of GPUs, which have been shipping since late 2024. Hopper chips are &quot;fine&quot; for some circumstances but &quot;not many,&quot; Huang joked at Nvidia&#x27;s GTC conference last week. &quot;In a reasoning model, Blackwell is 40 times the performance of Hopper. Straight up. Pretty amazing,&quot; Huang said. &quot;I said before that when Blackwell starts shipping in volume, you couldn&#x27;t give Hoppers away.&quot; That&#x27;s great for Nvidia, which needs to find ways to keep selling chips to the companies committed to the AI race, but it&#x27;s bad news for GPU clouds like CoreWeave. That&#x27;s because the New Jersey company models the future trajectory of its business based on how much it anticipates being able to rent Nvidia chips out for over the next five to six years. Huang may have been kidding, but Nvidia spent much of its event detailing just how much better its Blackwell chips are. In Nvidia&#x27;s view, the best way to decrease the high cost of serving AI is by buying faster chips. Blackwell systems are in full production and shipping to customers, and Nvidia plans to introduce an upgraded version of Blackwell in late 2026. When new chips come out, the older chips — the kind CoreWeave has a quarter of a million of — go down in price, Huang said. So too does the price of renting them. Older chips don&#x27;t just stop working when new ones come out. Most companies, including CoreWeave, plan to use Hopper chips for six years. But Nvidia is telling customers that its newer, faster chips are capable of producing more AI content, which leads to more revenues at a better margin for clouds. An H100 would have to be priced 65% lower per hour than an Nvidia Blackwell GB200 NVL system for the two systems to be competitive in price per output to a renter. Put another way, the H100 would have to rent at 98 cents per hour to match the price per output of a Blackwell rack system priced at $2.20 per hour per GPU, SemiAnalysis estimated, speaking generally about AI rentals. H100s rented for as much as $8 per hour back in 2023 and often required long commitments and lead times, but now, usage of those chips can be summoned in minutes with a credit card. Some services now offer rented H100 access for under $2 per hour. The industry could be entering a period where the useful life of AI chips is reduced, Barclays analyst Ross Sandler wrote in a note on Friday. He was focused on hyperscalers — Meta, Google and Amazon — but the trend affects smaller cloud providers like CoreWeave, too. &quot;These assets are becoming obsolete at a much more rapid pace given how much innovation and speed improvements happen with each generation,&quot; Sandler wrote. This threatens company earnings if they end up depreciating older equipment faster, he said. CoreWeave says that if there were to be changes to the &quot;significant&quot; assumptions it makes about the useful lifetime of its AI infrastructure, it could hurt its business or future prospects. CoreWeave has also borrowed nearly $8 billion to buy Nvidia chips and build its data centers, sometimes using the GPUs it amassed as collateral. Analysts and investors are also increasingly asking questions about the useful lifespan of these new AI systems and whether their financial depreciation schedules should be accelerated because the technology is improving so fast. CoreWeave says in its filing that it seeks to offer state-of-the-art infrastructure and says it will continue spending to expand and improve its data centers. &quot;Part of this process entails cycling out outdated components of our infrastructure and replacing them with the latest technology available,&quot; the New Jersey company said. &quot;This requires us to make certain estimates with respect to the useful life of the components of our infrastructure and to maximize the value of the components of our infrastructure, including our GPUs, to the fullest extent possible.&quot; CoreWeave and Nvidia maintain a good relationship. CoreWeave will certainly buy more chips from Nvidia, which owns more than 5% of the New Jersey company. &quot;We&#x27;re super proud of them,&quot; Huang said last week. But Nvidia&#x27;s road map for releasing new chips that it proudly touts will make their predecessors obsolete is a threat to CoreWeave&#x27;s ambitions. WATCH: CoreWeave begins marketing IPO, targeting price range of $47-$55 per share: Report</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Trump says Tesla CEO Elon Musk didn’t advise on auto tariffs &#x27;because he may have a conflict&#x27;</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20257:31 PM EDT</div>
                            <div class="article-description">When asked by reporters whether the new tariffs would be good for Musk&#x27;s autos business, Tesla, President Donald Trump said they may be &quot;net neutral or they may be good.&quot;</div>
                            <div class="article-content"><p>In this article After President Donald Trump said on Wednesday he would impose 25% tariffs on &quot;all cars that are not made in the United States,&quot; he said his key advisor, Tesla CEO Elon Musk, had not weighed in on the matter, &quot;because he may have a conflict.&quot; He added that Musk had never &quot;asked me for a favor in business whatsoever.&quot; Musk serves as a senior advisor to Trump, having earlier contributed $290 million to propel him back to the White House. While Musk remains at the helm of his companies, including SpaceX and Tesla, he is also leading the Department of Government Efficiency (DOGE), which is an effort to slash federal government spending, personnel and consolidate or eliminate various federal agencies and services. Earlier this month, Trump turned the South Lawn of the White House into a temporary Tesla showroom. The company delivered five of its electric vehicles there for the president to inspect after he had declared, in a post on Truth Social, that he would buy a Tesla to show support for Musk and the business. Musk stood by his side while Trump called the vehicles &quot;beautiful&quot; and praised the unorthodox design of the angular, steel Tesla Cybertruck. When asked by reporters whether the new tariffs would be good for Musk&#x27;s autos business, Tesla, Trump said they may be &quot;net neutral or they may be good.&quot; He pointed to Tesla&#x27;s vehicle assembly plants in Austin, Texas and Fremont, California and opined that, &quot;anybody that has plants in the United States — it&#x27;s going to be good for them.&quot; Tesla recently wrote, in a letter to the U.S. Trade Representative, that &quot;even with aggressive localization&quot; of its supply chain domestically, &quot;certain parts and components are difficult or impossible to source within the United States.&quot; The company urged the USTR to &quot;consider the downstream impacts of certain proposed actions taken to address unfair trade practices.&quot; Tesla and other automakers commonly buy headlamps, automotive glass, brakes, body panels, suspension parts, and printed circuit boards for various electrical systems in their vehicles from foreign suppliers in Mexico, Canada and China, especially. Musk and Tesla did not immediately respond to a request for comment about how the new 25% tariffs may impact their business. Tesla faces an onslaught of competition with more automakers selling fully electric models than ever before. However, the company&#x27;s most formidable rival in battery electric vehicles, BYD in China, has never been authorized to sell its electric cars in the United States. Domestic automakers including General Motors, Ford, Rivian and Tesla saw shares declining slightly after hours following the latest tariffs announcement.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">With 23andMe entering bankruptcy, here&#x27;s how to delete your genetic data</div>
                            <div class="article-source">CNBC - Published Tue, Mar 25 20252:37 PM EDT</div>
                            <div class="article-description">Now that 23andMe has filed for bankruptcy, another company could take control of its genetic database. </div>
                            <div class="article-content"><p>23andMe has officially filed for Chapter 11 bankruptcy protection, which means its assets — including its vast genetic database — will soon be up for sale. The company continues to sell its at-home DNA testing kits, allowing consumers to get insight into their family histories and genetic profiles. DNA data is particularly sensitive because each person&#x27;s sequence is unique, meaning it can never be fully anonymized, according to the National Human Genome Research Institute. If genetic data falls into the hands of bad actors, it could be used to facilitate identity theft, insurance fraud or other crimes. 23andMe has been plagued by privacy concerns in recent years after hackers accessed the information of nearly 7 million customers in October 2023. As part of the bankruptcy process, the company said it will seek a partner that shares its commitment to customer data privacy, and that there will be no changes to how it stores, manages and protects data through the sale process. &quot;Our users&#x27; privacy and data are important considerations in any transaction, and we remain committed to our users&#x27; privacy and to being transparent with our customers about how their data is managed,&quot; the company said in an FAQ page about the bankruptcy filing. &quot;Any buyer of 23andMe will be required to comply with applicable law with respect to the treatment of customer data.&quot; Still, experts and officials are urging 23andMe customers to proceed with caution. California Attorney General Rob Bonta on Friday issued a consumer alert, encouraging residents to consider deleting their genetic data from 23andMe, which is based in his home state. &quot;Given 23andMe&#x27;s reported financial distress, I remind Californians to consider invoking their rights and directing 23andMe to delete their data and destroy any samples of genetic material held by the company,&quot; Bonta said in the release. Adrianus Warmenhoven, who serves on the security advisory board at NordVPN, described genetic data as the &quot;blueprint of your entire biological profile.&quot; He encouraged consumers to delete their information and be mindful of the companies they chose to share it with going forward. &quot;Monitor your digital footprint regularly, and you can also sign up for credit monitoring or identity theft protection services,&quot; Warmenhoven said in a statement to CNBC. &quot;Revoke permissions you no longer require, shut down any account you don&#x27;t use, and learn about how your data is used.&quot; 23andMe said customers can still delete their account and accompanying data. Here&#x27;s how: At this point, your personal information and your account will be permanently deleted from 23andMe, according to the deletion email from the company. Additionally, your data will not be used in any future research projects, and any personal samples the company was storing will be discarded. Correction: A prior version of this story had incorrect information about what happens to downloaded data if you delete your account. WATCH: The rise and fall of 23andMe</p></div>
                        </div>
                
                    </section>
            
                    <section id="theme-6">
                        <h2>Healthcare and Medical Technology <small>(4 articles)</small></h2>
                        <p>Articles related to healthcare advancements, medical technology, and AI in healthcare</p>
                        
                        <h3>Articles in this Theme</h3>
            
                        <div class="article-card">
                            <div class="article-title">Amazon is testing shopping, health assistants as it pushes deeper into generative AI</div>
                            <div class="article-source">CNBC - Published Tue, Mar 25 202510:04 PM EDT</div>
                            <div class="article-description">With CEO Andy Jassy pushing employees to build AI apps across the company, Amazon is testing new shopping and health assistants.</div>
                            <div class="article-content"><p>In this article Amazon, in an effort to infuse generative artificial intelligence across a wider swath of its e-commerce universe, recently began testing a shopping assistant and a health-focused chatbot with a subset of users. AI has become a major area of investment across Amazon, including in its retail, cloud computing, devices and health-care businesses. Within the retail business, Amazon has already launched a shopping chatbot, an AI assistant for sellers and AI shopping guides. The new services Amazon is testing appeared on its app or website in recent weeks. An Amazon spokesperson confirmed the features are being tested in beta with some customers. The shopping tool, called Interests AI, prompts users to describe an interest &quot;using your own words,&quot; and then it generates a curated selection of products. The feature lets consumers browse for products using more conversational language and is separate from the main search bar on Amazon&#x27;s website. Within its core app, Amazon has a landing page for the feature. &quot;Describe your interest, like &#x27;coffee brewing gadgets&#x27; or &#x27;latest pickleball accessories&#x27; — and we&#x27;ll find relevant products for you,&quot; the page says. Other suggested searches include &quot;children books about persistence and dealing with failure,&quot; and &quot;brain teasers that are not too hard, made out of wood or metal.&quot; The Amazon spokesperson said Interests uses large language models to translate everyday words or phrases into queries and attributes that traditional search engines can turn into product recommendations. It&#x27;s unclear what models Interests relies on. Amazon said in a blog post after publication of this article that it expects to make the feature available to all U.S. users in the coming months. Amazon CEO Andy Jassy said last month that employees have built or are in the process of building roughly 1,000 generative AI applications across the company. Its cloud unit offers a chatbot for businesses, called Q. In commerce, the company has rolled out services for consumers as well as its millions of third-party sellers. Amazon is also exploring ways that artificial intelligence can address medical needs. The company is testing a chatbot on its website and mobile app called &quot;Health AI,&quot; which can answer health and wellness questions, &quot;provide common care options for health care needs,&quot; and suggest products. While Rufus, Amazon&#x27;s shopping chatbot, can suggest products like ice packs and ibuprofen, Health AI goes further, providing users with medical guidance and care tips, such as how to deal with cold symptoms or the flu. The site says the service can&#x27;t provide personalized medical advice. Some responses feature a &quot;clinically verified&quot; badge, which denotes information that&#x27;s been &quot;reviewed by US-based licensed clinicians,&quot; Amazon says. Health AI also steers users to Amazon&#x27;s online pharmacy, along with clinical services offered by One Medical, the primary care provider it acquired for roughly $3.9 billion in 2022. Amazon&#x27;s spokesperson said the health assistant uses Bedrock, a service launched by Amazon&#x27;s cloud unit that accesses AI models from the company and third parties. &quot;We are collecting feedback from customers, and plan to introduce new features to enhance the experience in the future,&quot; the spokesperson said in a statement. More consumers are embracing generative AI as a shopping tool, and with features like Health AI and Interests AI, Amazon wants shoppers to use its own services over rivals like OpenAI&#x27;s ChatGPT. With enough use, Amazon could gain valuable insights on the ways that people are interacting with AI assistants as the company prepares to overhaul Alexa, the digital assistant it launched more than a decade ago. Amazon announced Alexa+, a new version of the technology embedded with generative AI, late last month. The company says that Alexa+, which has yet to roll out, is capable of handling more complex tasks and can serve as an &quot;agent&quot; by taking actions for users without their direct involvement. Andrew Bell, an Amazon e-commerce manager for the National Fire Protection Association who also publishes research on Amazon&#x27;s patent filings and AI development, came across the new shopping and health features and recently posted about them on LinkedIn. Bell said in an interview that Alexa+ could potentially draw upon models developed for Amazon applications like Health AI to answer queries. &quot;If there&#x27;s a health-related question, Alexa+ is going to maybe call on Health AI,&quot; Bell said. &quot;If there&#x27;s a product-related question, Alexa+ can call on Rufus.&quot; WATCH: Amazon&#x27;s SVP of Devices on Alexa+</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">The concern with CoreWeave’s 250,000 Nvidia chips ahead of its IPO</div>
                            <div class="article-source">CNBC - Published Wed, Mar 26 20255:00 PM EDT</div>
                            <div class="article-description">CoreWeave sells access to Nvidia graphics processors as a service, allowing developers to rent chips by the hour so they can develop more advanced AI models.</div>
                            <div class="article-content"><p>In this article With 250,000 highly-desired Nvidia graphics processors, CoreWeave has become one of the most prominent &quot;GPU clouds,&quot; a status it hopes investors will value when it debuts on the public markets. But the world of artificial intelligence hardware is moving so quickly that it raises questions about how long those chips will remain on the cutting edge and in demand. It&#x27;s a concern that could impact investor demand for shares of CoreWeave, one of the most anticipated IPOs in years. CoreWeave, which rents out remote access to computers based on Nvidia AI chips, said in a financial filing this month that most of its AI chips are from Nvidia&#x27;s Hopper generation. Those chips, such as the H100, were state-of-the-art in 2023 and 2024. They were scarce as AI companies bought or rented all the chips they could get in the wake of OpenAI ushering in the generative AI age with the release of ChatGPT in late 2022. But these days, Nvidia CEO Jensen Huang says that his company&#x27;s Hopper chips are getting blown out of the water by their successors – the Blackwell generation of GPUs, which have been shipping since late 2024. Hopper chips are &quot;fine&quot; for some circumstances but &quot;not many,&quot; Huang joked at Nvidia&#x27;s GTC conference last week. &quot;In a reasoning model, Blackwell is 40 times the performance of Hopper. Straight up. Pretty amazing,&quot; Huang said. &quot;I said before that when Blackwell starts shipping in volume, you couldn&#x27;t give Hoppers away.&quot; That&#x27;s great for Nvidia, which needs to find ways to keep selling chips to the companies committed to the AI race, but it&#x27;s bad news for GPU clouds like CoreWeave. That&#x27;s because the New Jersey company models the future trajectory of its business based on how much it anticipates being able to rent Nvidia chips out for over the next five to six years. Huang may have been kidding, but Nvidia spent much of its event detailing just how much better its Blackwell chips are. In Nvidia&#x27;s view, the best way to decrease the high cost of serving AI is by buying faster chips. Blackwell systems are in full production and shipping to customers, and Nvidia plans to introduce an upgraded version of Blackwell in late 2026. When new chips come out, the older chips — the kind CoreWeave has a quarter of a million of — go down in price, Huang said. So too does the price of renting them. Older chips don&#x27;t just stop working when new ones come out. Most companies, including CoreWeave, plan to use Hopper chips for six years. But Nvidia is telling customers that its newer, faster chips are capable of producing more AI content, which leads to more revenues at a better margin for clouds. An H100 would have to be priced 65% lower per hour than an Nvidia Blackwell GB200 NVL system for the two systems to be competitive in price per output to a renter. Put another way, the H100 would have to rent at 98 cents per hour to match the price per output of a Blackwell rack system priced at $2.20 per hour per GPU, SemiAnalysis estimated, speaking generally about AI rentals. H100s rented for as much as $8 per hour back in 2023 and often required long commitments and lead times, but now, usage of those chips can be summoned in minutes with a credit card. Some services now offer rented H100 access for under $2 per hour. The industry could be entering a period where the useful life of AI chips is reduced, Barclays analyst Ross Sandler wrote in a note on Friday. He was focused on hyperscalers — Meta, Google and Amazon — but the trend affects smaller cloud providers like CoreWeave, too. &quot;These assets are becoming obsolete at a much more rapid pace given how much innovation and speed improvements happen with each generation,&quot; Sandler wrote. This threatens company earnings if they end up depreciating older equipment faster, he said. CoreWeave says that if there were to be changes to the &quot;significant&quot; assumptions it makes about the useful lifetime of its AI infrastructure, it could hurt its business or future prospects. CoreWeave has also borrowed nearly $8 billion to buy Nvidia chips and build its data centers, sometimes using the GPUs it amassed as collateral. Analysts and investors are also increasingly asking questions about the useful lifespan of these new AI systems and whether their financial depreciation schedules should be accelerated because the technology is improving so fast. CoreWeave says in its filing that it seeks to offer state-of-the-art infrastructure and says it will continue spending to expand and improve its data centers. &quot;Part of this process entails cycling out outdated components of our infrastructure and replacing them with the latest technology available,&quot; the New Jersey company said. &quot;This requires us to make certain estimates with respect to the useful life of the components of our infrastructure and to maximize the value of the components of our infrastructure, including our GPUs, to the fullest extent possible.&quot; CoreWeave and Nvidia maintain a good relationship. CoreWeave will certainly buy more chips from Nvidia, which owns more than 5% of the New Jersey company. &quot;We&#x27;re super proud of them,&quot; Huang said last week. But Nvidia&#x27;s road map for releasing new chips that it proudly touts will make their predecessors obsolete is a threat to CoreWeave&#x27;s ambitions. WATCH: CoreWeave begins marketing IPO, targeting price range of $47-$55 per share: Report</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Powerful Medical Receives FDA Breakthrough Device Designation for PMcardio STEMI AI ECG Model</div>
                            <div class="article-source">VentureBeat - March 25, 2025 10:25 AM</div>
                            <div class="article-description">Press Release Powerful Medical, a leader in AI-driven cardiovascular diagnostics, announces that its PMcardio STEMI AI ECG model has been granted Breakthrough Device Designation by the US Food and Drug Administration (FDA). This designation recognizes PMcardio as a…</div>
                            <div class="article-content"><p>NEW YORK–(BUSINESS WIRE)–March 25, 2025– Powerful Medical, a leader in AI-driven cardiovascular diagnostics, announces that its PMcardio STEMI AI ECG model has been granted Breakthrough Device Designation by the US Food and Drug Administration (FDA). This designation recognizes PMcardio as a breakthrough technology for the detection of ST-elevation myocardial infarction (STEMI) and STEMI equivalents-a life-threatening cardiac condition requiring immediate intervention. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20250325333487/en/ Powerful Medical Receives FDA Breakthrough Device Designation for PMcardio STEMI AI ECG Model Every 40 seconds, someone in the US suffers from a heart attack¹, where rapid diagnosis is crucial to saving heart muscle and preventing long-term damage, often leading to higher mortality rates. The ECG remains the primary tool for rapid diagnostics, yet the standard of care often fails to detect heart attacks accurately and timely, resulting in critical delays in treatment. PMcardio is the only solution capable of detecting both STEMI and STEMI equivalents on the ECG-aligning with the emerging emphasis on a paradigm shift towards Occlusion Myocardial Infarction (OMI) diagnosis and bridging a vital gap in early, precise heart attack diagnosis. “For the last 20 years, life-saving treatment exists for heart attack patients, yet far too many still don’t receive the urgent care they need due to delays in diagnosis and inefficient triage,” said Robert Herman, MD, PhD, Chief Medical Officer of Powerful Medical. This is especially critical in settings where immediate specialist evaluation isn’t available-only 17% of patients presenting to rural centers make it to the catheterization lab in time for intervention.² Dr. Herman added, “By equipping physicians and allied providers with an AI-powered tool for accurate and immediate STEMI detection, available around the clock, we can bridge this gap, ensure timely treatment, and improve patient outcomes, often preventing avoidable deaths”. The FDA’s Breakthrough Device Designation provides PMcardio with an expedited review process and close collaboration with the agency on its path toward market authorization. This designation is reserved for technologies that offer significant advantages over existing solutions and address unmet medical needs. This recognition underscores the FDA’s acknowledgment of Powerful Medical’s STEMI AI ECG Model, dubbed “Queen of Hearts”, to set a new standard in frontline heart attack detection and triage, ultimately enhancing care quality, accelerating treatment decisions, and saving lives through earlier and more accurate diagnosis. “FDA Breakthrough Device Designation is a pivotal milestone in our effort to revolutionize heart attack detection and ensure every patient receives immediate, life-saving care,” said Felix Bauer, COO of Powerful Medical. “We’re committed to bringing this life-saving technology to the US, the largest healthcare market in the world. This recognition by the FDA validates the impact of our innovation and brings us closer to transforming emergency cardiac care on a global scale,” added Martin Herman, CEO. With this designation, Powerful Medical not only works closely with the FDA on market approval but also gains improved access to CMS reimbursement mechanisms to bring the PMcardio STEMI AI ECG Model to healthcare providers nationwide, serving US public health. About Powerful Medical Powerful Medical is a pioneering health technology company specializing in AI-driven cardiovascular diagnostics. Its flagship product, PMcardio, harnesses AI to enhance ECG interpretation, streamline patient triage, and support clinical decision-making. With a mission to bridge the gap between innovation and clinical practice, Powerful Medical is committed to ensuring every patient receives the highest standard of care. References   View source version on businesswire.com: https://www.businesswire.com/news/home/20250325333487/en/ Lucia Bojkovskapr@powerfulmedical.com If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Microsoft infuses enterprise agents with deep reasoning, unveils data Analyst agent that outsmarts competitors</div>
                            <div class="article-source">VentureBeat - March 25, 2025 7:45 PM</div>
                            <div class="article-description">Microsoft announced Tuesday two significant additions to its Copilot Studio platform: deep reasoning capabilities that enable agents to tackle complex problems through careful, methodical thinking, and agent flows that combine AI flexibility with deterministic business process automation.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Microsoft has built the largest enterprise AI agent ecosystem, and is now extending its lead with powerful new capabilities that position the company ahead in one of enterprise tech’s most exciting segments. The company announced Tuesday evening two significant additions to its Copilot Studio platform: deep reasoning capabilities that enable agents to tackle complex problems through careful, methodical thinking, and agent flows that combine AI flexibility with deterministic business process automation. Microsoft also unveiled two specialized deep reasoning agents for Microsoft 365 Copilot: Researcher and Analyst. “We have customers with thousands of agents already,” Microsoft’s Corporate Vice President for Business and Industry Copilot Charles Lamanna, told VentureBeat in an exclusive interview on Monday. “You start to have this kind of agentic workforce where no matter what the job is, you probably have an agent that can help you get it done faster.” While the Researcher agent mirrors capabilities from competitors like OpenAI’s Deep Research and Google’s Deep Research, Microsoft’s Analyst agent represents a more differentiated offering. Designed to function like a personal data scientist, the Analyst agent can process diverse data sources, including Excel files, CSVs, and embedded tables in documents, generating insights through code execution and visualization. “This is not a base model off the shelf,” Lamanna emphasized. “This is quite a bit of extensions and tuning and training on top of the core models.” Microsoft has leveraged its deep understanding of Excel workflows and data analysis patterns to create an agent that aligns with how enterprise users actually work with data. The Analyst can automatically generate Python code to process uploaded data files, produce visualizations, and deliver business insights without requiring technical expertise from users. This makes it particularly valuable for financial analysis, budget forecasting and operational reporting use cases that typically require extensive data preparation. Microsoft’s deep reasoning capability extends agents’ abilities beyond simple task completion to complex judgment and analytical work. By integrating advanced reasoning models like OpenAI’s o1 and connecting them to enterprise data, these agents can tackle ambiguous business problems more methodically. The system dynamically determines when to invoke deeper reasoning, either implicitly based on task complexity or explicitly when users include prompts like “reason over this” or “think really hard about this.” Behind the scenes, the platform analyzes instructions, evaluates context, and selects appropriate tools based on the task requirements. This enables scenarios that were previously difficult to automate. For example, one large telecommunications company uses deep reasoning agents to generate complex RFP responses by assembling information from across multiple internal documents and knowledge sources, Lamanna told VentureBeat. Similarly, Thomson Reuters employs these capabilities for due diligence in mergers and acquisition reviews, processing unstructured documents to identify insights, he said. See an example of the agent reasoning at work in the video below: Microsoft has also introduced agent flows, which effectively evolve robotic process automation (RPA) by combining rule-based workflows with AI reasoning. This addresses customer demands for integrating deterministic business logic with flexible AI capabilities. “Sometimes they don’t want the model to freestyle. They don’t want the AI to make its own decisions. They want to have hard-coded business rules,” Lamanna explained. “Other times they do want the agent to freestyle and make judgment calls.” This hybrid approach enables scenarios like intelligent fraud prevention, where an agent flow might use conditional logic to route higher-value refund requests to an AI agent for deep analysis against policy documents. Pets at Home, a U.K.-based pet supplies retailer, has already deployed this technology for fraud prevention. Lamanna revealed the company has saved “over a million pounds” through the implementation. Similarly, Dow Chemical has realized “millions of dollars saved for transportation and freight management” through agent-based optimization. Below is a video showing the Agent Flows at work: Central to Microsoft’s agent strategy is its enterprise data integration through the Microsoft Graph, which is a comprehensive mapping of workplace relationships between people, documents, emails, calendar events, and business data. This provides agents with contextual awareness that generic models lack. “The lesser known secret capability of the Microsoft graph is that we’re able to improve relevance on the graph based on engagement and how tightly connected some files are,” Lamanna revealed. The system identifies which documents are most referenced, shared, or commented on, ensuring agents reference authoritative sources rather than outdated copies. This approach gives Microsoft a significant competitive advantage over standalone AI providers. While competitors may offer advanced models, Microsoft combines these with workplace context and fine-tuning optimized explicitly for enterprise use cases and Microsoft tools. Microsoft can leverage the same web data and model technology that competitors can, Lamanna noted, “but we then also have all the content inside the enterprise.” This creates a flywheel effect where each new agent interaction further enriches the graph’s understanding of workplace patterns. Microsoft has prioritized making these powerful capabilities accessible to organizations with varying technical resources, Lamanna said. The agents are exposed directly within Copilot, allowing users to interact through natural language without prompt engineering expertise. Meanwhile, Copilot Studio provides a low-code environment for custom agent development. “It’s in our DNA to have a tool for everybody, not just people who can boot up a Python SDK and make calls, but anybody can start to build these agents,” Lamanna emphasized. This accessibility approach has fueled rapid adoption. Microsoft previously revealed that over 100,000 organizations have used Copilot Studio and that more than 400,000 agents were created in the last quarter. While Microsoft appears to lead enterprise agent deployment today, competition is intensifying. Google has expanded its Gemini capabilities for agents and agentic coding, while OpenAI’s o1 model and Agents SDK provide powerful reasoning and agentic tools for developers. Big enterprise application companies like Salesforce, Oracle, ServiceNow, SAP and others have all launched agentic platforms for their customers over the last year. And also on Tuesday, Amazon’s AWS released an AI agent, called Amazon Q in Quicksight, to let employees to engage via natural language to perform data analysis without specialized skills. Employees can use natural language to perform expert-level data analysis, ask what-if questions, and get actionable recommendations, helping them unlock new insights and make decisions faster However, Microsoft’s advantage lies in its more comprehensive approach—a strong coupling with the leading reasoning model company, OpenAI, while also offering model choice, enterprise-grade infrastructure, extensive data integration across workplace tools, and a focus on business outcomes rather than raw AI capabilities. Microsoft has created an ecosystem that looks like best practice by combining personal copilots that understand individual work patterns with specialized agents for specific business processes. For enterprise decision-makers, the message is clear: agent technology has matured beyond experimentation to practical business applications with measurable ROI. The choice of platform increasingly depends on integration with existing tools and data. In this area, Microsoft holds an advantage in many application areas because of the number of users it has, for example, in Excel and Power Automate. Watch my full interview with Charles Lamanna embedded below to hear firsthand how Microsoft is driving its agent strategy, what these new capabilities mean for enterprise users, and how organizations are leveraging agents to deliver measurable business results: If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                    </section>
            
                    <section id="theme-7">
                        <h2>Consumer Electronics <small>(3 articles)</small></h2>
                        <p>Articles related to consumer electronics, gadgets, and their features</p>
                        
                        <h3>Articles in this Theme</h3>
            
                        <div class="article-card">
                            <div class="article-title">The best iPhone 16e cases: Top picks for protecting your new phone</div>
                            <div class="article-source">Business Insider - 2025-03-26T14:00:01Z</div>
                            <div class="article-description">The best iPhone 16e cases offer durable, practical, and visually appealing designs compatible with Qi wireless charging.</div>
                            <div class="article-content"><p>The best iPhone 16e cases offer durable, practical, and visually appealing designs compatible with Qi wireless charging. To help you decide which case best suits your needs, we&#x27;ve arranged our favorite available options from Apple and third-party brands like Zagg, Otterbox, Dbrand, Spigen, and others. Among the best iPhone 16e cases, our top pick is the Zagg Crystal Palace Snap case, a sturdy, clear model with an integrated array of magnets. Our favorite budget case is the affordable Spigen Tough Armor case, with its versatile design and raised edges for screen and camera protection. When you buy through our links, Business Insider may earn an affiliate commission. Learn more As noted in our iPhone 16e review, it&#x27;s wise to protect Apple&#x27;s latest budget phone with a high-quality case. Not even the best iPhone is impervious to accidental damage, and the aluminum edges and glass back of the iPhone 16e offer no exception. Coupled with a screen protector, a case remains the most cost-effective way to avoid the hassle of warranty repairs or the significant cost of replacing a phone outright. See our guide to the best iPhone 16e screen protectors for top options. Also, if you intend to use the iPhone 16e with magnetic accessories, you&#x27;ll need a case with a built-in magnetic array, as the phone notably lacks Apple&#x27;s MagSafe technology. As with any of the best phones, it&#x27;s important to consider cases that will provide adequate protection and functionality for your daily use of the iPhone 16e. Start by considering the level of protection that best compliments your life circumstances. Rugged cases with raised edges and shock-resistant materials are ideal for heavy-duty use or those prone to drops. In contrast, slim cases with moderate protection may be better suited to those looking for a lighter, less bulky option. It&#x27;s also worth ensuring that the case you choose will not interfere with features you want to use, such as Qi wireless charging. While the iPhone 16e lacks MagSafe, cases with integrated magnets can make the phone compatible with magnetic accessories and magnetic wireless chargers. However, the phone will wirelessly charge at slower speeds (around 7.5W) relative to the flagship iPhone 16 series and most recent iPhones, which feature the MagSafe charging standard (up to 25W). A case meant for the iPhone 16 will not fit the iPhone 16e, and vice versa, due to differences in their designs and dimensions. While the two phones share the same width and depth, the iPhone 16e is slightly shorter than the iPhone 16, has a distinct single-camera system, and lacks the flagship phone&#x27;s Camera Control button. See our guide to the best iPhone 16 cases to protect the flagship phone, and read our full iPhone 16 review or iPhone 16e vs. iPhone 16 guide for a comprehensive comparison of the models. You can purchase logo and accolade licensing to this story here.Disclosure: Written and researched by the Insider Reviews team. We highlight products and services you might find interesting. If you buy them, we may get a small share of the revenue from the sale from our partners. We may receive products free of charge from manufacturers to test. This does not drive our decision as to whether or not a product is featured or recommended. We operate independently from our advertising team. We welcome your feedback. Email us at reviews@businessinsider.com.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">Groq and PlayAI just made voice AI sound way more human — here’s how</div>
                            <div class="article-source">VentureBeat - March 26, 2025 8:30 AM</div>
                            <div class="article-description">Groq partners with PlayAI to deliver Dialog, an emotionally intelligent text-to-speech model that runs 10x faster than real-time speech, including the Middle East&#x27;s first Arabic voice AI model.</div>
                            <div class="article-content"><p>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Learn More Groq and PlayAI announced a partnership today to bring Dialog, an advanced text-to-speech model, to market through Groq’s high-speed inference platform. The partnership combines PlayAI’s expertise in voice AI with Groq’s specialized processing infrastructure, creating what the companies claim is one of the most natural-sounding and responsive text-to-speech systems available. “Groq provides a complete, low latency system for automatic speech recognition (ASR), GenAI, and text-to-speech, all in one place,” said Ian Andrews, Chief Revenue Officer at Groq, in an exclusive interview with VentureBeat. “With Dialog now running on GroqCloud, this means customers won’t have to use multiple providers for a single use case — Groq is a one stop solution.” Dialog is notable for being available in both English and Arabic, with the Arabic version representing the first voice AI specifically designed for the Middle East region. The inclusion of Arabic as one of the initial offerings was strategic for both companies. “Arabic is the fourth most spoken language globally — by partnering with PlayAI to offer an Arabic TTS model, Groq is unlocking a key global market and enabling broader access to fast AI inference,” Andrews told VentureBeat. The companies claim their solution addresses key shortcomings in existing voice AI technologies, particularly around natural speech patterns and response speed. According to benchmark testing conducted by third-party evaluator Podonos, Dialog was preferred by users at a rate of 10:1 versus ElevenLabs v2.5 Turbo and over 3:1 against ElevenLabs Multilingual v2.0. What sets Dialog apart is its sophisticated approach to context. Rather than treating each vocalization as an isolated event, the system maintains awareness of the entire conversation flow. “We built a novel architecture that we call an ‘adaptive speech contextualizer‘ (ASC), which allows the model to use the full context and history of a conversation,” said Mahmoud Felfel, co-founder and CEO of PlayAI, in an interview with VentureBeat. “This means that every response isn’t just a standalone output; it’s enriched with appropriate prosody, tone, and emotion that reflect the flow of the conversation.” For enterprises looking to implement conversational AI, latency — the delay between request and response — has been a persistent challenge. Groq’s specialized Language Processing Units (LPUs) appear to provide a significant advantage in this area. “Based on initial internal testing, Groq is delivering up to 140 characters per second on PlayAI’s Dialog model, a significant boost compared to the same model running on GPUs at 86 characters per second,” explained Andrews. “That means that Dialog generates text up to 10 times faster than real-time.” The partnership comes at a time of significant expansion for Groq, which recently secured a $1.5 billion commitment from Saudi Arabia to fund additional infrastructure. The company has established a data center in Dammam, which it describes as “the region’s largest inference cluster.” “Partnering with Groq was a no-brainer; they’re the industry leader in advanced AI inference infrastructure,” said Felfel. “With TTS and agents, low latency is key. We’ve already optimized Dialog for these real-time applications, but partnering with Groq allows us to deliver the lowest latency voice model on the market.” The voice AI market has seen rapid growth as businesses look to automate customer interactions while maintaining a natural, human-like experience. Applications range from customer service and sales automation to voice-overs and accessibility features for the visually impaired. “Beyond customer service, other enterprise use cases include automating sales and appointment scheduling, on-boarding and personal assistants, creating voice overs to existing content, translating English audio and video content into Arabic, increasing website and static content accessibility for the visually impaired, and more,” Andrews said. For PlayAI, which was founded by entrepreneurs from the Middle East and North Africa region, the inclusion of Arabic language capabilities was particularly meaningful. “As MENA founders, we know the region is heavily investing in AI capabilities and infrastructure as inflected in investments like Groq, but also world-leading adoption,” said Felfel. “Arabic is a global business language and one that we grew up speaking, so it was a natural choice as one of our core languages.” The companies have made the Dialog technology available through GroqCloud’s tiered service model, which includes both free and paid options. This approach allows developers to experiment with the technology before committing to larger implementations. “GroqCloud offers both free and paid plans. Anyone can create an account and create an API code for free,” Andrews explained. “Our paid Developer Tier is self-serve, meaning anyone with a credit card can sign up themselves.” As voice becomes an increasingly important interface for AI systems, this partnership positions both companies to capitalize on the growing demand for more natural and responsive conversational experiences. By addressing the technical challenges of latency and natural speech patterns, Groq and PlayAI may have removed significant barriers to wider adoption of voice AI in enterprise settings. If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                        <div class="article-card">
                            <div class="article-title">TensorOpera AI and Samsung Electronics Showcase the Future of Generative AI on Mobile Devices</div>
                            <div class="article-source">VentureBeat - March 26, 2025 4:25 AM</div>
                            <div class="article-description">Press Release TensorOpera AI and Samsung Electronics announced a pioneering collaboration, which was showcased at the Consumer Electronics Show (CES) 2025 in Las Vegas, demonstrating the immense potential of deploying multi-modal generative AI applications directly…</div>
                            <div class="article-content"><p>PALO ALTO, Calif.–(BUSINESS WIRE)–March 26, 2025– TensorOpera AI and Samsung Electronics announced a pioneering collaboration, which was showcased at the Consumer Electronics Show (CES) 2025 in Las Vegas, demonstrating the immense potential of deploying multi-modal generative AI applications directly on mobile devices powered by Samsung Exynos processors. This milestone marks a significant step forward in redefining the role of mobile technology in advancing artificial intelligence. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20250318795401/en/ Bringing generative AI to mobile phones is a transformative innovation that addresses critical challenges such as privacy, personalization, and cost. Smartphones, which store vast amounts of personal data, offer an untapped opportunity to create AI agents uniquely tailored to enhance productivity, communication, and social interactions. With over 80 apps installed on the average smartphone-spanning self-care, productivity, financial planning, and more-the possibilities for personalized AI applications are endless. Imagine a future where your phone acts as a personal creativity coach, helping you brainstorm innovative ideas, sketch designs, compose music, or generate stunning visual art-all tailored to your unique style and preferences, while safeguarding your privacy. Picture a financial advisor that not only optimizes investments but continuously tracks market trends, predicts opportunities, and customizes strategies to align seamlessly with your goals. Envision a writing assistant that adapts to your voice, integrates effortlessly across your apps, drafts emails, creates presentations, and generates compelling content-all personalized to you. This is the transformative potential of bringing multimodal generative AI directly to mobile devices. TensorOpera implements Android applications and highly optimized C++ multi-modal inference pipeline to import and release the Apps on top of Exynos AI Stack. These Apps completely run on any smartphones equipped with Exynos processors with a reasonable inference latency. “Our collaborative demonstration at CES 2025 showcases how advanced multi-modal generative AI applications can seamlessly integrate into mobile devices, paving the way for smarter, more personalized AI solutions that respect user privacy-powered by Samsung’s Exynos mobile chips,” said Dr. Kee-Bong Song, SVP and Head of S.LSI US R&amp;D Center at Samsung Electronics. TensorOpera AI: Pioneering Hybrid Edge-Cloud AI Development TensorOpera AI’s end-to-end platform enables developers to design and deploy AI applications across cloud, mobile, and hybrid mobile-cloud environments. The future of generative AI lies in hybrid edge-cloud deployments, which combine the strengths of on-device processing with the scalability of cloud infrastructure. This hybrid approach balances efficiency and personalization. By processing sensitive data locally on mobile devices, users retain control over their privacy while accessing the immense computational power of the cloud when necessary. This ensures cost-effective deployment, minimizes reliance on centralized cloud infrastructures, and enhances scalability for adoption across industries and geographies. The benefits of hybrid mobile-cloud architectures extend far beyond individual devices. By reducing dependency on expensive cloud GPUs, this distributed model democratizes AI, making advanced models accessible to a broader range of users and developers. This innovation aligns with industry predictions that the market for AI assistants could surpass $6 trillion, with hybrid architectures forming the foundation of next-generation AI ecosystems. “Our collaboration with Samsung Electronics demonstrates the future of AI: smarter, more secure, and deeply personalized solutions powered by the synergy of mobile and cloud technologies,” said Salman Avestimehr, Chairman of TensorOpera AI. “As generative AI continues to evolve, we see hybrid deployments as the key to unlocking its full potential for users worldwide.” Building on their success with Samsung’s on-device multi-modal GenAI project, the founders of TensorOpera have once again demonstrated their prowess in Mobile AI through the ChainOpera AI Terminal App. They have developed LLM-based AI Agent Networks that seamlessly integrate cloud, edge, and device capabilities, serving over a million users at a cost 10 times lower than industry standard, and state-of-the-art LLM performance with a federated AI approach. About TensorOpera AI TensorOpera, Inc. (formerly FedML, Inc.) is an innovative AI company based in Silicon Valley, specifically Palo Alto, California. TensorOpera specializes in developing scalable and secure AI platforms tailored for enterprises and developers with two flagship products: (1) TensorOpera® AI Platform: Accessible at TensorOpera.ai, this platform serves as a comprehensive generative AI ecosystem. It features robust tools for enterprise AI platforms, model deployment, model serving, AI agent APIs, and more. It supports launching training and inference jobs on a serverless/decentralized GPU cloud, experimental tracking for distributed training, and enhanced security and privacy measures. (2) TensorOpera® FedML: Available at FedML.ai, this platform is a leader in federated learning and analytics, supporting zero-code implementation. It includes a lightweight, cross-platform Edge AI SDK suitable for edge GPUs, smartphones, and IoT devices. Additionally, it offers a user-friendly MLOps platform to streamline decentralized machine learning and deployment in real-world applications. Founded in February 2022, TensorOpera has quickly grown to support a large number of enterprises and developers worldwide.  View source version on businesswire.com: https://www.businesswire.com/news/home/20250318795401/en/ contact@tensoropera.com If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI. Read our Privacy Policy Thanks for subscribing. Check out more VB newsletters here. An error occured.</p></div>
                        </div>
                
                    </section>
            
                </section>
                
                <footer>
                    <p>Generated using AI clustering and summarization</p>
                    <p><small>Click on any article title to view its full content</small></p>
                </footer>
            </div>
        </body>
        </html>
        